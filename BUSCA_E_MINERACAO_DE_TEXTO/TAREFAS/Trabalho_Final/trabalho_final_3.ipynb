{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gcNQeDWNNPX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "# Function to list files in a GitHub repository directory\n",
        "def list_files_in_github_repo(repo_url, branch, folder_path):\n",
        "    api_url = f\"https://api.github.com/repos/{repo_url.split('/')[-2]}/{repo_url.split('/')[-1]}/contents/{folder_path}?ref={branch}\"\n",
        "    response = requests.get(api_url)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "# Function to download a file from GitHub\n",
        "def download_file_from_github(download_url, local_path):\n",
        "    response = requests.get(download_url)\n",
        "    response.raise_for_status()\n",
        "    with open(local_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "# Function to download all PDFs from a GitHub directory\n",
        "def download_pdfs_from_github(repo_url, branch, folder_path, target_path):\n",
        "    # List all files and directories in the specified GitHub folder\n",
        "    items = list_files_in_github_repo(repo_url, branch, folder_path)\n",
        "\n",
        "    # Ensure the target directory exists\n",
        "    if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path)\n",
        "\n",
        "    for item in items:\n",
        "        item_path = os.path.join(folder_path, item['name'])\n",
        "        local_item_path = os.path.join(target_path, item['name'])\n",
        "\n",
        "        if item['type'] == 'dir':\n",
        "            # Recursively download PDFs from subdirectories\n",
        "            download_pdfs_from_github(repo_url, branch, item_path, local_item_path)\n",
        "        elif item['name'].endswith('.pdf'):\n",
        "            # Download PDF file\n",
        "            print(f\"Downloading {item['name']}...\")\n",
        "            if not os.path.exists(target_path):\n",
        "                os.makedirs(target_path)\n",
        "            download_file_from_github(item['download_url'], local_item_path)\n",
        "\n",
        "# Usage\n",
        "repo_url = 'https://github.com/geeklicantropo/MESTRADO_PUBLICO'\n",
        "branch = 'main'\n",
        "folder_path = 'BUSCA_E_MINERACAO_DE_TEXTO/TAREFAS/Trabalho_Final/papers'  # Path to the folder in the repository\n",
        "target_path = '/content/papers'  # Local path in Google Colab or local environment\n",
        "\n",
        "download_pdfs_from_github(repo_url, branch, folder_path, target_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fitz\n",
        "!pip install pip install PyMuPDF==1.19.0\n",
        "!python -m spacy download pt_core_news_sm\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "id": "Cxb75qn0OYIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "id": "1mk1Zhxp5fQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as bibliotecas\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import unicodedata\n",
        "import fitz  # PyMuPDF para leitura de PDFs\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "# Inicializa o modelo de lÃ­ngua portuguesa do SpaCy para processamento de texto\n",
        "nlp = spacy.load(\"pt_core_news_sm\")"
      ],
      "metadata": {
        "id": "fljCNfh_wHw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, additional_words):\n",
        "        self.stop_words = self.setup_custom_stopwords(additional_words)\n",
        "\n",
        "    def setup_custom_stopwords(self, additional_words):\n",
        "        spacy_stopwords = nlp.Defaults.stop_words\n",
        "        spacy_stopwords.update(additional_words)\n",
        "        return spacy_stopwords\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def consolidate_words_by_root(self, text):\n",
        "        doc = nlp(text)\n",
        "        root_dict = defaultdict(list)\n",
        "        for token in doc:\n",
        "            if token.text.lower() not in self.stop_words and len(token.text) >= 3:\n",
        "                root_dict[token.lemma_].append(token.text.lower())\n",
        "        consolidated_text = []\n",
        "        for lemma, words in root_dict.items():\n",
        "            most_common = Counter(words).most_common(1)[0][0]\n",
        "            consolidated_text.append(most_common)\n",
        "        return ' '.join(consolidated_text)\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_file):\n",
        "        text = ''\n",
        "        with fitz.open(pdf_file) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "        return text\n",
        "\n",
        "    def process_pdfs(self, base_path):\n",
        "        doc_contents = []\n",
        "        years = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
        "        for year in years:\n",
        "            year_path = os.path.join(base_path, year)\n",
        "            pdf_files = glob.glob(f\"{year_path}/*.pdf\")\n",
        "            for pdf_file in tqdm(pdf_files, desc=f\"Processing year {year}\"):\n",
        "                text = self.extract_text_from_pdf(pdf_file)\n",
        "                cleaned_text = self.clean_text(text)\n",
        "                consolidated_text = self.consolidate_words_by_root(cleaned_text)\n",
        "                doc_contents.append({'text': consolidated_text, 'year': year})\n",
        "        return pd.DataFrame(doc_contents)\n",
        "\n",
        "    def compute_tfidf(self, docs):\n",
        "        vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.95)\n",
        "        tfidf_matrix = vectorizer.fit_transform(docs)\n",
        "        return tfidf_matrix, vectorizer.get_feature_names_out()\n",
        "\n",
        "    def plot_wordcloud(self, text, title=\"Most Important Words in All Articles\", max_words=50):\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=max_words).generate(text)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_topic_evolution(self, docs_df, tfidf_features, tfidf_matrix):\n",
        "        tfidf_features_list = tfidf_features.tolist()\n",
        "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_features_list)\n",
        "        tfidf_df['year'] = docs_df['year'].values\n",
        "        yearly_data = tfidf_df.groupby('year').mean()\n",
        "        top_words = yearly_data.mean().sort_values(ascending=False).head(10).index\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        for word in top_words:\n",
        "            plt.plot(yearly_data.index, yearly_data[word], marker='o', label=word)\n",
        "        plt.title('Evolution of Top 10 Words Over Years')\n",
        "        plt.xlabel('Year')\n",
        "        plt.ylabel('Average TF-IDF Score')\n",
        "        plt.legend(title='Top Words')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_wordclouds_by_year(self, docs_df):\n",
        "        years = docs_df['year'].unique()\n",
        "        num_years = len(years)\n",
        "        cols = 2\n",
        "        rows = (num_years + 1) // cols\n",
        "\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, year in enumerate(years):\n",
        "            year_text = ' '.join(docs_df[docs_df['year'] == year]['text'])\n",
        "            wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=50).generate(year_text)\n",
        "            axes[i].imshow(wordcloud, interpolation='bilinear')\n",
        "            axes[i].axis(\"off\")\n",
        "            axes[i].set_title(f'Word Cloud for Year {year}')\n",
        "\n",
        "        for j in range(i + 1, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_topic_evolution_by_year(self, docs_df):\n",
        "        years = docs_df['year'].unique()\n",
        "        num_years = len(years)\n",
        "        cols = 2\n",
        "        rows = (num_years + 1) // cols\n",
        "\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, year in enumerate(years):\n",
        "            year_docs = docs_df[docs_df['year'] == year]['text']\n",
        "            tfidf_matrix, tfidf_features = self.compute_tfidf(year_docs)\n",
        "            tfidf_features_list = tfidf_features.tolist()\n",
        "            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_features_list)\n",
        "            top_words = tfidf_df.mean().sort_values(ascending=False).head(10).index\n",
        "            for word in top_words:\n",
        "                axes[i].plot(tfidf_df[word], marker='o', label=word)\n",
        "            axes[i].set_title(f'Top 10 Words for Year {year}')\n",
        "            axes[i].set_xlabel('Document Index')\n",
        "            axes[i].set_ylabel('TF-IDF Score')\n",
        "            axes[i].legend(title='Top Words')\n",
        "\n",
        "        for j in range(i + 1, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "04CN0zeaN19t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/papers'\n",
        "additional_words = ['umas', 'naquele', 'baixo', 'por', 'tive', 'contra', 'gerais', 'bastante', 'aquelas', 'meus', 'demais', 'seus', 'novos',\n",
        "                    'file_name', 'tabela', 'trabalho', 'estÃ¡', 'gaiola', 'real', 'perto', 'vem', 'era', 'fazemos', 'speed', 'esse', 'fui',\n",
        "                    'para', 'diferentes', 'tarde', 'vindo', 'catorze', 'questÃ£o', 'prÃ³ximos', 'neste', 'vÃ¡rios', 'nome', 'tantas', 'uxo',\n",
        "                    'apoio', 'seria', 'puderam', 'dezasseis', 'pole', 'sera', 'todas', 'local', 'diante', 'hoje', 'foram', 'inteligencia',\n",
        "                    'falhas', 'posiÃ§Ã£o', 'hvdc', 'primeiras', 'naqueles', 'uns', 'faz', 'quÃ¡is', 'desde', 'informacoes', 'este', 'nesta',\n",
        "                    'coisa', 'sois', 'obrigada', 'fez', 'nanceiros', 'trÃªs', 'minhas', 'agora', 'fazem', 'possivelmente', 'numa', 'todo',\n",
        "                    'exemplo', 'estas', 'parece', 'novo', 'relaÃ§Ã£o', 'vÃªm', 'nao', 'sÃ©timo', 'atrÃ¡s', 'com', 'variacoe', 'tem', 'vÃ£o',\n",
        "                    'outras', 'grandes', 'nada', 'isso', 'apÃ³s', 'tendes', 'estÃ¡s', 'tal', 'prÃ³prias', 'tensoe', 'estava', 'fim', 'vinte',\n",
        "                    'www.ri.light.com.br', 'das', 'nanceira', 'viagem', 'elas', 'depois', 'vez', 'meio', 'partir', 'nossas', 'sim', 'servicos',\n",
        "                    'menos', 'method', 'oito', 'vsc', 'analise', 'presented', 'dizer', 'naquelas', 'onde', 'nessas', 'algo', 'dessas', 'tivemos',\n",
        "                    'ver', 'system', 'nesses', 'ano', 'caixa', 'dezanove', 'dÃ£o', 'cada', 'pode', 'teste', 'quanto', 'segunda', 'uma', 'deverÃ¡',\n",
        "                    'pelos', 'final', 'valores', 'tÃ£o', 'minha', 'sobre', 'federal', 'terceiro', 'mesmo', 'lugar', 'introducao', 'vos', 'scal',\n",
        "                    'sou', 'dezoito', 'tudo', 'sua', 'essa', 'nestas', 'depreciacao', 'mil', 'primeiro', 'hora', 'estivemos', 'adeus', 'naquela',\n",
        "                    'quatro', 'boa', 'desses', 'assim', 'pÃµem', 'hidreletrica', 'primeira', 'dar', 'quero', 'vocÃªs', 'destas', 'momento', 'dizem',\n",
        "                    'foi', 'daqueles', 'futuros', 'muitos', 'poucos', 'paucas', 'pÃµe', 'logo', 'nosso', 'parte', 'caso', 'aquela', 'tambÃ©m',\n",
        "                    'querem', 'Ã¡rea', 'esses', 'daquelas', 'aces-', 'vens', 'favor', 'bom', 'nesse', 'vezes', 'longe', 'queremas', 'algorithm',\n",
        "                    'resultado', 'vossa', 'daquela', 'obrigado', 'mÃªs', 'outro', 'tanta', 'embora', 'falta', 'quarto', 'cima', 'maior', 'nunca',\n",
        "                    'outra', 'dois', 'prÃ³prio', 'estivestes', 'mais', 'prÃ³ximo', 'suas', 'tiveste', 'podem', 'geral', 'num', 'fazeis', 'quer',\n",
        "                    'nÃ­vel', 'foste', 'mÃ¡ximo', 'figura', 'todos', 'estiveram', 'sexta', 'grande', 'temos', 'tivestes', 'dÃºvida', 'destes',\n",
        "                    'prÃ³ximas', 'grupo', 'sete', 'nÃ£o', 'fazer', 'seis', 'desta', 'lado', 'rede', 'beto', 'toda', 'mesma', 'debaixo',\n",
        "                    'eles', 'teu', 'rio', 'diz', 'mas', 'possÃ­vel', 'quinze', 'cinco', 'dez', 'nÃºmero', 'dezassete', 'apenas', 'alÃ©m',\n",
        "                    'como', 'zero', 'pouca', 'teus', 'pÃ´de', 'quinta', 'nestes', 'terceira', 'outros', 'desse', 'nova', 'tanto', 'ela',\n",
        "                    'prÃ³pria', 'atÃ©', 'maioria', 'nossos', 'custa', 'prÃ³xima', 'ser', 'ontem', 'oes', 'artificial', 'abc', 'bons', 'boas',\n",
        "                    'fomos', 'quarta', 'dos', 'ali', 'obra', 'anos', 'tambem', 'quem', 'sistema', 'nove', 'talvez', 'horas', 'sao', 'aos',\n",
        "                    'estes', 'sexto', 'qual', 'alem', 'certamente', 'duas', 'oitava', 'nenhuma', 'nas', 'estÃ£o', 'quereis', 'tuas',\n",
        "                    'vossas', 'pouco', 'custo', '220/380/440', 'prÃ³prios', 'sempre', 'essas', 'breve', 'sei', 'cedo', 'aquilo', 'dessa',\n",
        "                    'sÃ©tima', 'vocÃª', 'disponivel', 'seu', 'primeiros', 'estou', 'sabe', 'porquÃª', 'conselho', 'certeza', 'apos', 'vossos',\n",
        "                    'algumas', 'cento', 'oitavo', 'pela', 'isto', 'forma', 'alguns', 'projecoe', 'aqui', 'atravÃ©s', 'estive', 'nos', 'total',\n",
        "                    'nanceiras', 'entre', 'antes', 'tua', 'application', 'tÃªm', 'bem', 'estar', 'vÃ³s', 'nÃ³s', 'janeiro', 'guanhaes', 'dentro',\n",
        "                    'dados', 'pelas', 'nessa', 'posso', 'onze', 'daquele', 'quinto', 'deve', 'tiveram', 'menor', 'meses', 'meu', 'pontos',\n",
        "                    'somos', 'ter', 'sob', 'nossa', 'queres', 'veloso', 'aquele', 'novas', 'deste', 'quando', 'ponto', 'pelo', 'ele',\n",
        "                    'treze', 'mal', 'tres', 'faÃ§o', 'tens', 'muito', 'sabem', 'tenho', 'fostes', 'esta', 'economica-', 'aqueles',\n",
        "                    'esteve', 'model', 'poder', 'vosso', 'noite', 'dia', 'ainda', 'que', 'teve', 'estiveste', 'quÃª', 'vais', 'nem', 'fazes',\n",
        "                    'vai', 'porem', 'segundo', 'for', 'sem', 'porque', 'sÃ£o', 'doze']"
      ],
      "metadata": {
        "id": "NzI9Dn7VIM_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Processamento dos PDFs e geraÃ§Ã£o dos resultados\n",
        "processor = TextProcessor(additional_words)\n",
        "docs_df = processor.process_pdfs(path)"
      ],
      "metadata": {
        "id": "gH01Kj5B7BSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotagens gerais\n",
        "all_text = ' '.join(docs_df['text'])\n",
        "tfidf_matrix, tfidf_features = processor.compute_tfidf(docs_df['text'])\n",
        "processor.plot_wordcloud(all_text)\n",
        "processor.plot_topic_evolution(docs_df, tfidf_features, tfidf_matrix)"
      ],
      "metadata": {
        "id": "PvNMbW_yD95F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotagens por ano\n",
        "processor.plot_wordclouds_by_year(docs_df)\n",
        "processor.plot_topic_evolution_by_year(docs_df)"
      ],
      "metadata": {
        "id": "xDPO3-1KD92r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYXhoe5uD90M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JVJVY5LUD9x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-cAr7YcUD9vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rfd3ScU87BFH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}