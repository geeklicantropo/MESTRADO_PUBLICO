{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Objetivo</h1>\n",
        "<h5><b>Avaliação:</b></h5><p>Para avaliar a performance da previsão do timestamp datahora, será usada a métrica RMSE. Para avaliar a performance da previsão de latitude e longitude, será usada a métrica Mean Haversine Distance</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Metodologia</h1>\n",
        "<p>\n",
        "<h3>Previsão datahora:</h3>\n",
        "1 - Divisão de prepared_data_hist em treino e teste:\n",
        "\n",
        "Dividir prepared_data_hist em conjunto de treino (dados mais antigos) e conjunto de teste (dados mais recentes) usando a coluna datahora_converted.\n",
        "Criar X removendo a coluna datahora e y contendo apenas datahora.\n",
        "Realizar validação cruzada, treinamento, teste e criação do modelo preditivo para prever datahora.\n",
        "\n",
        "2 - Criação do DataFrame previsto_treino_data_datahora:\n",
        "\n",
        "Fazer o merge de prepared_data_hist (sem a coluna datahora) com intermediate_treino_data_lat_long usando as colunas ordem, linha, latitude, longitude.\n",
        "Adicionar a coluna id ao DataFrame resultante.\n",
        "\n",
        "3 - Aplicação do modelo preditivo no DataFrame previsto_treino_data_datahora:\n",
        "\n",
        "Remover a coluna id temporariamente para evitar conflitos.\n",
        "Prever os valores de datahora usando o modelo criado no passo 1.\n",
        "Adicionar os valores previstos de datahora como uma nova coluna.\n",
        "Recolocar a coluna id.\n",
        "\n",
        "4 - Preparação do DataFrame final após a predição:\n",
        "\n",
        "Garantir que o DataFrame contém todas as colunas originais de prepared_data_hist (exceto datahora) mais a coluna datahora com os valores previstos e a coluna id.\n",
        "\n",
        "5 - Criação do arquivo submissao_datahora:\n",
        "\n",
        "Realizar o merge do DataFrame do passo 4 com intermediate_resposta_data_lat_long usando a coluna id.\n",
        "Gerar o arquivo submissao_datahora contendo as colunas id, latitude, longitude, e datahora.\n",
        "\n",
        "<h3>Previsão de latitude e longitude:</h3>\n",
        "\n",
        "6 - Divisão de prepared_data_hist em treino e teste:\n",
        "\n",
        "Dividir prepared_data_hist em conjunto de treino (dados mais antigos) e conjunto de teste (dados mais recentes) usando a coluna datahora_converted.\n",
        "Criar X removendo as colunas latitude e longitude e y contendo apenas latitude e longitude.\n",
        "Realizar validação cruzada, treinamento, teste e criação do modelo preditivo para prever latitude e longitude.\n",
        "\n",
        "7 - Criação do DataFrame previsto_treino_data_lat_long:\n",
        "\n",
        "Fazer o merge de prepared_data_hist (sem as colunas latitude e longitude) com intermediate_treino_data_datahora usando as colunas ordem, linha, datahora.\n",
        "Adicionar a coluna id ao DataFrame resultante.\n",
        "\n",
        "8 - Aplicação do modelo preditivo no DataFrame previsto_treino_data_lat_long:\n",
        "\n",
        "Remover a coluna id temporariamente para evitar conflitos.\n",
        "Prever os valores de latitude e longitude usando o modelo criado no passo 6.\n",
        "Adicionar os valores previstos de latitude e longitude como novas colunas.\n",
        "Recolocar a coluna id.\n",
        "\n",
        "9 - Preparação do DataFrame final após a predição:\n",
        "\n",
        "Garantir que o DataFrame contém todas as colunas originais de prepared_data_hist (exceto latitude e longitude) mais as colunas latitude e longitude com os valores previstos e a coluna id.\n",
        "\n",
        "10 - Criação do arquivo submissao_lat_long:\n",
        "\n",
        "Realizar o merge do DataFrame do passo 9 com intermediate_resposta_data_datahora usando a coluna id.\n",
        "Gerar o arquivo submissao_lat_long contendo as colunas id, datahora, latitude, e longitude.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from glob import glob\n",
        "import json\n",
        "import cudf\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from cuml.cluster import DBSCAN\n",
        "from memory_profiler import memory_usage\n",
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpE2fEW2O6nD"
      },
      "outputs": [],
      "source": [
        "# Caminhos das pastas\n",
        "base_dir = os.getcwd()\n",
        "dados_dir = os.path.join(base_dir, 'dados')\n",
        "intermediarios_dir = os.path.join(base_dir, 'intermediarios')\n",
        "processados_dir = os.path.join(base_dir, 'processados')\n",
        "\n",
        "# Criar diretórios se não existirem\n",
        "os.makedirs(intermediarios_dir, exist_ok=True)\n",
        "os.makedirs(processados_dir, exist_ok=True)\n",
        "\n",
        "# Lista global para armazenar os DataFrames processados\n",
        "final_df_list = []\n",
        "\n",
        "# Dicionário para armazenar o sentido atual de cada ônibus\n",
        "sentidos_atual = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QlZ8uYjonMx"
      },
      "outputs": [],
      "source": [
        "def extract_and_process_zip(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        for file in tqdm(zip_ref.namelist(), desc=f'Processing {os.path.basename(zip_path)}'):\n",
        "            if file.endswith('.json'):\n",
        "                zip_ref.extract(file, extract_to)\n",
        "                json_path = os.path.join(extract_to, file)\n",
        "                process_json(json_path)\n",
        "                os.remove(json_path)\n",
        "\n",
        "def process_json(json_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    df = cudf.DataFrame.from_pandas(pd.DataFrame(data))\n",
        "    original_size = df.memory_usage(deep=True).sum()\n",
        "\n",
        "    linhas_validas = [483, 864, 639, 3, 309, 774, 629, 371, 397, 100, 838, 315, 624, 388, 918, 665, 328, 497, 878, 355, 138, 606, 457, 550, 803, 917, 638, 2336, 399, 298, 867, 553, 565, 422, 756, 186012003, 292, 554, 634, 232, 415, 2803, 324, 852, 557, 759, 343, 779, 905, 108]\n",
        "    df = df[df['linha'].astype(str).isin(map(str, linhas_validas))]\n",
        "\n",
        "    #df['datahora'] = cudf.to_datetime(df['datahora'].astype('int64'), unit='ms')\n",
        "    df['datahoraenvio'] = cudf.to_datetime(df['datahoraenvio'].astype('int64'), unit='ms')\n",
        "    df['datahora_converted'] = cudf.to_datetime(df['datahora'].astype('int64'), unit='ms')\n",
        "\n",
        "    df = df[(df['datahora_converted'].dt.hour >= 8) & (df['datahora_converted'].dt.hour < 22)]\n",
        "\n",
        "    df['latitude'] = df['latitude'].str.replace(',', '.').astype('float32')\n",
        "    df['longitude'] = df['longitude'].str.replace(',', '.').astype('float32')\n",
        "\n",
        "    while df['latitude'].isnull().any() or df['longitude'].isnull().any():\n",
        "        df['latitude'] = df['latitude'].interpolate()\n",
        "        df['longitude'] = df['longitude'].interpolate()\n",
        "        df['latitude'] = df['latitude'].fillna(method='ffill').fillna(method='bfill')\n",
        "        df['longitude'] = df['longitude'].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    problematic_values = df[~df['velocidade'].astype(str).str.isnumeric()]\n",
        "    if not problematic_values.empty:\n",
        "        print(f\"Problematic values in 'velocidade':\\n{problematic_values[['velocidade', 'velocidade']].head()}\")\n",
        "\n",
        "    df['velocidade'] = df['velocidade'].astype('int32')\n",
        "\n",
        "    df = df[(df['velocidade'] >= 0) & (df['velocidade'] <= 250)]\n",
        "    df = df[df['latitude'].between(-90, 90) & df['longitude'].between(-180, 180)]\n",
        "\n",
        "    reduced_size = df.memory_usage(deep=True).sum()\n",
        "\n",
        "    final_df_list.append(df)\n",
        "\n",
        "def save_intermediate_data(df, filename):\n",
        "    intermediate_file_path = os.path.join(intermediarios_dir, filename)\n",
        "    df.to_parquet(intermediate_file_path, index=False)\n",
        "    print(f'Data saved to {intermediate_file_path}')\n",
        "\n",
        "def load_or_process_data():\n",
        "    global final_df_list\n",
        "    final_df_list = []\n",
        "\n",
        "    intermediate_file_path = os.path.join(intermediarios_dir, 'intermediate_data.parquet')\n",
        "\n",
        "    if os.path.exists(intermediate_file_path):\n",
        "        print(\"Intermediate file found. Loading data from intermediate file.\")\n",
        "        final_df = cudf.read_parquet(intermediate_file_path)\n",
        "    else:\n",
        "        print(\"Intermediate file not found. Processing JSON files.\")\n",
        "        zip_files = glob(os.path.join(dados_dir, '*.zip'))\n",
        "\n",
        "        for zip_file in tqdm(zip_files, desc='Overall Progress'):\n",
        "            extract_and_process_zip(zip_file, intermediarios_dir)\n",
        "\n",
        "        # Concatenar todos os DataFrames processados\n",
        "        final_df = cudf.concat(final_df_list, ignore_index=True)\n",
        "        save_intermediate_data(final_df, 'intermediate_data.parquet')\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def identify_garage_and_endpoints(df, epsilon=0.001, chunk_size=10000):\n",
        "    if df['datahora_converted'].dtype == 'object':\n",
        "        df['datahora_converted'] = cudf.to_datetime(df['datahora_converted'])\n",
        "\n",
        "    grouped = df.groupby(['linha'])\n",
        "\n",
        "    garage_routes = []\n",
        "    endpoints = []\n",
        "\n",
        "    for linha, group in grouped:\n",
        "        group = group.sort_values(by='datahora_converted')\n",
        "\n",
        "        ordens_unicas = group['ordem'].unique().to_pandas()\n",
        "        for ordem in ordens_unicas:\n",
        "            group_ordem = group[group['ordem'] == ordem].copy()\n",
        "\n",
        "            # Identificar pontos de garagem baseando-se em desvios significativos da média\n",
        "            mean_lat = group_ordem['latitude'].mean()\n",
        "            mean_lon = group_ordem['longitude'].mean()\n",
        "\n",
        "            diff_lat = (group_ordem['latitude'] - mean_lat).abs()\n",
        "            diff_lon = (group_ordem['longitude'] - mean_lon).abs()\n",
        "\n",
        "            garage_route = group_ordem[(diff_lat > epsilon) | (diff_lon > epsilon)]\n",
        "            if not garage_route.empty:\n",
        "                garage_routes.append(garage_route)\n",
        "\n",
        "            # Identificar pontos finais baseando-se em paradas de 10 a 30 minutos\n",
        "            group_ordem['time_diff'] = group_ordem['datahora_converted'].diff().astype('timedelta64[s]').fillna(0).astype('int32')\n",
        "            endpoint = group_ordem[(group_ordem['time_diff'] >= 600) & (group_ordem['time_diff'] <= 1800) & (group_ordem['velocidade'] == 0)]\n",
        "            if not endpoint.empty:\n",
        "                endpoints.append(endpoint)\n",
        "\n",
        "    if len(garage_routes) > 0:\n",
        "        garage_routes = cudf.concat(garage_routes)\n",
        "    else:\n",
        "        garage_routes = cudf.DataFrame()\n",
        "\n",
        "    if len(endpoints) > 0:\n",
        "        endpoints = cudf.concat(endpoints)\n",
        "    else:\n",
        "        endpoints = cudf.DataFrame()\n",
        "\n",
        "    def is_nearby_dbscan(df, points, epsilon, chunk_size):\n",
        "        clustering = DBSCAN(eps=epsilon, min_samples=1)\n",
        "        print(f\"Running DBSCAN on {len(points)} points with epsilon={epsilon}\")\n",
        "\n",
        "        num_rows = df.shape[0]\n",
        "        distances = np.empty(num_rows)\n",
        "        \n",
        "        for start in range(0, num_rows, chunk_size):\n",
        "            end = min(start + chunk_size, num_rows)\n",
        "            chunk = df[['latitude', 'longitude']].iloc[start:end]\n",
        "            clustering.fit(chunk.to_pandas().values)\n",
        "            labels = clustering.labels_\n",
        "            nearest_points = chunk.to_pandas().values[labels == 0]\n",
        "\n",
        "            chunk_distances = ((chunk.to_pandas().values[:, None] - nearest_points) ** 2).sum(axis=2) ** 0.5\n",
        "            distances[start:end] = chunk_distances.min(axis=1)\n",
        "\n",
        "            # Limpa a memória da GPU após cada batch\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        return cudf.Series(distances < epsilon)\n",
        "\n",
        "    if not endpoints.empty:\n",
        "        endpoint_points = endpoints[['latitude', 'longitude']]\n",
        "        df['ponto_final'] = is_nearby_dbscan(df, endpoint_points, epsilon, chunk_size)\n",
        "    else:\n",
        "        df['ponto_final'] = False\n",
        "\n",
        "    if not garage_routes.empty:\n",
        "        garage_points = garage_routes[['latitude', 'longitude']]\n",
        "        df['garagem'] = is_nearby_dbscan(df, garage_points, epsilon, chunk_size)\n",
        "    else:\n",
        "        df['garagem'] = False\n",
        "\n",
        "    return df\n",
        "\n",
        "def update_bus_direction(df):\n",
        "    global sentidos_atual\n",
        "\n",
        "    # Converte o DataFrame para pandas\n",
        "    df = df.to_pandas()\n",
        "\n",
        "    # Ordena o DataFrame pelas colunas 'ordem' e 'datahora_converted'\n",
        "    df = df.sort_values(by=['ordem', 'datahora_converted'])\n",
        "\n",
        "    # Inicializa a coluna 'sentido' como 'indo'\n",
        "    df['sentido'] = 'indo'\n",
        "\n",
        "    # Iteração sobre cada ônibus único\n",
        "    ordens_unicas = df['ordem'].unique()\n",
        "    for ordem in ordens_unicas:\n",
        "        df_ordem = df[df['ordem'] == ordem].copy()\n",
        "\n",
        "        sentido_atual = 'indo'\n",
        "        sentidos = []\n",
        "\n",
        "        for ponto_final in df_ordem['ponto_final']:\n",
        "            if ponto_final:\n",
        "                sentido_atual = 'voltando' if sentido_atual == 'indo' else 'indo'\n",
        "            sentidos.append(sentido_atual)\n",
        "\n",
        "        df.loc[df['ordem'] == ordem, 'sentido'] = sentidos\n",
        "\n",
        "    # Converte de volta para cudf\n",
        "    return cudf.DataFrame.from_pandas(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFnkKdz7onFV"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    start_time = time.time()\n",
        "    mem_usage_before = memory_usage()[0]\n",
        "\n",
        "    final_df = load_or_process_data()\n",
        "\n",
        "    mem_usage_after_load = memory_usage()[0]\n",
        "    load_time = time.time()\n",
        "\n",
        "    intermediate_path_after_loading = os.path.join(intermediarios_dir, 'data_after_loading.parquet')\n",
        "    if not os.path.exists(intermediate_path_after_loading):\n",
        "        save_intermediate_data(final_df, 'data_after_loading.parquet')\n",
        "\n",
        "    final_df = identify_garage_and_endpoints(final_df)\n",
        "\n",
        "    intermediate_path_after_processing = os.path.join(intermediarios_dir, 'data_after_garage_and_endpoints.parquet')\n",
        "    if not os.path.exists(intermediate_path_after_processing):\n",
        "        save_intermediate_data(final_df, 'data_after_garage_and_endpoints.parquet')\n",
        "\n",
        "    mem_usage_after_process = memory_usage()[0]\n",
        "    process_time = time.time()\n",
        "\n",
        "    #final_df['datahora'] = cudf.to_datetime(final_df['datahora'])\n",
        "    final_df['datahoraenvio'] = cudf.to_datetime(final_df['datahoraenvio'])\n",
        "    #final_df['datahora_converted'] = cudf.to_datetime(final_df['datahora'])\n",
        "    final_df['velocidade'] = final_df['velocidade'].astype('uint8')\n",
        "\n",
        "    # Atualizar o sentido dos ônibus\n",
        "    final_df = update_bus_direction(final_df)\n",
        "\n",
        "    # Salvar os dados intermediários após atualizar o sentido dos ônibus\n",
        "    intermediate_path_after_direction_update = os.path.join(intermediarios_dir, 'data_after_direction_update.parquet')\n",
        "    if not os.path.exists(intermediate_path_after_direction_update):\n",
        "        save_intermediate_data(final_df, 'data_after_direction_update.parquet')\n",
        "\n",
        "    save_time = time.time()\n",
        "    final_df.to_csv(os.path.join(processados_dir, 'processed_data.csv'), index=False, chunksize=10000)\n",
        "    final_df.to_parquet(os.path.join(processados_dir, 'processed_data.parquet'), index=False)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f'Tamanho final do DataFrame: {final_df.shape[0]}')\n",
        "    print(f'Memory usage before loading data: {mem_usage_before} MiB')\n",
        "    print(f'Memory usage after loading data: {mem_usage_after_load} MiB')\n",
        "    print(f'Memory usage after processing data: {mem_usage_after_process} MiB')\n",
        "    print(f'Time to load data: {load_time - start_time:.2f} seconds')\n",
        "    print(f'Time to process data: {process_time - load_time:.2f} seconds')\n",
        "    print(f'Time to save data: {end_time - save_time:.2f} seconds')\n",
        "    print(f'Total execution time: {end_time - start_time:.2f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zl1AW3yonCu"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>EDA</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_processed_parquet_df = pd.read_parquet(os.path.join(processados_dir, 'processed_data.parquet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_processed_parquet_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_processed_parquet_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_processed_parquet_df.ponto_final.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Verificando tipo de dados</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_processed_parquet_df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Análise Estatística</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_processed_parquet_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_processed_parquet_df.velocidade.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cudf\n",
        "import numpy as np\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import gc\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EDA:\n",
        "    def __init__(self, file_path):\n",
        "        \"\"\"\n",
        "        Inicializa a classe com o caminho do arquivo de dados processados.\n",
        "        \"\"\"\n",
        "        self.file_path = file_path  \n",
        "\n",
        "    def load_data(self, use_cudf=False):\n",
        "        \"\"\"\n",
        "        Carrega os dados do arquivo Parquet usando pandas ou cudf.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if use_cudf:\n",
        "                return cudf.read_parquet(self.file_path)  # Carrega o DataFrame usando cudf\n",
        "            else:\n",
        "                return pd.read_parquet(self.file_path)  # Carrega o DataFrame usando pandas\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao carregar os dados: {e}\")\n",
        "            return None\n",
        "\n",
        "    def initial_inspection(self):\n",
        "        \"\"\"\n",
        "        Realiza uma inspeção inicial dos dados, visualizando as primeiras linhas e um resumo estatístico.\n",
        "        \"\"\"\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            print(df.describe())  # Exibe um resumo estatístico dos dados\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def analyze_statistics(self):\n",
        "        \"\"\"\n",
        "        Analisa as estatísticas dos dados, incluindo a distribuição das velocidades e a contagem de registros por linha e sentido.\n",
        "        \"\"\"\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            # Plota a distribuição das velocidades\n",
        "            df['velocidade'].hist(bins=50)  # Plota um histograma\n",
        "            plt.title('Distribuição de Velocidades')\n",
        "            plt.xlabel('Velocidade')\n",
        "            plt.ylabel('Frequência')\n",
        "            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "            plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo x\n",
        "            plt.show()\n",
        "\n",
        "            # Conta registros por linha e sentido e exibe os resultados\n",
        "            #print(df.groupby(['linha', 'sentido']).size().to_frame('count').reset_index())\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def analyze_temporal_patterns(self):\n",
        "        \"\"\"\n",
        "        Analisa padrões temporais dos dados, incluindo o volume de dados por hora e padrões de movimentação ao longo do tempo.\n",
        "        \"\"\"\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            # Extrai a hora do timestamp\n",
        "            #df['datahora_converted'] = pd.to_datetime(df['datahora'])\n",
        "            df['hora'] = df['datahora_converted'].dt.hour\n",
        "\n",
        "            # Filtra as horas entre 8 e 22\n",
        "            df = df[(df['hora'] >= 8) & (df['hora'] < 22)]\n",
        "\n",
        "            # Conta registros por hora e plota o volume de dados por hora\n",
        "            hourly_counts = df.groupby('hora').size().to_frame('count').reset_index()\n",
        "            hourly_counts.plot(x='hora', y='count', kind='bar')\n",
        "            plt.title('Volume de Dados por Hora')\n",
        "            plt.xlabel('Hora do Dia')\n",
        "            plt.ylabel('Contagem')\n",
        "            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "            plt.show()\n",
        "\n",
        "            # Conta registros por dia ao longo do tempo e plota padrões de movimentação\n",
        "            df['data'] = df['datahora_converted'].dt.date  # Extrai apenas a data\n",
        "            daily_counts = df.groupby('data').size().to_frame('count').reset_index()\n",
        "            daily_counts.plot(x='data', y='count', kind='line')\n",
        "            plt.title('Padrões de Movimentação ao Longo do Tempo')\n",
        "            plt.xlabel('Data')\n",
        "            plt.ylabel('Contagem')\n",
        "            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Formata o eixo x para datas\n",
        "            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.show()\n",
        "\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def analyze_geographical_patterns(self, linhas=None):\n",
        "        \"\"\"\n",
        "        Analisa padrões geográficos dos dados, plotando um mapa de calor dos trajetos dos ônibus.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df_cudf = self.load_data(use_cudf=True)\n",
        "            if df_cudf is not None:\n",
        "                df_cudf['linha'] = df_cudf['linha'].astype(str)  # Garante que a coluna 'linha' seja string\n",
        "\n",
        "                if linhas:\n",
        "                    linhas = list(map(str, linhas))\n",
        "                    for linha in linhas:\n",
        "                        if linha not in df_cudf['linha'].to_pandas().tolist():\n",
        "                            raise ValueError(f\"Linha {linha} não encontrada nos dados.\")\n",
        "\n",
        "                    df_cudf = df_cudf[df_cudf['linha'].isin(linhas)]\n",
        "\n",
        "                num_linhas = len(linhas) if linhas else len(df_cudf['linha'].unique())\n",
        "                sample_fraction = min(1.0 / num_linhas, 0.01)  # Ajusta a fração de amostra de acordo com o número de linhas\n",
        "                max_points = min(5000 * num_linhas, 50000)  # Ajusta o número máximo de pontos de acordo com o número de linhas\n",
        "\n",
        "                # Define o centro do mapa baseado na média de latitude e longitude usando cudf\n",
        "                map_center = [float(df_cudf['latitude'].mean()), float(df_cudf['longitude'].mean())]\n",
        "                map_osm = folium.Map(location=map_center, zoom_start=12)  # Cria um mapa centrado na localização média\n",
        "\n",
        "                # Amostra os dados antes de converter para pandas para evitar problemas de memória\n",
        "                if len(df_cudf) > max_points:\n",
        "                    df_sample = df_cudf.sample(frac=sample_fraction)\n",
        "                    if len(df_sample) > max_points:\n",
        "                        df_sample = df_sample.sample(n=max_points)\n",
        "                else:\n",
        "                    df_sample = df_cudf\n",
        "\n",
        "                df_pandas = df_sample.to_pandas()\n",
        "\n",
        "                # Dados para o mapa de calor\n",
        "                heat_data = [[row['latitude'], row['longitude']] for index, row in df_pandas[['latitude', 'longitude']].iterrows()]\n",
        "                HeatMap(heat_data).add_to(map_osm)  # Adiciona o mapa de calor ao mapa\n",
        "\n",
        "                title = 'Mapa de Calor de Todas as Linhas'\n",
        "                if linhas:\n",
        "                    title = f'Mapa de Calor das Linhas: {\", \".join(map(str, linhas))}'\n",
        "                map_osm.get_root().html.add_child(folium.Element(f\"<h1>{title}</h1>\"))\n",
        "\n",
        "                map_osm.save('bus_routes.html')  # Salva o mapa como um arquivo HTML\n",
        "                print(f\"Mapa de calor salvo como 'bus_routes.html' com {len(df_pandas)} pontos de amostra\")\n",
        "                \n",
        "                del df_cudf  # Libera a memória\n",
        "                del df_pandas  # Libera a memória\n",
        "                gc.collect()  # Coleta o lixo\n",
        "            else:\n",
        "                print(\"Erro ao carregar os dados.\")\n",
        "        except ValueError as ve:\n",
        "            print(ve)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao gerar o mapa de calor: {e}\")\n",
        "\n",
        "    def plot_routes_and_endpoints(self, linhas=None):\n",
        "        \"\"\"\n",
        "        Plota os trajetos dos ônibus, locais de garagem e pontos finais com diferentes cores.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df_cudf = self.load_data(use_cudf=True)\n",
        "            if df_cudf is not None:\n",
        "                df_cudf['linha'] = df_cudf['linha'].astype(str)  # Garante que a coluna 'linha' seja string\n",
        "\n",
        "                if linhas:\n",
        "                    linhas = list(map(str, linhas))\n",
        "                    for linha in linhas:\n",
        "                        if linha not in df_cudf['linha'].to_pandas().tolist():\n",
        "                            raise ValueError(f\"Linha {linha} não encontrada nos dados.\")\n",
        "\n",
        "                    df_cudf = df_cudf[df_cudf['linha'].isin(linhas)]\n",
        "\n",
        "                num_linhas = len(linhas) if linhas else len(df_cudf['linha'].unique())\n",
        "                sample_fraction = min(1.0 / num_linhas, 0.01)  # Ajusta a fração de amostra de acordo com o número de linhas\n",
        "                max_points = min(5000 * num_linhas, 50000)  # Ajusta o número máximo de pontos de acordo com o número de linhas\n",
        "\n",
        "                # Define o centro do mapa baseado na média de latitude e longitude usando cudf\n",
        "                map_center = [float(df_cudf['latitude'].mean()), float(df_cudf['longitude'].mean())]\n",
        "                map_osm = folium.Map(location=map_center, zoom_start=12)  # Cria um mapa centrado na localização média\n",
        "\n",
        "                # Amostra os dados antes de converter para pandas para evitar problemas de memória\n",
        "                if len(df_cudf) > max_points:\n",
        "                    df_sample = df_cudf.sample(frac=sample_fraction)\n",
        "                    if len(df_sample) > max_points:\n",
        "                        df_sample = df_sample.sample(n=max_points)\n",
        "                else:\n",
        "                    df_sample = df_cudf\n",
        "\n",
        "                df_pandas = df_sample.to_pandas()\n",
        "\n",
        "                # Plota os trajetos normais\n",
        "                normal_routes = df_pandas[~df_pandas['garagem'] & ~df_pandas['ponto_final']]\n",
        "                HeatMap([[row['latitude'], row['longitude']] for index, row in normal_routes[['latitude', 'longitude']].iterrows()],\n",
        "                        name='Trajetos Normais', gradient={0.4: 'blue', 1: 'lightblue'}).add_to(map_osm)\n",
        "\n",
        "                # Plota os locais de garagem\n",
        "                garage_routes = df_pandas[df_pandas['garagem']]\n",
        "                HeatMap([[row['latitude'], row['longitude']] for index, row in garage_routes[['latitude', 'longitude']].iterrows()],\n",
        "                        name='Locais de Garagem', gradient={0.4: 'red', 1: 'darkred'}).add_to(map_osm)\n",
        "\n",
        "                # Plota os pontos finais\n",
        "                end_points = df_pandas[df_pandas['ponto_final']]\n",
        "                HeatMap([[row['latitude'], row['longitude']] for index, row in end_points[['latitude', 'longitude']].iterrows()],\n",
        "                        name='Pontos Finais', gradient={0.4: 'yellow', 1: 'gold'}).add_to(map_osm)\n",
        "\n",
        "                title = 'Mapa de Trajetos, Locais de Garagem e Pontos Finais de Todas as Linhas'\n",
        "                if linhas:\n",
        "                    title = f'Mapa de Trajetos, Locais de Garagem e Pontos Finais das Linhas: {\", \".join(map(str, linhas))}'\n",
        "                map_osm.get_root().html.add_child(folium.Element(f\"<h1>{title}</h1>\"))\n",
        "\n",
        "                map_osm.save('bus_routes_endpoints.html')  # Salva o mapa como um arquivo HTML\n",
        "                print(f\"Mapa de trajetos e pontos salvos como 'bus_routes_endpoints.html' com {len(df_pandas)} pontos de amostra\")\n",
        "\n",
        "                del df_cudf  # Libera a memória\n",
        "                del df_pandas  # Libera a memória\n",
        "                gc.collect()  # Coleta o lixo\n",
        "            else:\n",
        "                print(\"Erro ao carregar os dados.\")\n",
        "        except ValueError as ve:\n",
        "            print(ve)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao gerar o mapa de trajetos e pontos: {e}\")\n",
        "\n",
        "    def analyze_endpoints_garage(self):\n",
        "        \"\"\"\n",
        "        Analisa a distribuição dos pontos finais e pontos de garagem.\n",
        "        \"\"\"\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            # Plota pontos finais\n",
        "            df[df['ponto_final'] == True][['latitude', 'longitude']].plot(kind='scatter', x='longitude', y='latitude')\n",
        "            plt.title('Distribuição de Pontos Finais')\n",
        "            plt.show()\n",
        "\n",
        "            # Plota pontos de garagem\n",
        "            df[df['garagem'] == True][['latitude', 'longitude']].plot(kind='scatter', x='longitude', y='latitude')\n",
        "            plt.title('Distribuição de Pontos de Garagem')\n",
        "            plt.show()\n",
        "\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def analyze_routes(self):\n",
        "        \"\"\"\n",
        "        Analisa os trajetos dos ônibus para cada linha.\n",
        "        \"\"\"\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            linhas = df['linha'].unique()  # Obtém todas as linhas únicas\n",
        "            num_linhas = len(linhas)\n",
        "            num_cols = 3\n",
        "            num_rows = (num_linhas + num_cols - 1) // num_cols  # Calcula o número de linhas para os subplots\n",
        "            fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
        "            axes = axes.flatten()\n",
        "\n",
        "            for idx, linha in enumerate(linhas):\n",
        "                trajeto = df[df['linha'] == linha]  # Filtra os dados pela linha\n",
        "                ax = axes[idx]\n",
        "                ax.plot(trajeto['longitude'], trajeto['latitude'], label=f'Linha {linha}')  # Plota o trajeto\n",
        "                ax.set_title(f'Trajeto da Linha {linha}')\n",
        "                ax.set_xlabel('Longitude')\n",
        "                ax.set_ylabel('Latitude')\n",
        "                ax.legend()\n",
        "                ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "                ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo x\n",
        "\n",
        "            # Remove subplots vazios\n",
        "            for idx in range(len(linhas), len(axes)):\n",
        "                fig.delaxes(axes[idx])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def check_data_quality(self):\n",
        "        \"\"\"\n",
        "        Verifica a qualidade dos dados, incluindo valores nulos, dados inconsistentes e outliers.\n",
        "        \"\"\"\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            # Verifica valores nulos em cada coluna\n",
        "            print(\"Verificando valores nulos...\")\n",
        "            print(df.isnull().sum())\n",
        "\n",
        "            # Verifica valores não numéricos na coluna 'velocidade'\n",
        "            print(\"Verificando dados inconsistentes...\")\n",
        "            problematic_values = df[~df['velocidade'].astype(str).str.isnumeric()]\n",
        "            if not problematic_values.empty:\n",
        "                print(f\"Valores problemáticos em 'velocidade':\\n{problematic_values[['velocidade', 'velocidade']].head()}\")\n",
        "\n",
        "            # Verifica outliers de velocidade\n",
        "            print(\"Verificando outliers de velocidade [Velocidade acima de 150km/h]...\")\n",
        "            outliers = df[(df['velocidade'] < 0) | (df['velocidade'] > 150)]\n",
        "            print(f\"Outliers:\\n{outliers}\")\n",
        "\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def feature_engineering(self, df):\n",
        "        \"\"\"\n",
        "        Cria novas features a partir das informações temporais.\n",
        "        \"\"\"\n",
        "        print(\"Criando novas features...\")\n",
        "        #df['datahora_converted'] = pd.to_datetime(df['datahora'])\n",
        "        df['dia_da_semana'] = df['datahora_converted'].dt.dayofweek  # Adiciona a feature 'dia_da_semana'\n",
        "        df['hora'] = df['datahora_converted'].dt.hour  # Adiciona a feature 'hora'\n",
        "        print(\"Novas features criadas: 'dia_da_semana', 'hora'\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    def run_all(self):\n",
        "        \"\"\"\n",
        "        Executa todos os métodos de análise exploratória de dados em sequência.\n",
        "        \"\"\"\n",
        "        self.initial_inspection()\n",
        "        self.analyze_statistics()\n",
        "        self.analyze_temporal_patterns()\n",
        "        self.analyze_geographical_patterns()\n",
        "        self.plot_routes_and_endpoints()\n",
        "        self.analyze_endpoints_garage()\n",
        "        self.analyze_routes()\n",
        "        self.check_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = os.getcwd()\n",
        "processados_dir = os.path.join(base_dir, 'processados')\n",
        "eda = EDA(file_path=os.path.join(processados_dir, 'processed_data.parquet'))\n",
        "\n",
        "# Carregar dados\n",
        "#eda.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizar inspeção inicial\n",
        "eda.initial_inspection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisar estatísticas\n",
        "eda.analyze_statistics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisar padrões temporais\n",
        "eda.analyze_temporal_patterns()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisar padrões geográficos\n",
        "eda.analyze_geographical_patterns(linhas=[\"324\", \"3\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar trajetos e pontos\n",
        "eda.plot_routes_and_endpoints(linhas=[\"324\", \"3\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisar pontos finais e pontos de garagem\n",
        "eda.analyze_endpoints_garage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisar trajetos dos ônibus\n",
        "eda.analyze_routes()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar qualidade dos dados\n",
        "eda.check_data_quality()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizar validação cruzada temporal\n",
        "#eda.temporal_cross_validation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Data Processing</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Para não sobreescrever o dataframe processado final, irei criar um novo dataframe que sofrerá as devidas mudanças.</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "from cuml.cluster import DBSCAN\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suprime os avisos de SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataPreprocessing:\n",
        "    def __init__(self, file_path, intermediarios_dir, processados_dir, chunk_size=1000000):\n",
        "        self.file_path = file_path\n",
        "        self.intermediarios_dir = intermediarios_dir\n",
        "        self.processados_dir = processados_dir\n",
        "        self.cleaned_file = os.path.join(intermediarios_dir, 'cleaned_data.parquet')\n",
        "        self.prepared_file = os.path.join(processados_dir, 'prepared_data_hist.parquet')\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def load_data(self):\n",
        "        try:\n",
        "            print(\"Carregando dados...\")\n",
        "            df = cudf.read_parquet(self.file_path)\n",
        "            if 'datahora' not in df.columns:\n",
        "                raise KeyError(\"A coluna 'datahora' não está presente nos dados carregados.\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao carregar os dados: {e}\")\n",
        "            return None\n",
        "\n",
        "    def save_intermediate_data(self, df, filename):\n",
        "        print(f\"Salvando dados intermediários em '{filename}'...\")\n",
        "        df.to_parquet(filename)\n",
        "        print(f\"Dados salvos como '{filename}'\")\n",
        "\n",
        "    def remove_outliers(self, eps=0.001, min_samples=10):\n",
        "        if os.path.exists(self.cleaned_file):\n",
        "            print(f\"Arquivo intermediário encontrado. Carregando dados de '{self.cleaned_file}'\")\n",
        "            df = cudf.read_parquet(self.cleaned_file)\n",
        "            if 'datahora' not in df.columns:\n",
        "                raise KeyError(\"A coluna 'datahora' não está presente nos dados intermediários carregados.\")\n",
        "            return df\n",
        "\n",
        "        df = self.load_data()\n",
        "        if df is not None:\n",
        "            if 'datahora_converted' not in df.columns or 'linha' not in df.columns or 'ordem' not in df.columns:\n",
        "                raise KeyError(\"As colunas necessárias não estão presentes nos dados.\")\n",
        "\n",
        "            lines = df['linha'].unique().to_pandas()\n",
        "            cleaned_dfs = []\n",
        "\n",
        "            print(\"Removendo outliers...\")\n",
        "            for linha in tqdm(lines, desc=\"Linhas\"):\n",
        "                line_data = df[df['linha'] == linha][['latitude', 'longitude', 'datahora_converted', 'linha', 'ordem', 'datahora']].copy()\n",
        "                if len(line_data) < min_samples:\n",
        "                    cleaned_dfs.append(line_data)\n",
        "                    continue\n",
        "\n",
        "                # Converte para pandas para DBSCAN\n",
        "                line_data_pd = line_data[['latitude', 'longitude']].to_pandas()\n",
        "\n",
        "                db = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "                labels = db.fit_predict(line_data_pd)\n",
        "\n",
        "                # Adiciona os rótulos ao DataFrame\n",
        "                line_data_pd['labels'] = labels\n",
        "\n",
        "                # Filtra os pontos que não são outliers\n",
        "                non_outliers = line_data_pd[line_data_pd['labels'] != -1]\n",
        "                non_outliers = non_outliers.copy()  # Para evitar SettingWithCopyWarning\n",
        "                non_outliers.loc[:, 'datahora_converted'] = line_data['datahora_converted'].to_pandas().loc[non_outliers.index]\n",
        "                non_outliers.loc[:, 'linha'] = line_data['linha'].to_pandas().loc[non_outliers.index]\n",
        "                non_outliers.loc[:, 'ordem'] = line_data['ordem'].to_pandas().loc[non_outliers.index]\n",
        "                non_outliers.loc[:, 'datahora'] = line_data['datahora'].to_pandas().loc[non_outliers.index]\n",
        "\n",
        "                cleaned_dfs.append(non_outliers)\n",
        "\n",
        "            cleaned_df = cudf.DataFrame.from_pandas(pd.concat(cleaned_dfs, ignore_index=True))\n",
        "            self.save_intermediate_data(cleaned_df, self.cleaned_file)\n",
        "\n",
        "            if 'datahora' not in cleaned_df.columns:\n",
        "                raise KeyError(\"A coluna 'datahora' foi removida durante a remoção de outliers.\")\n",
        "\n",
        "            return cleaned_df\n",
        "        else:\n",
        "            print(\"Erro ao carregar os dados para remoção de outliers.\")\n",
        "            return None\n",
        "\n",
        "    def feature_engineering(self, df):\n",
        "        print(\"Criando novas features...\")\n",
        "\n",
        "        if 'datahora_converted' not in df.columns or 'linha' not in df.columns or 'ordem' not in df.columns:\n",
        "            raise KeyError(\"As colunas necessárias não estão presentes nos dados.\")\n",
        "\n",
        "        df = df.sort_values(by=['linha', 'ordem', 'datahora_converted'])\n",
        "\n",
        "        df['dia_da_semana'] = df['datahora_converted'].dt.weekday\n",
        "        df['hora'] = df['datahora_converted'].dt.hour\n",
        "        df['diff_timestamp'] = df.groupby(['linha', 'ordem'])['datahora_converted'].diff().astype('int64') / 10**6\n",
        "\n",
        "        df['latitude_diff'] = df.groupby(['linha', 'ordem'])['latitude'].diff().fillna(method='ffill')\n",
        "        df['longitude_diff'] = df.groupby(['linha', 'ordem'])['longitude'].diff().fillna(method='ffill')\n",
        "\n",
        "        df_pandas = df[['latitude', 'longitude', 'latitude_diff', 'longitude_diff']].to_pandas()\n",
        "        lat1 = np.radians(df_pandas['latitude'])\n",
        "        lon1 = np.radians(df_pandas['longitude'])\n",
        "        lat2 = np.radians(df_pandas['latitude'] + df_pandas['latitude_diff'])\n",
        "        lon2 = np.radians(df_pandas['longitude'] + df_pandas['longitude_diff'])\n",
        "\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "\n",
        "        a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "        r = 6371\n",
        "        df_pandas['distancia'] = c * r * 1000\n",
        "\n",
        "        df['distancia'] = cudf.from_pandas(df_pandas['distancia'])\n",
        "\n",
        "        print(\"Novas features criadas: 'dia_da_semana', 'hora', 'diff_timestamp', 'latitude_diff', 'longitude_diff', 'distancia'\")\n",
        "\n",
        "        if 'datahora' not in df.columns:\n",
        "            raise KeyError(\"A coluna 'datahora' foi removida durante a engenharia de features.\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_data_for_prediction(self):\n",
        "        if os.path.exists(self.prepared_file):\n",
        "            print(f\"Arquivo intermediário encontrado. Carregando dados de '{self.prepared_file}'\")\n",
        "            df = cudf.read_parquet(self.prepared_file)\n",
        "            if 'datahora' not in df.columns:\n",
        "                raise KeyError(\"A coluna 'datahora' não está presente nos dados intermediários carregados.\")\n",
        "            return df\n",
        "\n",
        "        df = self.remove_outliers()\n",
        "        if df is not None:\n",
        "            df = self.feature_engineering(df)\n",
        "            self.save_intermediate_data(df, self.prepared_file)\n",
        "\n",
        "            if 'datahora' not in df.columns:\n",
        "                raise KeyError(\"A coluna 'datahora' foi removida durante o processamento.\")\n",
        "            return df\n",
        "        else:\n",
        "            print(\"Erro ao preparar os dados para previsão.\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = os.getcwd()\n",
        "intermediarios_dir = os.path.join(base_dir, 'intermediarios')\n",
        "processados_dir = os.path.join(base_dir, 'processados')\n",
        "preprocessing = DataPreprocessing(os.path.join(processados_dir, 'processed_data.parquet'), intermediarios_dir, processados_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remover outliers e preparar dados\n",
        "prepared_data = preprocessing.prepare_data_for_prediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Pré processando dados de teste</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cudf\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import json\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from sklearn.cluster import DBSCAN\n",
        "import warnings\n",
        "# Suprimindo warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestDataProcessor:\n",
        "    def __init__(self, base_dir):\n",
        "        self.base_dir = os.path.join(base_dir, 'dados_teste')\n",
        "        self.intermediarios_teste_dir = os.path.join(self.base_dir, 'intermediarios_teste')\n",
        "        self.processados_teste_dir = os.path.join(self.base_dir, 'processados_teste')\n",
        "        self.chunk_size = 100000\n",
        "\n",
        "        os.makedirs(self.intermediarios_teste_dir, exist_ok=True)\n",
        "        os.makedirs(self.processados_teste_dir, exist_ok=True)\n",
        "\n",
        "    def extract_zip_files(self, zip_files):\n",
        "        for zip_file in tqdm(zip_files, desc=\"Extracting zip files\"):\n",
        "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.intermediarios_teste_dir)\n",
        "\n",
        "    def process_json(self, json_path, df_list):\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        df = cudf.DataFrame.from_pandas(pd.DataFrame(data))\n",
        "        df_list.append(df)\n",
        "\n",
        "    def process_and_concat_files(self, prefix, output_parquet):\n",
        "        all_files = []\n",
        "        for root, _, files in os.walk(self.intermediarios_teste_dir):\n",
        "            for file in files:\n",
        "                if file.startswith(prefix) and file.endswith('.json'):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "        \n",
        "        final_df_list = []\n",
        "        for json_file in tqdm(all_files, desc=f\"Processing {prefix} files\"):\n",
        "            self.process_json(json_file, final_df_list)\n",
        "\n",
        "        if final_df_list:\n",
        "            final_df = cudf.concat(final_df_list, ignore_index=True)\n",
        "            final_df.to_parquet(output_parquet)\n",
        "            print(f\"Saved concatenated {prefix} data to {output_parquet}\")\n",
        "        else:\n",
        "            print(f\"No {prefix} JSON files found.\")\n",
        "            final_df = None\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    def process_recent_data_iteratively(self, prefix, output_csv):\n",
        "        all_files = []\n",
        "        for root, _, files in os.walk(self.intermediarios_teste_dir):\n",
        "            for file in files:\n",
        "                if file.startswith(prefix) and file.endswith('.json'):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "\n",
        "        final_df_list = []\n",
        "        for json_file in tqdm(all_files, desc=f\"Processing {prefix} files\"):\n",
        "            self.process_and_append_json(json_file, final_df_list, output_csv)\n",
        "\n",
        "        if final_df_list:\n",
        "            final_df = cudf.concat(final_df_list, ignore_index=True)\n",
        "        else:\n",
        "            final_df = None\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    def process_and_append_json(self, json_path, df_list, output_csv):\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        df = cudf.DataFrame.from_pandas(pd.DataFrame(data))\n",
        "\n",
        "        # Processando dados recentes\n",
        "        linhas_validas = [483, 864, 639, 3, 309, 774, 629, 371, 397, 100, 838, 315, 624, 388, 918, 665, 328, 497, 878, 355, 138, 606, 457, 550, 803, 917, 638, 2336, 399, 298, 867, 553, 565, 422, 756, 186012003, 292, 554, 634, 232, 415, 2803, 324, 852, 557, 759, 343, 779, 905, 108]\n",
        "        df = df[df['linha'].astype(str).isin(map(str, linhas_validas))]\n",
        "        \n",
        "        df['datahoraenvio'] = cudf.to_datetime(df['datahoraenvio'].astype('int64'), unit='ms')\n",
        "        df['datahora_converted'] = cudf.to_datetime(df['datahora'].astype('int64'), unit='ms')\n",
        "        df = df[(df['datahora_converted'].dt.hour >= 8) & (df['datahora_converted'].dt.hour < 22)]\n",
        "\n",
        "        df['latitude'] = df['latitude'].str.replace(',', '.').astype('float32')\n",
        "        df['longitude'] = df['longitude'].str.replace(',', '.').astype('float32')\n",
        "\n",
        "        while df['latitude'].isnull().any() or df['longitude'].isnull().any():\n",
        "            df['latitude'] = df['latitude'].interpolate().ffill().bfill()\n",
        "            df['longitude'] = df['longitude'].interpolate().ffill().bfill()\n",
        "\n",
        "        df['velocidade'] = df['velocidade'].astype('int32')\n",
        "        df = df[(df['velocidade'] >= 0) & (df['velocidade'] <= 250)]\n",
        "        df = df[df['latitude'].between(-90, 90) & df['longitude'].between(-180, 180)]\n",
        "\n",
        "        df_list.append(df)\n",
        "\n",
        "        # Salva intermediário iterativamente em CSV\n",
        "        if os.path.exists(output_csv):\n",
        "            df.to_pandas().to_csv(output_csv, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            df.to_pandas().to_csv(output_csv, index=False)\n",
        "\n",
        "    def run(self):\n",
        "        zip_files = glob(os.path.join(self.base_dir, '*.zip'))\n",
        "        \n",
        "        # Debug: print o diretório atual e os arquivos encontrados\n",
        "        print(f\"Procurando arquivos zip em: {self.base_dir}\")\n",
        "        print(f\"Arquivos zip encontrados: {zip_files}\")\n",
        "\n",
        "        if not zip_files:\n",
        "            raise FileNotFoundError(\"No zip files found in the specified directory.\")\n",
        "\n",
        "        self.extract_zip_files(zip_files)\n",
        "\n",
        "        treino_intermediate_parquet = os.path.join(self.intermediarios_teste_dir, 'intermediate_treino_data.parquet')\n",
        "        resposta_intermediate_parquet = os.path.join(self.intermediarios_teste_dir, 'intermediate_resposta_data.parquet')\n",
        "        recent_intermediate_csv = os.path.join(self.intermediarios_teste_dir, 'intermediate_recent_data.csv')\n",
        "        recent_intermediate_parquet = os.path.join(self.intermediarios_teste_dir, 'intermediate_recent_data.parquet')\n",
        "        recent_raw_intermediate_parquet = os.path.join(self.intermediarios_teste_dir, 'intermediate_recent_raw_data.parquet')\n",
        "\n",
        "        # Concatenando arquivos de treino e resposta\n",
        "        #treino_data_df = self.process_and_concat_files('treino', treino_intermediate_parquet)\n",
        "        #resposta_data_df = self.process_and_concat_files('resposta', resposta_intermediate_parquet)\n",
        "\n",
        "        # Processamento iterativo dos dados recentes\n",
        "        if os.path.exists(recent_intermediate_csv):\n",
        "            recent_data_df = cudf.read_csv(recent_intermediate_csv)\n",
        "        else:\n",
        "            recent_data_df = self.process_recent_data_iteratively('2024', recent_intermediate_csv)\n",
        "\n",
        "        # Salvar como parquet após processamento iterativo\n",
        "        if recent_data_df is not None:\n",
        "            recent_data_df.to_parquet(recent_intermediate_parquet)\n",
        "\n",
        "        # Salvar dados recentes sem processamento\n",
        "        if os.path.exists(recent_intermediate_csv):\n",
        "            recent_raw_data_df = cudf.read_csv(recent_intermediate_csv)\n",
        "            recent_raw_data_df.to_parquet(recent_raw_intermediate_parquet)\n",
        "            print(f\"Saved raw recent data to {recent_raw_intermediate_parquet}\")\n",
        "\n",
        "        data_preprocessor = DataPreprocessing(\n",
        "            file_path=recent_intermediate_parquet,\n",
        "            intermediarios_dir=self.intermediarios_teste_dir,\n",
        "            processados_dir=self.processados_teste_dir,\n",
        "            chunk_size=self.chunk_size\n",
        "        )\n",
        "\n",
        "        data_preprocessor.prepare_data_for_prediction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    base_dir = os.getcwd()\n",
        "    processor = TestDataProcessor(base_dir)\n",
        "    processor.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Ajustar dataset de treino e de resposta</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cudf \n",
        "\n",
        "base_dir = os.getcwd()\n",
        "dados_teste_dir = os.path.join(base_dir, 'dados_teste')\n",
        "intermediarios_teste_dir = os.path.join(dados_teste_dir, 'intermediarios_teste')\n",
        "processados_teste_dir = os.path.join(dados_teste_dir, 'processados_teste')\n",
        "\n",
        "#Path for loading\n",
        "path = os.path.join(intermediarios_teste_dir, \"intermediate_treino_data.parquet\")\n",
        "path2 = os.path.join(intermediarios_teste_dir, \"intermediate_resposta_data.parquet\")\n",
        "\n",
        "#Path for saving the data for not null datahora\n",
        "path3_datahora = os.path.join(processados_teste_dir, \"intermediate_treino_data_datahora.parquet\")\n",
        "path4_datahora = os.path.join(processados_teste_dir, \"intermediate_resposta_data_datahora.parquet\")\n",
        "\n",
        "#Path for saving the data for not null lat/long\n",
        "path3_lat_long = os.path.join(processados_teste_dir, \"intermediate_treino_data_lat_long.parquet\")\n",
        "path4_lat_long = os.path.join(processados_teste_dir, \"intermediate_resposta_data_lat_long.parquet\")\n",
        "\n",
        "treino = cudf.read_parquet(path)\n",
        "resposta = cudf.read_parquet(path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "treino[treino['datahora'].isna() == False].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Criando 2 diferentes datasets. Um para ser usado para a previsão de latitude e \n",
        "longitude (possui a coluna datahora) e outro para a previsão de datahora (possui latitude e longitude)</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Mudanças para o dataset de treino\n",
        "treino_horadata = treino.copy()\n",
        "treino_lat_long = treino.copy()\n",
        "\n",
        "treino_horadata.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
        "treino_horadata.dropna(subset=['datahora'],inplace=True)\n",
        "\n",
        "treino_lat_long.drop(['datahora'], axis=1, inplace=True)\n",
        "treino_lat_long.dropna(subset=['latitude', 'longitude'],inplace=True)\n",
        "\n",
        "#Mudanças para o dataset de resposta\n",
        "resposta_horadata = resposta.copy()\n",
        "resposta_lat_long = resposta.copy()\n",
        "\n",
        "resposta_horadata.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
        "resposta_horadata.dropna(subset=['datahora'],inplace=True)\n",
        "\n",
        "resposta_lat_long.drop(['datahora'], axis=1, inplace=True)\n",
        "resposta_lat_long.dropna(subset=['latitude', 'longitude'],inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "treino_horadata.to_parquet(path3_datahora)\n",
        "treino_lat_long.to_parquet(path3_lat_long)\n",
        "\n",
        "resposta_horadata.to_parquet(path4_datahora)\n",
        "resposta_lat_long.to_parquet(path4_lat_long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#intermediate_resposta_data_datahora\n",
        "resposta_horadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#intermediate_resposta_data_lat_long\n",
        "resposta_lat_long.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "treino_horadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "treino_lat_long.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "treino.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#treino_horadata será usado para prever resposta_lat_long\n",
        "treino_horadata.shape, resposta_lat_long.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#treino_lat_long será usado para prever resposta_hora_data\n",
        "treino_lat_long.shape, resposta_horadata.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Previsão de duas métricas diferentes: Datahora e Latitude/Longitude</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import optuna\n",
        "from optuna.integration import LightGBMPruningCallback\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuração de logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def haversine_rmse(y_true, y_pred):\n",
        "    R = 6371  # Radius of the Earth in kilometers\n",
        "    lat1 = np.radians(y_true[:, 0])\n",
        "    lon1 = np.radians(y_true[:, 1])\n",
        "    lat2 = np.radians(y_pred[:, 0])\n",
        "    lon2 = np.radians(y_pred[:, 1])\n",
        "\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return np.sqrt(np.mean(distance**2))\n",
        "\n",
        "def objective_xgb(trial, X, y, is_location):\n",
        "    param = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 200),\n",
        "        'tree_method': 'gpu_hist',\n",
        "        'predictor': 'gpu_predictor'\n",
        "    }\n",
        "    model = XGBRegressor(**param)\n",
        "    if is_location:\n",
        "        model = MultiOutputRegressor(model)\n",
        "    scores = []\n",
        "    kfold = TimeSeriesSplit(n_splits=5)\n",
        "    for train_index, test_index in kfold.split(X):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "        try:\n",
        "            logging.info(f\"X_test: {X_test.head()}\")\n",
        "            logging.info(f\"y_test: {y_test.head()}\")\n",
        "            logging.info(f\"Predictions: {preds[:5]}\")\n",
        "            \n",
        "            if np.any(np.isnan(preds)):\n",
        "                logging.error(f\"NaN values found in predictions: {preds}\")\n",
        "                raise ValueError(\"NaN values found in predictions.\")\n",
        "            if not all(dtype == 'int' for dtype in y_test.dtypes) and not is_location:\n",
        "                logging.error(f\"y_test data type is not int: {y_test.dtypes}\")\n",
        "                raise ValueError(f\"y_test data type is not int: {y_test.dtypes}\")\n",
        "            if is_location:\n",
        "                score = haversine_rmse(y_test.values, preds)\n",
        "            else:\n",
        "                y_test_converted = y_test.values.astype(np.int64) // 10**9\n",
        "                preds_converted = preds.astype(np.int64) // 10**9\n",
        "                score = np.sqrt(mean_squared_error(y_test_converted, preds_converted))\n",
        "            logging.info(f\"Score: {score}\")\n",
        "            scores.append(score)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in calculating score: {e}\")\n",
        "            raise e\n",
        "    return np.mean(scores)\n",
        "\n",
        "def objective_lgbm(trial, X, y, is_location):\n",
        "    param = {\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 127),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 200),\n",
        "        'device': 'cuda',\n",
        "        'gpu_platform_id': 0,\n",
        "        'gpu_device_id': 0\n",
        "    }\n",
        "    model = LGBMRegressor(**param)\n",
        "    if is_location:\n",
        "        model = MultiOutputRegressor(model)\n",
        "    scores = []\n",
        "    kfold = TimeSeriesSplit(n_splits=5)\n",
        "    for train_index, test_index in kfold.split(X):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "        try:\n",
        "            logging.info(f\"X_test: {X_test.head()}\")\n",
        "            logging.info(f\"y_test: {y_test.head()}\")\n",
        "            logging.info(f\"Predictions: {preds[:5]}\")\n",
        "            \n",
        "            if np.any(np.isnan(preds)):\n",
        "                logging.error(f\"NaN values found in predictions: {preds}\")\n",
        "                raise ValueError(\"NaN values found in predictions.\")\n",
        "            if not all(dtype == 'int' for dtype in y_test.dtypes) and not is_location:\n",
        "                logging.error(f\"y_test data type is not int: {y_test.dtypes}\")\n",
        "                raise ValueError(f\"y_test data type is not int: {y_test.dtypes}\")\n",
        "            if is_location:\n",
        "                score = haversine_rmse(y_test.values, preds)\n",
        "            else:\n",
        "                y_test_converted = y_test.values.astype(np.int64) // 10**9\n",
        "                preds_converted = preds.astype(np.int64) // 10**9\n",
        "                score = np.sqrt(mean_squared_error(y_test_converted, preds_converted))\n",
        "            logging.info(f\"Score: {score}\")\n",
        "            scores.append(score)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in calculating score: {e}\")\n",
        "            raise e\n",
        "    return np.mean(scores)\n",
        "\n",
        "def run_optimized_models(df, target_cols, save_dir, is_location=False):\n",
        "    X = df.drop(columns=target_cols)\n",
        "    y = df[target_cols]\n",
        "\n",
        "    model_definitions = {\n",
        "        'XGBoost': XGBRegressor(tree_method='gpu_hist', predictor='gpu_predictor', gpu_id=0, n_jobs=-1),\n",
        "        'LightGBM': LGBMRegressor(device='cuda', gpu_platform_id=0, gpu_device_id=0, force_row_wise=True, n_jobs=-1)\n",
        "    }\n",
        "\n",
        "    best_params = {}\n",
        "    model_results = {}\n",
        "    rmse_scores = {'XGBoost': [], 'LightGBM': []}\n",
        "\n",
        "    for model_name, model in tqdm(model_definitions.items(), desc=\"Optimizing models\"):\n",
        "        model_path = os.path.join(save_dir, f\"{model_name}_best_model.pkl\")\n",
        "        if os.path.exists(model_path):\n",
        "            logging.info(f\"Model {model_name} already exists. Loading from file.\")\n",
        "            best_model = joblib.load(model_path)\n",
        "            model_results[model_name] = best_model\n",
        "            continue\n",
        "\n",
        "        if model_name == 'XGBoost':\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "            for _ in tqdm(range(10), desc=f\"Optuna Trials for {model_name}\"):\n",
        "                study.optimize(lambda trial: objective_xgb(trial, X, y, is_location), n_trials=1)\n",
        "            best_params[model_name] = study.best_params\n",
        "            best_params[model_name].pop('gpu_id', None)\n",
        "            best_params[model_name].pop('n_jobs', None)\n",
        "            best_model = XGBRegressor(**best_params[model_name], gpu_id=0, n_jobs=-1)\n",
        "            if is_location:\n",
        "                best_model = MultiOutputRegressor(best_model)\n",
        "        else:\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "            for _ in tqdm(range(10), desc=f\"Optuna Trials for {model_name}\"):\n",
        "                study.optimize(lambda trial: objective_lgbm(trial, X, y, is_location), n_trials=1)\n",
        "            best_params[model_name] = study.best_params\n",
        "            best_params[model_name].pop('device', None)\n",
        "            best_params[model_name].pop('gpu_platform_id', None)\n",
        "            best_params[model_name].pop('gpu_device_id', None)\n",
        "            best_model = LGBMRegressor(**best_params[model_name], device='cuda', gpu_platform_id=0, gpu_device_id=0, force_row_wise=True, n_jobs=-1)\n",
        "            if is_location:\n",
        "                best_model = MultiOutputRegressor(best_model)\n",
        "\n",
        "        kfold = TimeSeriesSplit(n_splits=10)\n",
        "        for train_index, test_index in kfold.split(X):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "            best_model.fit(X_train, y_train)\n",
        "            preds = best_model.predict(X_test)\n",
        "            try:\n",
        "                if np.any(np.isnan(preds)):\n",
        "                    logging.error(f\"NaN values found in predictions for {model_name}: {preds}\")\n",
        "                    raise ValueError(\"NaN values found in predictions.\")\n",
        "                if not all(dtype == 'int' for dtype in y_test.dtypes) and not is_location:\n",
        "                    logging.error(f\"y_test data type is not int for {model_name}: {y_test.dtypes}\")\n",
        "                    raise ValueError(f\"y_test data type is not int for {model_name}: {y_test.dtypes}\")\n",
        "                if is_location:\n",
        "                    score = haversine_rmse(y_test.values, preds)\n",
        "                else:\n",
        "                    y_test_converted = y_test.values.astype(np.int64) // 10**9\n",
        "                    preds_converted = preds.astype(np.int64) // 10**9\n",
        "                    score = np.sqrt(mean_squared_error(y_test_converted, preds_converted))\n",
        "                rmse_scores[model_name].append(score)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in calculating score for {model_name}: {e}\")\n",
        "                raise e\n",
        "\n",
        "        joblib.dump(best_model, model_path)\n",
        "        model_results[model_name] = best_model\n",
        "\n",
        "    mean_rmse_scores = {model: np.mean(scores) for model, scores in rmse_scores.items() if scores}\n",
        "    best_model_name = min(mean_rmse_scores, key=mean_rmse_scores.get) if mean_rmse_scores else None\n",
        "    logging.info(f\"Mean RMSE scores: {mean_rmse_scores}\")\n",
        "    logging.info(f\"Best model: {best_model_name} with mean score: {mean_rmse_scores.get(best_model_name)}\")\n",
        "    \n",
        "    return model_results, best_model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_final_model(df, target_cols, best_model_name, best_params, save_dir, is_location=False):\n",
        "    X = df.drop(columns=target_cols)\n",
        "    y = df[target_cols]\n",
        "\n",
        "    if best_model_name == 'XGBoost':\n",
        "        best_params.pop('gpu_id', None)\n",
        "        best_params.pop('n_jobs', None)\n",
        "        best_model = XGBRegressor(**best_params, gpu_id=0, n_jobs=-1)\n",
        "        if is_location:\n",
        "            best_model = MultiOutputRegressor(best_model)\n",
        "    else:\n",
        "        best_params.pop('device', None)\n",
        "        best_params.pop('gpu_platform_id', None)\n",
        "        best_params.pop('gpu_device_id', None)\n",
        "        best_model = LGBMRegressor(**best_params, device='cuda', gpu_platform_id=0, gpu_device_id=0, force_row_wise=True, n_jobs=-1)\n",
        "        if is_location:\n",
        "            best_model = MultiOutputRegressor(best_model)\n",
        "\n",
        "    best_model.fit(X, y)\n",
        "    joblib.dump(best_model, os.path.join(save_dir, f\"final_{best_model_name}_model.pkl\"))\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_and_submit(model, recent_data, submission_path, predict_cols):\n",
        "    preds = model.predict(recent_data.drop(columns=predict_cols))\n",
        "    for idx, col in enumerate(predict_cols):\n",
        "        recent_data[col] = preds[:, idx] if preds.ndim > 1 else preds\n",
        "    recent_data.to_parquet(submission_path, index=False)\n",
        "\n",
        "def plot_differences(df_real, df_pred, columns, title, ylabel, output_path):\n",
        "    differences = df_real[columns] - df_pred[columns]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(differences)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    base_dir = os.getcwd()\n",
        "    dados_teste_dir = os.path.join(base_dir, 'dados_teste', 'processados_teste')\n",
        "    resultados_dir = os.path.join(dados_teste_dir, 'resultados')\n",
        "    submissoes_dir = os.path.join(dados_teste_dir, 'submissoes')\n",
        "    os.makedirs(resultados_dir, exist_ok=True)\n",
        "    os.makedirs(submissoes_dir, exist_ok=True)\n",
        "\n",
        "    arquivos = [\n",
        "        'prepared_data_hist.parquet',\n",
        "        'intermediate_treino_data_datahora.parquet',\n",
        "        'intermediate_treino_data_lat_long.parquet',\n",
        "        'intermediate_resposta_data_datahora.parquet',\n",
        "        'intermediate_resposta_data_lat_long.parquet'\n",
        "    ]\n",
        "\n",
        "    arquivos_existentes = {arquivo: os.path.exists(os.path.join(dados_teste_dir, arquivo)) for arquivo in arquivos}\n",
        "\n",
        "    if all(arquivos_existentes.values()):\n",
        "        logging.info(\"Todos os arquivos necessários já existem. Carregando os dados...\")\n",
        "        df_hist = pd.read_parquet(os.path.join(dados_teste_dir, 'prepared_data_hist.parquet'))\n",
        "        df_treino_datahora = pd.read_parquet(os.path.join(dados_teste_dir, 'intermediate_treino_data_datahora.parquet'))\n",
        "        df_treino_lat_long = pd.read_parquet(os.path.join(dados_teste_dir, 'intermediate_treino_data_lat_long.parquet'))\n",
        "        df_resposta_datahora = pd.read_parquet(os.path.join(dados_teste_dir, 'intermediate_resposta_data_datahora.parquet'))\n",
        "        df_resposta_lat_long = pd.read_parquet(os.path.join(dados_teste_dir, 'intermediate_resposta_data_lat_long.parquet'))\n",
        "\n",
        "        # Prever datahora baseado em latitude e longitude\n",
        "        df_lat_long = df_hist[['latitude', 'longitude', 'datahora']]\n",
        "        na_cols = df_lat_long.columns[df_lat_long.isna().any()].tolist()\n",
        "        if na_cols:\n",
        "            raise ValueError(f\"NaNs found in data for 'Prever datahora': {na_cols}\")\n",
        "\n",
        "        df_lat_long['datahora'] = pd.to_datetime(df_lat_long['datahora']).view(np.int64) // 10**9\n",
        "        df_treino_lat_long['datahora'] = pd.to_datetime(df_treino_lat_long['datahora']).view(np.int64) // 10**9\n",
        "\n",
        "        model_results, best_model_name = run_optimized_models(df_lat_long.copy(), target_cols=['datahora'], save_dir=resultados_dir)\n",
        "        best_model = create_final_model(df_lat_long.copy(), target_cols=['datahora'], best_model_name=best_model_name, best_params=model_results[best_model_name].get_params(), save_dir=resultados_dir)\n",
        "        prepare_and_submit(best_model, df_treino_lat_long.copy(), os.path.join(resultados_dir, 'previsto_treino_data_datahora.parquet'), predict_cols=['datahora'])\n",
        "\n",
        "        # Prever latitude e longitude baseado em datahora\n",
        "        df_datahora = df_hist[['datahora', 'latitude', 'longitude']]\n",
        "        na_cols = df_datahora.columns[df_datahora.isna().any()].tolist()\n",
        "        if na_cols:\n",
        "            raise ValueError(f\"NaNs found in data for 'Prever latitude e longitude': {na_cols}\")\n",
        "\n",
        "        df_datahora['datahora'] = pd.to_datetime(df_datahora['datahora']).view(np.int64) // 10**9\n",
        "        df_treino_datahora['datahora'] = pd.to_datetime(df_treino_datahora['datahora']).view(np.int64) // 10**9\n",
        "\n",
        "        model_results, best_model_name = run_optimized_models(df_datahora.copy(), target_cols=['latitude', 'longitude'], save_dir=resultados_dir, is_location=True)\n",
        "        best_model = create_final_model(df_datahora.copy(), target_cols=['latitude', 'longitude'], best_model_name=best_model_name, best_params=model_results[best_model_name].get_params(), save_dir=resultados_dir, is_location=True)\n",
        "        prepare_and_submit(best_model, df_treino_datahora.copy(), os.path.join(resultados_dir, 'previsto_treino_data_lat_long.parquet'), predict_cols=['latitude', 'longitude'])\n",
        "\n",
        "        # Criar submissão\n",
        "        df_previsto_datahora = pd.read_parquet(os.path.join(resultados_dir, 'previsto_treino_data_datahora.parquet'))\n",
        "        df_previsto_lat_long = pd.read_parquet(os.path.join(resultados_dir, 'previsto_treino_data_lat_long.parquet'))\n",
        "\n",
        "        submissao_datahora = df_resposta_datahora.merge(df_previsto_datahora[['id', 'datahora']], on='id', how='left')\n",
        "        submissao_datahora['datahora'] = pd.to_datetime(submissao_datahora['datahora'] * 10**9)\n",
        "        submissao_datahora.to_parquet(os.path.join(submissoes_dir, 'submissao_datahora.parquet'), index=False)\n",
        "\n",
        "        submissao_lat_long = df_resposta_lat_long.merge(df_previsto_lat_long[['id', 'latitude', 'longitude']], on='id', how='left')\n",
        "        submissao_lat_long.to_parquet(os.path.join(submissoes_dir, 'submissao_lat_long.parquet'), index=False)\n",
        "\n",
        "        # Plotar diferenças\n",
        "        df_resposta_datahora['datahora'] = pd.to_datetime(df_resposta_datahora['datahora'])\n",
        "        df_previsto_datahora['datahora'] = pd.to_datetime(df_previsto_datahora['datahora'] * 10**9)\n",
        "        df_resposta_lat_long['datahora'] = pd.to_datetime(df_resposta_lat_long['datahora'])\n",
        "        df_previsto_lat_long['datahora'] = pd.to_datetime(df_previsto_lat_long['datahora'] * 10**9)\n",
        "\n",
        "        plot_differences(df_resposta_datahora, df_previsto_datahora, 'datahora', 'Diferença de Datahora (Previsto vs Real)', 'Diferença em Segundos', os.path.join(resultados_dir, 'diferenca_datahora.png'))\n",
        "        plot_differences(df_resposta_lat_long, df_previsto_lat_long, ['latitude', 'longitude'], 'Diferença de Latitude e Longitude (Previsto vs Real)', 'Diferença em Graus', os.path.join(resultados_dir, 'diferenca_lat_long.png'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TESTANDO ATÉ FUNCIONAR ABAIXO Mean Haversine Distance\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "#df['latitude'] = df['latitude'].str.replace(',', '.').astype('float32')\n",
        "#df['longitude'] = df['longitude'].str.replace(',', '.').astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Loading preprocessed data from temp files...\n",
            "INFO:root:Model file /mnt/d/POS_GRADUACAO/MESTRADO_PUBLICO/DATA_MINING/TAREFAS/Tarefa3/dados_teste/processados_teste/resultados/final_model_datahora.pkl already exists. Skipping optimization and loading the model.\n",
            "INFO:root:Model file /mnt/d/POS_GRADUACAO/MESTRADO_PUBLICO/DATA_MINING/TAREFAS/Tarefa3/dados_teste/processados_teste/resultados/final_model_lat_long.pkl already exists. Skipping optimization and loading the model.\n",
            "INFO:root:Preparing data for merging...\n",
            "INFO:root:Merging and creating the dataframe df_previsto_datahora...\n",
            "INFO:root:Finished merging and creating the dataframe df_previsto_datahora...\n",
            "INFO:root:Input features for datahora prediction: ['latitude', 'longitude', 'labels', 'linha', 'ordem', 'dia_da_semana', 'hora', 'diff_timestamp', 'latitude_diff', 'longitude_diff', 'distancia']\n",
            "INFO:root:Predicting 'datahora'...\n",
            "INFO:root:Creating the submission file to predict datahora...\n",
            "INFO:root:/mnt/d/POS_GRADUACAO/MESTRADO_PUBLICO/DATA_MINING/TAREFAS/Tarefa3/dados_teste/processados_teste/submissoes/submissao_datahora.csv already exists.\n",
            "INFO:root:Finished creating the submission file to predict datahora...\n",
            "INFO:root:Merging and creating the dataframe df_previsto_lat_long...\n",
            "INFO:root:Finished merging and creating the dataframe df_previsto_lat_long...\n",
            "INFO:root:Input features for latitude/longitude prediction: ['labels', 'linha', 'ordem', 'datahora', 'dia_da_semana', 'hora', 'diff_timestamp', 'latitude_diff', 'longitude_diff', 'distancia']\n",
            "INFO:root:Predicting 'latitude' and 'longitude'...\n",
            "INFO:root:Creating the submission file to predict latitude and longitude...\n",
            "Creating submission file: 100%|██████████| 78/78 [03:37<00:00,  2.78s/it]\n",
            "INFO:root:Saving the final submission as Parquet...\n",
            "INFO:root:Finished creating the submission file to predict latitude and longitude...\n",
            "INFO:root:Cleaning up...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import cudf\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import logging\n",
        "import joblib\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Directories\n",
        "base_dir = os.getcwd()\n",
        "dados_teste_dir = os.path.join(base_dir, 'dados_teste', 'processados_teste')\n",
        "resultados_dir = os.path.join(dados_teste_dir, 'resultados')\n",
        "submissoes_dir = os.path.join(dados_teste_dir, 'submissoes')\n",
        "temp_dir = os.path.join(resultados_dir, 'temp')\n",
        "os.makedirs(resultados_dir, exist_ok=True)\n",
        "os.makedirs(submissoes_dir, exist_ok=True)\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "arquivos = [\n",
        "    'prepared_data_hist.parquet',\n",
        "    'intermediate_treino_data_datahora.parquet',\n",
        "    'intermediate_treino_data_lat_long.parquet',\n",
        "    'intermediate_resposta_data_datahora.parquet',\n",
        "    'intermediate_resposta_data_lat_long.parquet'\n",
        "]\n",
        "\n",
        "def convert_dtypes_pandas(df):\n",
        "    if \"latitude\" in df.columns and (df[\"latitude\"].dtype == 'object' or df[\"latitude\"].dtype.name == 'category'):\n",
        "        df[\"latitude\"] = df[\"latitude\"].str.replace(',', '.').astype('float32')\n",
        "    if \"longitude\" in df.columns and (df[\"longitude\"].dtype == 'object' or df[\"longitude\"].dtype.name == 'category'):\n",
        "        df[\"longitude\"] = df[\"longitude\"].str.replace(',', '.').astype('float32')\n",
        "    if \"datahora\" in df.columns:\n",
        "        df['datahora'] = df['datahora'].astype('int64')\n",
        "    if \"linha\" in df.columns:\n",
        "        df['linha'] = pd.Categorical(df['linha']).codes\n",
        "    if \"ordem\" in df.columns:\n",
        "        df['ordem'] = pd.Categorical(df['ordem']).codes\n",
        "    if \"dia_da_semana\" in df.columns:\n",
        "        df['dia_da_semana'] = df['dia_da_semana'].astype('int16')\n",
        "    if \"hora\" in df.columns:\n",
        "        df['hora'] = df['hora'].astype('int16')\n",
        "    if \"diff_timestamp\" in df.columns:\n",
        "        df['diff_timestamp'] = df['diff_timestamp'].astype('float64')\n",
        "    if \"latitude_diff\" in df.columns:\n",
        "        df['latitude_diff'] = df['latitude_diff'].astype('float32')\n",
        "    if \"longitude_diff\" in df.columns:\n",
        "        df['longitude_diff'] = df['longitude_diff'].astype('float32')\n",
        "    if \"distancia\" in df.columns:\n",
        "        df['distancia'] = df['distancia'].astype('float32')\n",
        "    return df\n",
        "\n",
        "def convert_to_int_for_merge_pandas(df, col_name):\n",
        "    df[f'{col_name}_int'] = (df[col_name].astype(str).str.replace(',', '.').astype(float) * 1e6).astype(int)\n",
        "    return df\n",
        "\n",
        "def add_date_merge_column(df):\n",
        "    df['date_merge'] = pd.to_datetime(df['datahora'], unit='ms').dt.date\n",
        "    return df\n",
        "\n",
        "def load_and_prepare_data(dados_teste_dir, arquivos, horas_teste, temp_dir):\n",
        "    temp_files = {\n",
        "        'train_hist': os.path.join(temp_dir, 'train_hist.parquet'),\n",
        "        'test_hist': os.path.join(temp_dir, 'test_hist.parquet'),\n",
        "        'train_datahora': os.path.join(temp_dir, 'train_datahora.parquet'),\n",
        "        'train_lat_long': os.path.join(temp_dir, 'train_lat_long.parquet'),\n",
        "        'resposta_datahora': os.path.join(temp_dir, 'resposta_datahora.parquet'),\n",
        "        'resposta_lat_long': os.path.join(temp_dir, 'resposta_lat_long.parquet')\n",
        "    }\n",
        "\n",
        "    if all(os.path.exists(file) for file in temp_files.values()):\n",
        "        logging.info(\"Loading preprocessed data from temp files...\")\n",
        "        df_hist_train = cudf.read_parquet(temp_files['train_hist'])\n",
        "        df_hist_test = cudf.read_parquet(temp_files['test_hist'])\n",
        "        df_treino_datahora = cudf.read_parquet(temp_files['train_datahora'])\n",
        "        df_treino_lat_long = cudf.read_parquet(temp_files['train_lat_long'])\n",
        "        df_resposta_datahora = cudf.read_parquet(temp_files['resposta_datahora'])\n",
        "        df_resposta_lat_long = cudf.read_parquet(temp_files['resposta_lat_long'])\n",
        "    else:\n",
        "        logging.info(\"Preprocessing data and saving to temp files...\")\n",
        "        if not all(os.path.exists(os.path.join(dados_teste_dir, arquivo)) for arquivo in arquivos):\n",
        "            logging.error(\"Nem todos os arquivos necessários foram encontrados. Certifique-se de que todos os arquivos intermediários estão disponíveis.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        df_hist = pd.read_parquet(os.path.join(dados_teste_dir, 'prepared_data_hist.parquet'))\n",
        "        df_hist = convert_dtypes_pandas(df_hist)\n",
        "\n",
        "        df_treino_datahora = pd.read_parquet(os.path.join(dados_teste_dir, 'intermediate_treino_data_datahora.parquet'))\n",
        "        df_treino_datahora = convert_dtypes_pandas(df_treino_datahora)\n",
        "\n",
        "        df_treino_lat_long = pd.read_parquet(os.path.join(dados_teste_dir, 'intermediate_treino_data_lat_long.parquet'))\n",
        "        df_treino_lat_long = convert_dtypes_pandas(df_treino_lat_long)\n",
        "\n",
        "        df_resposta_datahora = pd.read_parquet(os.path.join(dados_teste_dir, 'intermediate_resposta_data_datahora.parquet'))\n",
        "        df_resposta_datahora = convert_dtypes_pandas(df_resposta_datahora)\n",
        "\n",
        "        df_resposta_lat_long = pd.read_parquet(os.path.join(dados_teste_dir, 'intermediate_resposta_data_lat_long.parquet'))\n",
        "        df_resposta_lat_long = convert_dtypes_pandas(df_resposta_lat_long)\n",
        "\n",
        "        # Divisão do dataframe de treino e teste\n",
        "        max_date = pd.to_datetime(df_hist['datahora'].max(), unit='ms')\n",
        "        test_start_date = max_date - pd.Timedelta(hours=horas_teste)\n",
        "        \n",
        "        df_hist['datahora_converted'] = pd.to_datetime(df_hist['datahora'], unit='ms')\n",
        "        df_hist_train = df_hist[df_hist['datahora_converted'] < test_start_date]\n",
        "        df_hist_test = df_hist[df_hist['datahora_converted'] >= test_start_date]\n",
        "        \n",
        "        # Remover a coluna 'datahora_converted'\n",
        "        df_hist_train = df_hist_train.drop(columns=['datahora_converted'])\n",
        "        df_hist_test = df_hist_test.drop(columns=['datahora_converted'])\n",
        "\n",
        "        # Adicionar colunas auxiliares para o merge\n",
        "        df_hist_train = add_date_merge_column(df_hist_train)\n",
        "        df_hist_train = convert_to_int_for_merge_pandas(df_hist_train, 'latitude')\n",
        "        df_hist_train = convert_to_int_for_merge_pandas(df_hist_train, 'longitude')\n",
        "\n",
        "        df_treino_lat_long = convert_to_int_for_merge_pandas(df_treino_lat_long, 'latitude')\n",
        "        df_treino_lat_long = convert_to_int_for_merge_pandas(df_treino_lat_long, 'longitude')    \n",
        "        df_treino_datahora = add_date_merge_column(df_treino_datahora)\n",
        "\n",
        "        # Salvar dataframes como parquet\n",
        "        df_hist_train.to_parquet(temp_files['train_hist'])\n",
        "        df_hist_test.to_parquet(temp_files['test_hist'])\n",
        "        df_treino_datahora.to_parquet(temp_files['train_datahora'])\n",
        "        df_treino_lat_long.to_parquet(temp_files['train_lat_long'])\n",
        "        df_resposta_datahora.to_parquet(temp_files['resposta_datahora'])\n",
        "        df_resposta_lat_long.to_parquet(temp_files['resposta_lat_long'])\n",
        "\n",
        "        del df_hist, df_treino_datahora, df_treino_lat_long, df_resposta_datahora, df_resposta_lat_long\n",
        "        gc.collect()\n",
        "\n",
        "        # Recarregar dataframes como cudf.DataFrame\n",
        "        df_hist_train = cudf.read_parquet(temp_files['train_hist'])\n",
        "        df_hist_test = cudf.read_parquet(temp_files['test_hist'])\n",
        "        df_treino_datahora = cudf.read_parquet(temp_files['train_datahora'])\n",
        "        df_treino_lat_long = cudf.read_parquet(temp_files['train_lat_long'])\n",
        "        df_resposta_datahora = cudf.read_parquet(temp_files['resposta_datahora'])\n",
        "        df_resposta_lat_long = cudf.read_parquet(temp_files['resposta_lat_long'])\n",
        "\n",
        "    return df_hist_train, df_hist_test, df_treino_datahora, df_treino_lat_long, df_resposta_datahora, df_resposta_lat_long\n",
        "\n",
        "def encode_categorical(df, unique_values):\n",
        "    mappings = {}\n",
        "    for col, unique_vals in unique_values.items():\n",
        "        cat_type = pd.CategoricalDtype(categories=[val for val in unique_vals if pd.notna(val)], ordered=True)\n",
        "        df[col] = df[col].astype(cat_type).cat.codes\n",
        "        mappings[col] = {val: code for code, val in enumerate(cat_type.categories)}\n",
        "    return df, mappings\n",
        "\n",
        "def ensure_categorical_consistency(train_df, test_df, cat_features):\n",
        "    for col in cat_features:\n",
        "        if train_df[col].dtype.name == 'category' and test_df[col].dtype.name == 'category':\n",
        "            train_cats = set(train_df[col].cat.categories)\n",
        "            test_cats = set(test_df[col].cat.categories)\n",
        "            common_cats = train_cats.intersection(test_cats)\n",
        "            train_df[col] = train_df[col].cat.set_categories(common_cats)\n",
        "            test_df[col] = test_df[col].cat.set_categories(common_cats)\n",
        "    return train_df, test_df\n",
        "\n",
        "def collect_unique_values(dfs, cat_columns):\n",
        "    combined_unique_values = {col: set() for col in cat_columns}\n",
        "    for df in dfs:\n",
        "        for col in cat_columns:\n",
        "            if col in df.columns:\n",
        "                combined_unique_values[col].update(df[col].unique().to_pandas())\n",
        "    return combined_unique_values\n",
        "\n",
        "def mean_haversine_distance(y_true, y_pred):\n",
        "    R = 6371  # Radius of the earth in kilometers\n",
        "    y_true = cp.radians(y_true)\n",
        "    y_pred = cp.radians(y_pred)\n",
        "    dlat = y_pred[:, 0] - y_true[:, 0]\n",
        "    dlon = y_pred[:, 1] - y_true[:, 1]\n",
        "    a = cp.sin(dlat / 2) ** 2 + cp.cos(y_true[:, 0]) * cp.cos(y_pred[:, 0]) * cp.sin(dlon / 2) ** 2\n",
        "    c = 2 * cp.arctan2(cp.sqrt(a), cp.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return cp.mean(distance).get()  # Convert to NumPy array\n",
        "\n",
        "def run_optimized_models(df, target_cols, save_dir, metric='rmse', combined_unique_values=None, is_location=False):\n",
        "    df, mappings = encode_categorical(df.to_pandas(), combined_unique_values)\n",
        "    df = cudf.DataFrame.from_pandas(df)\n",
        "    X = df.drop(columns=target_cols)\n",
        "    y = df[target_cols]\n",
        "    \n",
        "    X = convert_dtypes_pandas(X.to_pandas())\n",
        "    y = convert_dtypes_pandas(y.to_pandas())\n",
        "\n",
        "    def objective_lgbm(trial):\n",
        "        params = {\n",
        "            'objective': 'regression',\n",
        "            'verbosity': -1,\n",
        "            'boosting_type': 'gbdt',\n",
        "            'device': 'cuda',\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
        "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 2, 256),\n",
        "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
        "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n",
        "        }\n",
        "        \n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        if is_location:\n",
        "            model = MultiOutputRegressor(model)\n",
        "        scores = []\n",
        "        \n",
        "        for train_index, test_index in tqdm(KFold(n_splits=3).split(X), desc=\"Cross-validation (LightGBM)\"):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "            \n",
        "            X_train = convert_dtypes_pandas(X_train)\n",
        "            X_test = convert_dtypes_pandas(X_test)\n",
        "            y_train = convert_dtypes_pandas(y_train)\n",
        "            y_test = convert_dtypes_pandas(y_test)\n",
        "            \n",
        "            model.fit(X_train, y_train)\n",
        "            preds = model.predict(X_test)\n",
        "            if metric == 'rmse':\n",
        "                score = mean_squared_error(y_test, preds, squared=False)\n",
        "            elif metric == 'mean_haversine_distance':\n",
        "                score = mean_haversine_distance(cp.array(y_test), cp.array(preds))\n",
        "            scores.append(score)\n",
        "        \n",
        "        return np.mean(scores)\n",
        "    \n",
        "    def objective_xgb(trial):\n",
        "        params = {\n",
        "            'verbosity': 0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'booster': 'gbtree',\n",
        "            'tree_method': 'gpu_hist',\n",
        "            'gpu_id': 0,\n",
        "            'eta': trial.suggest_loguniform('eta', 1e-4, 1e-1),\n",
        "            'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
        "            'enable_categorical': True  # Habilitar suporte a categorias\n",
        "        }\n",
        "        \n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        if is_location:\n",
        "            model = MultiOutputRegressor(model)\n",
        "        scores = []\n",
        "        \n",
        "        for train_index, test_index in tqdm(KFold(n_splits=3).split(X), desc=\"Cross-validation (XGBoost)\"):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "            \n",
        "            X_train = convert_dtypes_pandas(X_train)\n",
        "            X_test = convert_dtypes_pandas(X_test)\n",
        "            y_train = convert_dtypes_pandas(y_train)\n",
        "            y_test = convert_dtypes_pandas(y_test)\n",
        "            \n",
        "            model.fit(X_train, y_train)\n",
        "            preds = model.predict(X_test)\n",
        "            if metric == 'rmse':\n",
        "                score = mean_squared_error(y_test, preds, squared=False)\n",
        "            elif metric == 'mean_haversine_distance':\n",
        "                score = mean_haversine_distance(cp.array(y_test), cp.array(preds))\n",
        "            scores.append(score)\n",
        "        \n",
        "        return np.mean(scores)\n",
        "    \n",
        "    # Optuna study for LightGBM\n",
        "    study_lgbm = optuna.create_study(direction='minimize')\n",
        "    for _ in tqdm(range(10), desc=\"LightGBM optimization trials\"):\n",
        "        study_lgbm.optimize(objective_lgbm, n_trials=1)\n",
        "    \n",
        "    lgbm_study_path = os.path.join(save_dir, 'lgbm_study.pkl')\n",
        "    if not os.path.exists(lgbm_study_path):\n",
        "        with open(lgbm_study_path, 'wb') as f:\n",
        "            joblib.dump(study_lgbm, f)\n",
        "    \n",
        "    # Optuna study for XGBoost\n",
        "    study_xgb = optuna.create_study(direction='minimize')\n",
        "    for _ in tqdm(range(10), desc=\"XGBoost optimization trials\"):\n",
        "        study_xgb.optimize(objective_xgb, n_trials=1)\n",
        "    \n",
        "    xgb_study_path = os.path.join(save_dir, 'xgb_study.pkl')\n",
        "    if not os.path.exists(xgb_study_path):\n",
        "        with open(xgb_study_path, 'wb') as f:\n",
        "            joblib.dump(study_xgb, f)\n",
        "\n",
        "    best_lgbm_trial = study_lgbm.best_trial\n",
        "    best_xgb_trial = study_xgb.best_trial\n",
        "\n",
        "    if best_lgbm_trial.value < best_xgb_trial.value:\n",
        "        return 'lgbm', best_lgbm_trial.params\n",
        "    else:\n",
        "        return 'xgb', best_xgb_trial.params\n",
        "\n",
        "def create_final_model(df, target_cols, best_model_name, best_params, save_dir, model_type):\n",
        "    final_model_path = os.path.join(save_dir, f'final_model_{model_type}.pkl')\n",
        "    \n",
        "    if os.path.exists(final_model_path):\n",
        "        logging.info(f\"Model file {final_model_path} already exists. Skipping model creation.\")\n",
        "        return joblib.load(final_model_path)\n",
        "    \n",
        "    df = convert_dtypes_pandas(df.to_pandas())\n",
        "    X = df.drop(columns=target_cols)\n",
        "    y = df[target_cols]\n",
        "    \n",
        "    X = convert_dtypes_pandas(X)\n",
        "    y = convert_dtypes_pandas(y)\n",
        "\n",
        "    if best_model_name == 'lgbm':\n",
        "        model = lgb.LGBMRegressor(**best_params)\n",
        "        if model_type == 'lat_long':\n",
        "            model = MultiOutputRegressor(model)\n",
        "    else:\n",
        "        model = xgb.XGBRegressor(**best_params)\n",
        "        if model_type == 'lat_long':\n",
        "            model = MultiOutputRegressor(model)\n",
        "\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    joblib.dump(model, final_model_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_submission(df_previsto_lat_long, df_resposta_datahora, submissao_lat_long_path):\n",
        "    previstos_lat_long_short = df_previsto_lat_long[[\"id\", \"latitude\", \"longitude\"]].copy()\n",
        "    # Split the data into manageable chunks to prevent memory issues\n",
        "    chunk_size = 500000  \n",
        "    total_rows = previstos_lat_long_short.shape[0]\n",
        "    chunks = total_rows // chunk_size + 1\n",
        "\n",
        "    for i in tqdm(range(chunks), desc=\"Creating submission file\"):\n",
        "        start_row = i * chunk_size\n",
        "        end_row = min(start_row + chunk_size, total_rows)\n",
        "        \n",
        "        df_chunk = previstos_lat_long_short[start_row:end_row].copy()\n",
        "        df_chunk = df_chunk.merge(\n",
        "            df_resposta_datahora[['id', 'datahora']].to_pandas(),\n",
        "            on='id',\n",
        "            how='inner'\n",
        "        )\n",
        "        \n",
        "        # Convert dtypes to reduce memory usage\n",
        "        for col in df_chunk.select_dtypes(include=['int64']).columns:\n",
        "            df_chunk[col] = df_chunk[col].astype('int32')\n",
        "        for col in df_chunk.select_dtypes(include=['float64']).columns:\n",
        "            df_chunk[col] = df_chunk[col].astype('float32')\n",
        "        for col in df_chunk.select_dtypes(include=['object']).columns:\n",
        "            df_chunk[col] = df_chunk[col].astype('category')\n",
        "\n",
        "        if i == 0:\n",
        "            df_chunk.to_csv(submissao_lat_long_path, index=False, mode='w', header=True)\n",
        "        else:\n",
        "            df_chunk = df_chunk.drop_duplicates(keep='first')\n",
        "            df_chunk.to_csv(submissao_lat_long_path, index=False, mode='a', header=False)\n",
        "\n",
        "    # Save as Parquet\n",
        "    logging.info(\"Saving the final submission as Parquet...\")\n",
        "    df_final = pd.read_csv(submissao_lat_long_path)\n",
        "    df_final.to_parquet(submissao_lat_long_path.replace('.csv', '.parquet'), index=False)\n",
        "\n",
        "def main():\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    horas_teste = 5\n",
        "\n",
        "    df_hist_train, df_hist_test, df_treino_datahora, df_treino_lat_long, df_resposta_datahora, df_resposta_lat_long = load_and_prepare_data(dados_teste_dir, arquivos, horas_teste, temp_dir)\n",
        "    \n",
        "    if df_hist_train is None:\n",
        "        return\n",
        "    \n",
        "    cat_features = {'linha': df_hist_train['linha'].unique(), 'ordem': df_hist_train['ordem'].unique()}\n",
        "\n",
        "    df_hist_train, df_hist_test = ensure_categorical_consistency(df_hist_train, df_hist_test, cat_features)\n",
        "    \n",
        "    # Collect unique values for categorical consistency\n",
        "    combined_unique_values = collect_unique_values(\n",
        "        [df_hist_train, df_hist_test],\n",
        "        ['linha', 'ordem']\n",
        "    )\n",
        "\n",
        "    # Training and creating model for 'datahora'\n",
        "    final_model_datahora_path = os.path.join(resultados_dir, 'final_model_datahora.pkl')\n",
        "    if not os.path.exists(final_model_datahora_path):\n",
        "        logging.info(\"Starting optimization for 'datahora' prediction...\")\n",
        "        df_train_datahora = df_hist_train.drop(columns=['dia_da_semana', 'hora', 'diff_timestamp', 'date_merge', 'latitude_int', 'longitude_int'])\n",
        "        best_model_name, best_params = run_optimized_models(\n",
        "            df_train_datahora,\n",
        "            target_cols=['datahora'], \n",
        "            save_dir=resultados_dir, \n",
        "            metric='rmse', \n",
        "            combined_unique_values=combined_unique_values\n",
        "        )\n",
        "        best_model_datahora = create_final_model(\n",
        "            df_train_datahora, \n",
        "            target_cols=['datahora'], \n",
        "            best_model_name=best_model_name, \n",
        "            best_params=best_params, \n",
        "            save_dir=resultados_dir, \n",
        "            model_type='datahora'\n",
        "        )\n",
        "    else:\n",
        "        logging.info(f\"Model file {final_model_datahora_path} already exists. Skipping optimization and loading the model.\")\n",
        "        best_model_datahora = joblib.load(final_model_datahora_path)\n",
        "\n",
        "    # Training and creating model for 'latitude' and 'longitude'\n",
        "    final_model_lat_long_path = os.path.join(resultados_dir, 'final_model_lat_long.pkl')\n",
        "    if not os.path.exists(final_model_lat_long_path):\n",
        "        logging.info(\"Starting optimization for 'latitude' and 'longitude' prediction...\")\n",
        "        df_train_lat_long = df_hist_train.drop(columns=['latitude_diff', 'longitude_diff', 'date_merge', 'latitude_int', 'longitude_int'])\n",
        "        best_model_name, best_params = run_optimized_models(\n",
        "            df_train_lat_long,  # Drop latitude/longitude-related columns\n",
        "            target_cols=['latitude', 'longitude'], \n",
        "            save_dir=resultados_dir, \n",
        "            metric='mean_haversine_distance', \n",
        "            combined_unique_values=combined_unique_values,\n",
        "            is_location=True\n",
        "        )\n",
        "        best_model_lat_long = create_final_model(\n",
        "            df_train_lat_long, \n",
        "            target_cols=['latitude', 'longitude'], \n",
        "            best_model_name=best_model_name, \n",
        "            best_params=best_params, \n",
        "            save_dir=resultados_dir, \n",
        "            model_type='lat_long'\n",
        "        )\n",
        "    else:\n",
        "        logging.info(f\"Model file {final_model_lat_long_path} already exists. Skipping optimization and loading the model.\")\n",
        "        best_model_lat_long = joblib.load(final_model_lat_long_path)\n",
        "\n",
        "    logging.info(\"Preparing data for merging...\")\n",
        "    hist_train_datahora = df_hist_train.copy()\n",
        "    hist_train_lat_long = df_hist_train.copy()\n",
        "\n",
        "    logging.info(\"Merging and creating the dataframe df_previsto_datahora...\")\n",
        "    df_previsto_datahora = cudf.merge(\n",
        "        hist_train_datahora.drop(columns=['datahora']),\n",
        "        df_treino_lat_long[['ordem', 'linha', 'latitude_int', 'longitude_int', 'id']],\n",
        "        on=['ordem', 'linha', 'latitude_int', 'longitude_int'],\n",
        "        how='inner'\n",
        "    ).drop(columns=['latitude_int', 'longitude_int', 'date_merge'])\n",
        "    logging.info(\"Finished merging and creating the dataframe df_previsto_datahora...\")\n",
        "\n",
        "    df_previsto_datahora_id = df_previsto_datahora['id']\n",
        "    df_previsto_datahora = df_previsto_datahora.drop(columns=['id'])\n",
        "\n",
        "    df_previsto_datahora = convert_dtypes_pandas(df_previsto_datahora.to_pandas())\n",
        "    logging.info(f\"Input features for datahora prediction: {list(df_previsto_datahora.columns)}\")\n",
        "\n",
        "    df_previsto_datahora = df_previsto_datahora.drop(columns=['dia_da_semana', 'hora', 'diff_timestamp'])\n",
        "\n",
        "    logging.info(\"Predicting 'datahora'...\")\n",
        "    df_previsto_datahora['datahora'] = best_model_datahora.predict(df_previsto_datahora)\n",
        "    df_previsto_datahora['id'] = df_previsto_datahora_id.to_pandas()\n",
        "    \n",
        "    logging.info(\"Creating the submission file to predict datahora...\")\n",
        "    submissao_datahora_path = os.path.join(submissoes_dir, 'submissao_datahora.csv')\n",
        "    if not os.path.exists(submissao_datahora_path):\n",
        "        df_submissao_datahora = pd.merge(\n",
        "            df_previsto_datahora[['id', 'datahora']],\n",
        "            df_resposta_lat_long[['id', 'latitude', 'longitude']].to_pandas(),\n",
        "            on='id',\n",
        "            how='inner'\n",
        "        )\n",
        "        df_submissao_datahora.to_csv(submissao_datahora_path, index=False)\n",
        "    else:\n",
        "        logging.info(f\"{submissao_datahora_path} already exists.\")\n",
        "    logging.info(\"Finished creating the submission file to predict datahora...\")\n",
        "\n",
        "    logging.info(\"Merging and creating the dataframe df_previsto_lat_long...\")\n",
        "    df_previsto_lat_long = cudf.merge(\n",
        "        hist_train_lat_long.drop(columns=['latitude', 'longitude']),\n",
        "        df_treino_datahora[['ordem', 'linha', 'date_merge', 'id']],\n",
        "        on=['ordem', 'linha', 'date_merge'],\n",
        "        how='inner'\n",
        "    ).drop(columns=['date_merge', 'latitude_int', 'longitude_int'])\n",
        "    logging.info(\"Finished merging and creating the dataframe df_previsto_lat_long...\")\n",
        "\n",
        "    df_previsto_lat_long_id = df_previsto_lat_long['id']\n",
        "    df_previsto_lat_long = df_previsto_lat_long.drop(columns=['id'])\n",
        "\n",
        "    df_previsto_lat_long = convert_dtypes_pandas(df_previsto_lat_long.to_pandas())\n",
        "    logging.info(f\"Input features for latitude/longitude prediction: {list(df_previsto_lat_long.columns)}\")\n",
        "    \n",
        "    df_previsto_lat_long = df_previsto_lat_long.drop(columns=['latitude_diff', 'longitude_diff'])\n",
        "\n",
        "    logging.info(\"Predicting 'latitude' and 'longitude'...\")\n",
        "    predicted_lat_long = best_model_lat_long.predict(df_previsto_lat_long)\n",
        "    df_previsto_lat_long['latitude'] = predicted_lat_long[:, 0]\n",
        "    df_previsto_lat_long['longitude'] = predicted_lat_long[:, 1]\n",
        "    df_previsto_lat_long['id'] = df_previsto_lat_long_id.to_pandas()\n",
        "\n",
        "    logging.info(\"Creating the submission file to predict latitude and longitude...\")\n",
        "    submissao_lat_long_path = os.path.join(submissoes_dir, 'submissao_lat_long.csv')\n",
        "    if not os.path.exists(submissao_lat_long_path):\n",
        "        create_submission(df_previsto_lat_long, df_resposta_datahora, submissao_lat_long_path)\n",
        "    else:\n",
        "        logging.info(f\"{submissao_lat_long_path} already exists.\")\n",
        "    logging.info(\"Finished creating the submission file to predict latitude and longitude...\")\n",
        "    \n",
        "    logging.info(\"Cleaning up...\")\n",
        "    del df_hist_train, df_hist_test, df_treino_datahora, df_treino_lat_long, df_resposta_datahora, df_resposta_lat_long\n",
        "    del hist_train_datahora, hist_train_lat_long, df_previsto_datahora, df_previsto_lat_long\n",
        "    gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_dir = os.getcwd()\n",
        "dados_teste_dir = os.path.join(base_dir, 'dados_teste', 'processados_teste')\n",
        "resultados_dir = os.path.join(dados_teste_dir, 'resultados')\n",
        "path = os.path.join(dados_teste_dir, 'prepared_data_hist.parquet')\n",
        "path2 = os.path.join(dados_teste_dir, 'intermediate_resposta_data_datahora.parquet')\n",
        "path3 = os.path.join(dados_teste_dir, 'intermediate_resposta_data_lat_long.parquet')\n",
        "path4 = os.path.join(dados_teste_dir, 'intermediate_treino_data_datahora.parquet')\n",
        "path5 = os.path.join(dados_teste_dir, 'intermediate_treino_data_lat_long.parquet')\n",
        "df_hist = pd.read_parquet(path)\n",
        "df_resposta_datahora = pd.read_parquet(path2)\n",
        "df_resposta_lat_long = pd.read_parquet(path3)\n",
        "df_treino_datahora = pd.read_parquet(path4)\n",
        "df_treino_lat_long = pd.read_parquet(path5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_hist.linha.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_hist.datahora_converted.min(), df_hist.datahora_converted.max() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_hist.datahora_converted.dt.day.min(), df_hist.datahora_converted.dt.hour.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Colunas do arquivo prepared_data_hist.parquet\")\n",
        "print(df_hist.head(2))\n",
        "print(f\"Tamanho do Dataframe: {df_hist.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Colunas do arquivo intermediate_resposta_data_datahora.parquet\")\n",
        "print(df_resposta_datahora.head(2))\n",
        "print(f\"Tamanho do Dataframe: {df_resposta_datahora.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Colunas do arquivo intermediate_resposta_data_lat_long.parquet\")\n",
        "print(df_resposta_lat_long.head(2))\n",
        "print(f\"Tamanho do Dataframe: {df_resposta_lat_long.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Colunas do arquivo intermediate_treino_data_datahora.parquet\")\n",
        "print(df_treino_datahora.head(2))\n",
        "print(f\"Tamanho do Dataframe: {df_treino_datahora.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Colunas do arquivo intermediate_treino_data_lat_long.parquet\")\n",
        "print(df_treino_lat_long.head(2))\n",
        "print(f\"Tamanho do Dataframe: {df_treino_lat_long.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_treino_datahora.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_hist.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"intermediate_treino_data_datahora.parquet\")\n",
        "df_datahora.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"intermediate_treino_data_lat_long.parquet\")\n",
        "df_lat_long.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_lat_long.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
