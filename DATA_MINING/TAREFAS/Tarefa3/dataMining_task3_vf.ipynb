{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Objetivo</h1>\n",
        "<h5><b>Avaliação:</b></h5><p>Para avaliar a performance da previsão do timestamp datahora, será usada a métrica RMSE. Para avaliar a performance da previsão de latitude e longitude, será usada a métrica Mean Haversine Distance</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Manipulação de arquivos\n",
        "import os\n",
        "import zipfile\n",
        "from glob import glob\n",
        "import json\n",
        "import gc\n",
        "import warnings\n",
        "\n",
        "#Manipulação e processamento de dados\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cudf\n",
        "import cupy as cp\n",
        "\n",
        "#Machine Learning\n",
        "from cuml.cluster import DBSCAN\n",
        "from sklearn.cluster import DBSCAN as SklearnDBSCAN\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import torch\n",
        "import joblib\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from cuml.ensemble import RandomForestRegressor as cuRF\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "#Barra de progresso\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "\n",
        "#Visualização\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "#Perfil de memória\n",
        "from memory_profiler import memory_usage\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpE2fEW2O6nD"
      },
      "outputs": [],
      "source": [
        "#Caminhos das pastas\n",
        "base_dir = os.getcwd()\n",
        "dados_dir = os.path.join(base_dir, 'dados')\n",
        "intermediarios_dir = os.path.join(dados_dir, 'intermediarios')\n",
        "processados_dir = os.path.join(dados_dir, 'processados')\n",
        "\n",
        "#Criar diretórios se não existirem\n",
        "os.makedirs(intermediarios_dir, exist_ok=True)\n",
        "os.makedirs(processados_dir, exist_ok=True)\n",
        "\n",
        "#Lista global para armazenar os DataFrames processados\n",
        "final_df_list = []\n",
        "\n",
        "#Dicionário para armazenar o sentido atual de cada ônibus\n",
        "sentidos_atual = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>EDA</h1>\n",
        "<p>Fazendo algumas análises estatísticas e plotagens de dados</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EDA:\n",
        "    def __init__(self, file_path):\n",
        "        #Inicializa a classe com o caminho do arquivo de dados processados.\n",
        "        self.file_path = file_path  \n",
        "\n",
        "    #Carrega os dados do arquivo Parquet usando pandas ou cudf.\n",
        "    def load_data(self, use_cudf=False):\n",
        "        try:\n",
        "            if use_cudf:\n",
        "                return cudf.read_parquet(self.file_path) \n",
        "            else:\n",
        "                return pd.read_parquet(self.file_path) \n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao carregar os dados: {e}\")\n",
        "            return None\n",
        "\n",
        "    #Realiza uma inspeção inicial dos dados, visualizando as primeiras linhas e um resumo estatístico.\n",
        "    def initial_inspection(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            print(df.describe()) \n",
        "            del df  \n",
        "            gc.collect() \n",
        "\n",
        "    #Analisa as estatísticas dos dados, incluindo a distribuição das velocidades e a contagem de registros por linha e sentido.\n",
        "    def analyze_statistics(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            #Plota a distribuição das velocidades\n",
        "            df['velocidade'].hist(bins=50)  # Plota um histograma\n",
        "            plt.title('Distribuição de Velocidades')\n",
        "            plt.xlabel('Velocidade')\n",
        "            plt.ylabel('Frequência')\n",
        "            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "            plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo x\n",
        "            plt.show()\n",
        "\n",
        "            #Conta registros por linha e sentido e exibe os resultados\n",
        "            #print(df.groupby(['linha', 'sentido']).size().to_frame('count').reset_index())\n",
        "            del df \n",
        "            gc.collect()\n",
        "\n",
        "    #Analisa padrões temporais dos dados, incluindo o volume de dados por hora e padrões de movimentação ao longo do tempo.\n",
        "    def analyze_temporal_patterns(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            #Extrai a hora do timestamp\n",
        "            #df['datahora_converted'] = pd.to_datetime(df['datahora'])\n",
        "            df['hora'] = df['datahora_converted'].dt.hour\n",
        "\n",
        "            # Filtra as horas entre 8 e 22\n",
        "            df = df[(df['hora'] >= 8) & (df['hora'] < 22)]\n",
        "\n",
        "            #Conta registros por hora e plota o volume de dados por hora\n",
        "            hourly_counts = df.groupby('hora').size().to_frame('count').reset_index()\n",
        "            hourly_counts.plot(x='hora', y='count', kind='bar')\n",
        "            plt.title('Volume de Dados por Hora')\n",
        "            plt.xlabel('Hora do Dia')\n",
        "            plt.ylabel('Contagem')\n",
        "            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "            plt.show()\n",
        "\n",
        "            #Conta registros por dia ao longo do tempo e plota padrões de movimentação\n",
        "            df['data'] = df['datahora_converted'].dt.date  # Extrai apenas a data\n",
        "            daily_counts = df.groupby('data').size().to_frame('count').reset_index()\n",
        "            daily_counts.plot(x='data', y='count', kind='line')\n",
        "            plt.title('Padrões de Movimentação ao Longo do Tempo')\n",
        "            plt.xlabel('Data')\n",
        "            plt.ylabel('Contagem')\n",
        "            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Formata o eixo x para datas\n",
        "            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.show()\n",
        "\n",
        "            del df  \n",
        "            gc.collect()  \n",
        "\n",
        "    #Analisa padrões geográficos dos dados, plotando um mapa de calor dos trajetos dos ônibus.\n",
        "    def analyze_geographical_patterns(self, linhas=None):\n",
        "        try:\n",
        "            df_cudf = self.load_data(use_cudf=True)\n",
        "            if df_cudf is not None:\n",
        "                df_cudf['linha'] = df_cudf['linha'].astype(str)  # Garante que a coluna 'linha' seja string\n",
        "\n",
        "                if linhas:\n",
        "                    linhas = list(map(str, linhas))\n",
        "                    for linha in linhas:\n",
        "                        if linha not in df_cudf['linha'].to_pandas().tolist():\n",
        "                            raise ValueError(f\"Linha {linha} não encontrada nos dados.\")\n",
        "\n",
        "                    df_cudf = df_cudf[df_cudf['linha'].isin(linhas)]\n",
        "\n",
        "                num_linhas = len(linhas) if linhas else len(df_cudf['linha'].unique())\n",
        "                sample_fraction = min(1.0 / num_linhas, 0.01)  # Ajusta a fração de amostra de acordo com o número de linhas\n",
        "                max_points = min(5000 * num_linhas, 50000)  # Ajusta o número máximo de pontos de acordo com o número de linhas\n",
        "\n",
        "                #Define o centro do mapa baseado na média de latitude e longitude usando cudf\n",
        "                map_center = [float(df_cudf['latitude'].mean()), float(df_cudf['longitude'].mean())]\n",
        "                map_osm = folium.Map(location=map_center, zoom_start=12)  # Cria um mapa centrado na localização média\n",
        "\n",
        "                #Mostra os dados antes de converter para pandas para evitar problemas de memória\n",
        "                if len(df_cudf) > max_points:\n",
        "                    df_sample = df_cudf.sample(frac=sample_fraction)\n",
        "                    if len(df_sample) > max_points:\n",
        "                        df_sample = df_sample.sample(n=max_points)\n",
        "                else:\n",
        "                    df_sample = df_cudf\n",
        "\n",
        "                df_pandas = df_sample.to_pandas()\n",
        "\n",
        "                #Dados para o mapa de calor\n",
        "                heat_data = [[row['latitude'], row['longitude']] for index, row in df_pandas[['latitude', 'longitude']].iterrows()]\n",
        "                HeatMap(heat_data).add_to(map_osm)  # Adiciona o mapa de calor ao mapa\n",
        "\n",
        "                title = 'Mapa de Calor de Todas as Linhas'\n",
        "                if linhas:\n",
        "                    title = f'Mapa de Calor das Linhas: {\", \".join(map(str, linhas))}'\n",
        "                map_osm.get_root().html.add_child(folium.Element(f\"<h1>{title}</h1>\"))\n",
        "\n",
        "                map_osm.save('bus_routes.html')  # Salva o mapa como um arquivo HTML\n",
        "                print(f\"Mapa de calor salvo como 'bus_routes.html' com {len(df_pandas)} pontos de amostra\")\n",
        "                \n",
        "                del df_cudf  \n",
        "                del df_pandas  \n",
        "                gc.collect()\n",
        "            else:\n",
        "                print(\"Erro ao carregar os dados.\")\n",
        "        except ValueError as ve:\n",
        "            print(ve)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao gerar o mapa de calor: {e}\")\n",
        "\n",
        "    #Plota os trajetos dos ônibus, locais de garagem e pontos finais com diferentes cores.\n",
        "    def plot_routes_and_endpoints(self, linhas=None):\n",
        "        try:\n",
        "            df_cudf = self.load_data(use_cudf=True)\n",
        "            if df_cudf is not None:\n",
        "                df_cudf['linha'] = df_cudf['linha'].astype(str)  # Garante que a coluna 'linha' seja string\n",
        "\n",
        "                if linhas:\n",
        "                    linhas = list(map(str, linhas))\n",
        "                    for linha in linhas:\n",
        "                        if linha not in df_cudf['linha'].to_pandas().tolist():\n",
        "                            raise ValueError(f\"Linha {linha} não encontrada nos dados.\")\n",
        "\n",
        "                    df_cudf = df_cudf[df_cudf['linha'].isin(linhas)]\n",
        "\n",
        "                num_linhas = len(linhas) if linhas else len(df_cudf['linha'].unique())\n",
        "                sample_fraction = min(1.0 / num_linhas, 0.01)  # Ajusta a fração de amostra de acordo com o número de linhas\n",
        "                max_points = min(5000 * num_linhas, 50000)  # Ajusta o número máximo de pontos de acordo com o número de linhas\n",
        "\n",
        "                #Define o centro do mapa baseado na média de latitude e longitude usando cudf\n",
        "                map_center = [float(df_cudf['latitude'].mean()), float(df_cudf['longitude'].mean())]\n",
        "                map_osm = folium.Map(location=map_center, zoom_start=12)  # Cria um mapa centrado na localização média\n",
        "\n",
        "                #Mostra os dados antes de converter para pandas para evitar problemas de memória\n",
        "                if len(df_cudf) > max_points:\n",
        "                    df_sample = df_cudf.sample(frac=sample_fraction)\n",
        "                    if len(df_sample) > max_points:\n",
        "                        df_sample = df_sample.sample(n=max_points)\n",
        "                else:\n",
        "                    df_sample = df_cudf\n",
        "\n",
        "                df_pandas = df_sample.to_pandas()\n",
        "\n",
        "                #Plota os trajetos normais\n",
        "                normal_routes = df_pandas[~df_pandas['garagem'] & ~df_pandas['ponto_final']]\n",
        "                HeatMap([[row['latitude'], row['longitude']] for index, row in normal_routes[['latitude', 'longitude']].iterrows()],\n",
        "                        name='Trajetos Normais', gradient={0.4: 'blue', 1: 'lightblue'}).add_to(map_osm)\n",
        "\n",
        "                #Plota os locais de garagem\n",
        "                garage_routes = df_pandas[df_pandas['garagem']]\n",
        "                HeatMap([[row['latitude'], row['longitude']] for index, row in garage_routes[['latitude', 'longitude']].iterrows()],\n",
        "                        name='Locais de Garagem', gradient={0.4: 'red', 1: 'darkred'}).add_to(map_osm)\n",
        "\n",
        "                #Plota os pontos finais\n",
        "                end_points = df_pandas[df_pandas['ponto_final']]\n",
        "                HeatMap([[row['latitude'], row['longitude']] for index, row in end_points[['latitude', 'longitude']].iterrows()],\n",
        "                        name='Pontos Finais', gradient={0.4: 'yellow', 1: 'gold'}).add_to(map_osm)\n",
        "\n",
        "                title = 'Mapa de Trajetos, Locais de Garagem e Pontos Finais de Todas as Linhas'\n",
        "                if linhas:\n",
        "                    title = f'Mapa de Trajetos, Locais de Garagem e Pontos Finais das Linhas: {\", \".join(map(str, linhas))}'\n",
        "                map_osm.get_root().html.add_child(folium.Element(f\"<h1>{title}</h1>\"))\n",
        "\n",
        "                map_osm.save('bus_routes_endpoints.html')  # Salva o mapa como um arquivo HTML\n",
        "                print(f\"Mapa de trajetos e pontos salvos como 'bus_routes_endpoints.html' com {len(df_pandas)} pontos de amostra\")\n",
        "\n",
        "                del df_cudf  #Libera a memória\n",
        "                del df_pandas  #Libera a memória\n",
        "                gc.collect()  #Coleta o lixo\n",
        "            else:\n",
        "                print(\"Erro ao carregar os dados.\")\n",
        "        except ValueError as ve:\n",
        "            print(ve)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao gerar o mapa de trajetos e pontos: {e}\")\n",
        "\n",
        "    #Analisa a distribuição dos pontos finais e pontos de garagem.\n",
        "    def analyze_endpoints_garage(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            #Plota pontos finais\n",
        "            df[df['ponto_final'] == True][['latitude', 'longitude']].plot(kind='scatter', x='longitude', y='latitude')\n",
        "            plt.title('Distribuição de Pontos Finais')\n",
        "            plt.show()\n",
        "\n",
        "            #Plota pontos de garagem\n",
        "            df[df['garagem'] == True][['latitude', 'longitude']].plot(kind='scatter', x='longitude', y='latitude')\n",
        "            plt.title('Distribuição de Pontos de Garagem')\n",
        "            plt.show()\n",
        "\n",
        "            del df  #Libera a memória\n",
        "            gc.collect()  #Coleta o lixo\n",
        "\n",
        "    #Analisa os trajetos dos ônibus para cada linha.\n",
        "    def analyze_routes(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            linhas = df['linha'].unique()  # Obtém todas as linhas únicas\n",
        "            num_linhas = len(linhas)\n",
        "            num_cols = 3\n",
        "            num_rows = (num_linhas + num_cols - 1) // num_cols  # Calcula o número de linhas para os subplots\n",
        "            fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
        "            axes = axes.flatten()\n",
        "\n",
        "            for idx, linha in enumerate(linhas):\n",
        "                trajeto = df[df['linha'] == linha]  # Filtra os dados pela linha\n",
        "                ax = axes[idx]\n",
        "                ax.plot(trajeto['longitude'], trajeto['latitude'], label=f'Linha {linha}')  # Plota o trajeto\n",
        "                ax.set_title(f'Trajeto da Linha {linha}')\n",
        "                ax.set_xlabel('Longitude')\n",
        "                ax.set_ylabel('Latitude')\n",
        "                ax.legend()\n",
        "                ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "                ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo x\n",
        "\n",
        "            # Remove subplots vazios\n",
        "            for idx in range(len(linhas), len(axes)):\n",
        "                fig.delaxes(axes[idx])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def check_data_quality(self):\n",
        "        \"\"\"\n",
        "        Verifica a qualidade dos dados, incluindo valores nulos, dados inconsistentes e outliers.\n",
        "        \"\"\"\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            # Verifica valores nulos em cada coluna\n",
        "            print(\"Verificando valores nulos...\")\n",
        "            print(df.isnull().sum())\n",
        "\n",
        "            # Verifica valores não numéricos na coluna 'velocidade'\n",
        "            print(\"Verificando dados inconsistentes...\")\n",
        "            problematic_values = df[~df['velocidade'].astype(str).str.isnumeric()]\n",
        "            if not problematic_values.empty:\n",
        "                print(f\"Valores problemáticos em 'velocidade':\\n{problematic_values[['velocidade', 'velocidade']].head()}\")\n",
        "\n",
        "            # Verifica outliers de velocidade\n",
        "            print(\"Verificando outliers de velocidade [Velocidade acima de 150km/h]...\")\n",
        "            outliers = df[(df['velocidade'] < 0) | (df['velocidade'] > 150)]\n",
        "            print(f\"Outliers:\\n{outliers}\")\n",
        "\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def feature_engineering(self, df):\n",
        "        \"\"\"\n",
        "        Cria novas features a partir das informações temporais.\n",
        "        \"\"\"\n",
        "        print(\"Criando novas features...\")\n",
        "        #df['datahora_converted'] = pd.to_datetime(df['datahora'])\n",
        "        df['dia_da_semana'] = df['datahora_converted'].dt.dayofweek  # Adiciona a feature 'dia_da_semana'\n",
        "        df['hora'] = df['datahora_converted'].dt.hour  # Adiciona a feature 'hora'\n",
        "        print(\"Novas features criadas: 'dia_da_semana', 'hora'\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    def run_all(self):\n",
        "        \"\"\"\n",
        "        Executa todos os métodos de análise exploratória de dados em sequência.\n",
        "        \"\"\"\n",
        "        self.initial_inspection()\n",
        "        self.analyze_statistics()\n",
        "        self.analyze_temporal_patterns()\n",
        "        self.analyze_geographical_patterns()\n",
        "        self.plot_routes_and_endpoints()\n",
        "        self.analyze_endpoints_garage()\n",
        "        self.analyze_routes()\n",
        "        self.check_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eda = EDA(file_path=os.path.join(processados_dir, 'processed_data.parquet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Realizar inspeção inicial\n",
        "eda.initial_inspection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar estatísticas\n",
        "eda.analyze_statistics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar padrões temporais\n",
        "eda.analyze_temporal_patterns()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar padrões geográficos\n",
        "eda.analyze_geographical_patterns(linhas=[\"324\", \"3\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotar trajetos e pontos\n",
        "eda.plot_routes_and_endpoints(linhas=[\"324\", \"3\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar pontos finais e pontos de garagem\n",
        "eda.analyze_endpoints_garage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar trajetos dos ônibus\n",
        "eda.analyze_routes()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Verificar qualidade dos dados\n",
        "eda.check_data_quality()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Treinando 2 horas de dados consecutivas para prever a próxima hora</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Extraindo os dados dos arquivos .zip e concatenando os arquivos .json:</h3>\n",
        "<p>Nessa parte do código a ideia é extrair os arquivos .json dos arquivos .zip. Cada arquivo .zip contém 2 tipos de dados: dados históricos e dados de teste. Portanto, ao final, todos os arquivos de dados históricos foram concatenados em 1 arquivo e todos os arquivos de teste foram concatenados em outro arquivo.</p>\n",
        "<h3>2 tipos de arquivos de teste:</h3>\n",
        "<p>Existem 2 arquivos de teste: Um para testar previsão de datahora e outro para testar a previsão de latitude e longitude. Portanto, 2 arquivos de teste foram criados</p>\n",
        "<h3>Filtragem Inicial:</h3>\n",
        "<p>Para atender as demandas do enunciado do professor Zimbrão, algumas filtragens iniciais foram feitas:\n",
        "<li>apenas linhas específicas que serão analizadas</li>\n",
        "<li>considerei apenas os limites de velocidade entre 0 e 250</li>\n",
        "<li>considerei apenas os limites de latitude e longitude possíveis: entre -90 e 90 e entre -180 e 180</li>\n",
        "<li>eu adicionei os 2 últimos dígitos de cada arquivo .json em uma nova coluna para ambos os datasets de treino e teste e chamei essa nova coluna de hour_from_file. O objetivo é usar essa coluna como referência na hora de treinar as 2 últimas horas para prever a hora seguinte</li>\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FinalDataProcessor:\n",
        "    def __init__(self, base_dir):\n",
        "        #Inicializa diretórios e cria pastas necessárias\n",
        "        self.base_dir = os.path.join(base_dir, 'dados_finais')\n",
        "        self.intermediarios_teste_dir = os.path.join(self.base_dir, 'intermediarios_teste')\n",
        "        self.processados_teste_dir = os.path.join(self.base_dir, 'processados_teste')\n",
        "        self.chunk_size = 100000\n",
        "\n",
        "        os.makedirs(self.intermediarios_teste_dir, exist_ok=True)\n",
        "        os.makedirs(self.processados_teste_dir, exist_ok=True)\n",
        "\n",
        "    #Função para extrair arquivos ZIP\n",
        "    def extract_zip_files(self, zip_files):\n",
        "        #Itera sobre cada arquivo ZIP\n",
        "        for zip_file in tqdm(zip_files, desc=\"Extracting zip files\"):\n",
        "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.intermediarios_teste_dir)\n",
        "\n",
        "    #Função para processar arquivos JSON\n",
        "    def process_json(self, json_path, df_list):\n",
        "        #Abre e carrega o arquivo JSON\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        #Converte para DataFrame cuDF\n",
        "        df = cudf.DataFrame.from_pandas(pd.DataFrame(data))\n",
        "\n",
        "        #Adiciona coluna com a hora extraída do nome do arquivo\n",
        "        hour_from_file = int(json_path.split('_')[-1].split('.')[0])\n",
        "        df['hour_from_file'] = hour_from_file\n",
        "\n",
        "        #Adiciona DataFrame à lista\n",
        "        df_list.append(df)\n",
        "\n",
        "    #Função para processar e concatenar arquivos\n",
        "    def process_and_concat_files(self, prefix, output_parquet, is_test=False):\n",
        "        all_files = []\n",
        "\n",
        "        #Caminha pelos diretórios e adiciona arquivos JSON que começam com o prefixo especificado\n",
        "        for root, _, files in os.walk(self.intermediarios_teste_dir):\n",
        "            for file in files:\n",
        "                if file.startswith(prefix) and file.endswith('.json'):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "\n",
        "        final_df_list = []\n",
        "\n",
        "        #Itera sobre cada arquivo JSON e processa\n",
        "        for json_file in tqdm(all_files, desc=f\"Processing {prefix} files\"):\n",
        "            self.process_json(json_file, final_df_list)\n",
        "\n",
        "        #Concatena DataFrames se a lista não estiver vazia\n",
        "        if final_df_list:\n",
        "            final_df = cudf.concat(final_df_list, ignore_index=True)\n",
        "            final_df.to_parquet(output_parquet)\n",
        "\n",
        "            if is_test:\n",
        "                self.split_test_data(final_df)\n",
        "            else:\n",
        "                final_df.to_csv(output_parquet.replace('.parquet', '.csv'), index=False)\n",
        "                print(f\"Saved concatenated {prefix} data to {output_parquet} and {output_parquet.replace('.parquet', '.csv')}\")\n",
        "        else:\n",
        "            print(f\"No {prefix} JSON files found.\")\n",
        "            final_df = None\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    #Função para dividir os dados de teste\n",
        "    def split_test_data(self, df):\n",
        "        #Separa dados de datahora e latitude/longitude\n",
        "        test_datahora = df.dropna(subset=['datahora']).drop(columns=['latitude', 'longitude'])\n",
        "        test_lat_long = df.dropna(subset = ['latitude', 'longitude']).drop(columns=['datahora'])\n",
        "\n",
        "        #Define caminhos para salvar os dados\n",
        "        test_datahora_parquet = os.path.join(self.processados_teste_dir, 'test_datahora.parquet')\n",
        "        test_datahora_csv = os.path.join(self.processados_teste_dir, 'test_datahora.csv')\n",
        "        test_lat_long_parquet = os.path.join(self.processados_teste_dir, 'test_lat_long.parquet')\n",
        "        test_lat_long_csv = os.path.join(self.processados_teste_dir, 'test_lat_long.csv')\n",
        "\n",
        "        #Salva dados em formato Parquet e CSV\n",
        "        test_datahora.to_parquet(test_datahora_parquet)\n",
        "        test_datahora.to_csv(test_datahora_csv, index=False)\n",
        "        test_lat_long.to_parquet(test_lat_long_parquet)\n",
        "        test_lat_long.to_csv(test_lat_long_csv, index=False)\n",
        "\n",
        "        print(f\"Saved test_datahora to {test_datahora_parquet} and {test_datahora_csv}\")\n",
        "        print(f\"Saved test_lat_long to {test_lat_long_parquet} and {test_lat_long_csv}\")\n",
        "\n",
        "    #Função para processar dados recentes iterativamente\n",
        "    def process_recent_data_iteratively(self, prefix, output_csv):\n",
        "        all_files = []\n",
        "\n",
        "        #Caminha pelos diretórios e adiciona arquivos JSON que começam com o prefixo especificado\n",
        "        for root, _, files in os.walk(self.intermediarios_teste_dir):\n",
        "            for file in files:\n",
        "                if file.startswith(prefix) and file.endswith('.json'):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "\n",
        "        final_df_list = []\n",
        "\n",
        "        #Itera sobre cada arquivo JSON e processa\n",
        "        for json_file in tqdm(all_files, desc=f\"Processing {prefix} files\"):\n",
        "            self.process_and_append_json(json_file, final_df_list, output_csv)\n",
        "\n",
        "        #Concatena DataFrames se a lista não estiver vazia\n",
        "        if final_df_list:\n",
        "            final_df = cudf.concat(final_df_list, ignore_index=True)\n",
        "        else:\n",
        "            final_df = None\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    #Função para processar e adicionar dados JSON\n",
        "    def process_and_append_json(self, json_path, df_list, output_csv):\n",
        "        #Abre e carrega o arquivo JSON\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        #Converte para DataFrame cuDF\n",
        "        df = cudf.DataFrame.from_pandas(pd.DataFrame(data))\n",
        "\n",
        "        #Adiciona coluna com a hora extraída do nome do arquivo\n",
        "        hour_from_file = int(json_path.split('_')[-1].split('.')[0])\n",
        "        df['hour_from_file'] = hour_from_file\n",
        "\n",
        "        #Lista de linhas válidas\n",
        "        linhas_validas = [483, 864, 639, 3, 309, 774, 629, 371, 397, 100, 838, 315, 624, 388, 918, 665, 328, 497, 878, 355, 138, 606, 457, 550, 803, 917, 638, 2336, 399, 298, 867, 553, 565, 422, 756, 186012003, 292, 554, 634, 232, 415, 2803, 324, 852, 557, 759, 343, 779, 905, 108]\n",
        "        #Filtra linhas válidas\n",
        "        df = df[df['linha'].astype(str).isin(map(str, linhas_validas))]\n",
        "\n",
        "        #Converte colunas de data e hora\n",
        "        df['datahoraenvio'] = cudf.to_datetime(df['datahoraenvio'].astype('int64'), unit='ms')\n",
        "        df['datahora_converted'] = cudf.to_datetime(df['datahora'].astype('int64'), unit='ms')\n",
        "\n",
        "        #Converte colunas de latitude e longitude\n",
        "        df['latitude'] = df['latitude'].str.replace(',', '.').astype('float32')\n",
        "        df['longitude'] = df['longitude'].str.replace(',', '.').astype('float32')\n",
        "\n",
        "        #Interpola valores nulos de latitude e longitude\n",
        "        while df['latitude'].isnull().any() or df['longitude'].isnull().any():\n",
        "            df['latitude'] = df['latitude'].interpolate().ffill().bfill()\n",
        "            df['longitude'] = df['longitude'].interpolate().ffill().bfill()\n",
        "\n",
        "        #Converte a coluna 'velocidade' para int32\n",
        "        df['velocidade'] = df['velocidade'].astype('int32')\n",
        "        # Filtra valores de velocidade e coordenadas válidas\n",
        "        df = df[(df['velocidade'] >= 0) & (df['velocidade'] <= 250)]\n",
        "        df = df[df['latitude'].between(-90, 90) & df['longitude'].between(-180, 180)]\n",
        "\n",
        "        #Adiciona DataFrame à lista\n",
        "        df_list.append(df)\n",
        "\n",
        "        #Salva intermediário iterativamente em CSV\n",
        "        if os.path.exists(output_csv):\n",
        "            df.to_pandas().to_csv(output_csv, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            df.to_pandas().to_csv(output_csv, index=False)\n",
        "\n",
        "    #Função principal para execução do processamento\n",
        "    def run(self):\n",
        "        #Obtém arquivos ZIP no diretório base\n",
        "        zip_files = glob(os.path.join(self.base_dir, '*.zip'))\n",
        "\n",
        "        if not zip_files:\n",
        "            raise FileNotFoundError(\"No zip files found in the specified directory.\")\n",
        "\n",
        "        #Extrai arquivos ZIP\n",
        "        self.extract_zip_files(zip_files)\n",
        "\n",
        "        treino_intermediate_parquet = os.path.join(self.processados_teste_dir, 'treino_data.parquet')\n",
        "        treino_intermediate_csv = os.path.join(self.processados_teste_dir, 'treino_data.csv')\n",
        "\n",
        "        #Concatena arquivos de treinamento\n",
        "        treino_data_df = self.process_and_concat_files('2024', treino_intermediate_parquet)\n",
        "\n",
        "        #Salva dados de treinamento não processados\n",
        "        if treino_data_df is not None:\n",
        "            treino_data_df.to_csv(treino_intermediate_csv, index=False)\n",
        "            print(f\"Saved training data to {treino_intermediate_parquet} and {treino_intermediate_csv}\")\n",
        "\n",
        "        #Processa e divide arquivos de teste\n",
        "        self.process_and_concat_files('teste', os.path.join(self.intermediarios_teste_dir, 'intermediate_test_data.parquet'), is_test=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    base_dir = os.getcwd()\n",
        "    processor = FinalDataProcessor(base_dir)\n",
        "    processor.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Pre processing the data</h3>\n",
        "<p>Agora eu estou aplicando alguns pre processamentos, como a filtragem por linhas ( por alguma razão, a filtragem anterior não funcionou), a diminuição do tamanho da palavra do tipo de dados para tornar o arquivo final menor, e a exclusão de outliers. Outliers são todos os pontos de latitude e longitude que fogem da rota usual da linha ao longo do dia.Outliers incluem garagens e pontos finais. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linhas_validas = [\n",
        "    483, 864, 639, 3, 309, 774, 629, 371, 397, 100, 838, 315, 624, 388, 918, 665, 328, 497, 878, 355, 138, 606, 457, \n",
        "    550, 803, 917, 638, 2336, 399, 298, 867, 553, 565, 422, 756, 186012003, 292, 554, 634, 232, 415, 2803, 324, 852, \n",
        "    557, 759, 343, 779, 905, 108\n",
        "]\n",
        "\n",
        "class DataPreprocessing:\n",
        "    def __init__(self, file_path=None, intermediarios_dir=None, processados_dir=None, output_filename=None, chunk_size=1000000):\n",
        "        # Inicializa as variáveis de caminho e cria os diretórios necessários\n",
        "        self.file_path = file_path\n",
        "        self.intermediarios_dir = intermediarios_dir\n",
        "        self.processados_dir = processados_dir\n",
        "        self.cleaned_file = os.path.join(intermediarios_dir, f'cleaned_{output_filename}') if intermediarios_dir and output_filename else None\n",
        "        self.prepared_file = os.path.join(processados_dir, 'treated', output_filename) if processados_dir and output_filename else None\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def load_data(self):\n",
        "        return cudf.read_parquet(self.file_path)\n",
        "\n",
        "    #Função para salvar dados intermediários em formato Parquet\n",
        "    def save_intermediate_data(self, df, filename):\n",
        "        df.to_parquet(filename)\n",
        "\n",
        "    #Função para converter os tipos de dados das colunas para tamanhos menores\n",
        "    def convert_dtypes(self, df):\n",
        "        if \"latitude\" in df.columns and (df[\"latitude\"].dtype == 'object' or df[\"latitude\"].dtype.name == 'category'):\n",
        "            df[\"latitude\"] = df[\"latitude\"].str.replace(',', '.').astype('float32')\n",
        "        if \"longitude\" in df.columns and (df[\"longitude\"].dtype == 'object' or df[\"longitude\"].dtype.name == 'category'):\n",
        "            df[\"longitude\"] = df[\"longitude\"].str.replace(',', '.').astype('float32')\n",
        "        if \"datahora\" in df.columns:\n",
        "            df['datahora'] = df['datahora'].astype('int64')\n",
        "        if \"linha\" in df.columns:\n",
        "            df['linha'] = df['linha'].astype('int32')\n",
        "        if \"ordem\" in df.columns:\n",
        "            df['ordem'] = df['ordem'].astype('object')\n",
        "        if \"velocidade\" in df.columns:\n",
        "            df['velocidade'] = df['velocidade'].astype('int32')\n",
        "        if \"datahoraenvio\" in df.columns:\n",
        "            df['datahoraenvio'] = df['datahoraenvio'].astype('int64')\n",
        "        if \"datahoraservidor\" in df.columns:\n",
        "            df['datahoraservidor'] = df['datahoraservidor'].astype('int64')\n",
        "        if \"dia_da_semana\" in df.columns:\n",
        "            df['dia_da_semana'] = df['dia_da_semana'].astype('int32')\n",
        "        if \"hora\" in df.columns:\n",
        "            df['hora'] = df['hora'].astype('int16')\n",
        "        if \"diff_timestamp\" in df.columns:\n",
        "            df['diff_timestamp'] = df['diff_timestamp'].astype('float64')\n",
        "        if \"latitude_diff\" in df.columns:\n",
        "            df['latitude_diff'] = df['latitude_diff'].astype('float32')\n",
        "        if \"longitude_diff\" in df.columns:\n",
        "            df['longitude_diff'] = df['longitude_diff'].astype('float32')\n",
        "        if \"distancia\" in df.columns:\n",
        "            df['distancia'] = df['distancia'].astype('float32')\n",
        "        return df\n",
        "\n",
        "    #Função para filtrar linhas válidas\n",
        "    def filter_valid_linhas(self, df):\n",
        "        if \"linha\" in df.columns:\n",
        "            df = df[df['linha'].astype(str).isin(map(str, linhas_validas))]\n",
        "        return df\n",
        "\n",
        "    '''\n",
        "    Isso é mais visível na plotagem de gráficos, mas basicamente, toda linha possui uma \n",
        "    rota fixa ao longo do dia. Qualquer rota que desvie dessa rota fixa é considerado\n",
        "    um outlier. Para encontrar e remover os outliers, a estratégia abaixo está sendo usada.\n",
        "    '''\n",
        "    def remove_outliers(self, df, eps=0.001, min_samples=10):\n",
        "        if os.path.exists(self.cleaned_file):\n",
        "            df = cudf.read_parquet(self.cleaned_file)\n",
        "            if 'datahora' in df.columns:\n",
        "                return df\n",
        "\n",
        "        if 'datahora_converted' not in df.columns and 'datahora' in df.columns:\n",
        "            df['datahora_converted'] = cudf.to_datetime(df['datahora'], unit='ms')\n",
        "\n",
        "        if 'linha' not in df.columns or 'ordem' not in df.columns:\n",
        "            raise KeyError(\"Necessary columns are not present in the data.\")\n",
        "\n",
        "        if 'latitude' not in df.columns or 'longitude' not in df.columns:\n",
        "            return df\n",
        "\n",
        "        lines = df['linha'].unique().to_pandas()\n",
        "        cleaned_dfs = []\n",
        "\n",
        "        #Itera sobre cada linha única e utiliza o DBSCAN para encontrar pontos fora da rota\n",
        "        for linha in tqdm(lines, desc=\"Lines\", leave=True, dynamic_ncols=True):\n",
        "            line_data = df[df['linha'] == linha].copy()\n",
        "            if len(line_data) < min_samples:\n",
        "                cleaned_dfs.append(line_data)\n",
        "                continue\n",
        "\n",
        "            if 'latitude' in line_data.columns and 'longitude' in line_data.columns:\n",
        "                line_data_pd = line_data[['latitude', 'longitude']].to_pandas()\n",
        "                db = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "                labels = db.fit_predict(line_data_pd)\n",
        "\n",
        "                line_data_pd['labels'] = labels\n",
        "                non_outliers = line_data_pd[line_data_pd['labels'] != -1]\n",
        "                non_outliers = non_outliers.copy()\n",
        "\n",
        "                for col in line_data.columns:\n",
        "                    if col not in ['latitude', 'longitude', 'labels']:\n",
        "                        if col in non_outliers:\n",
        "                            non_outliers[col] = line_data[col].to_pandas().loc[non_outliers.index]\n",
        "\n",
        "                cleaned_dfs.append(non_outliers)\n",
        "\n",
        "        if cleaned_dfs:\n",
        "            cleaned_df = cudf.DataFrame.from_pandas(pd.concat(cleaned_dfs, ignore_index=True))\n",
        "            self.save_intermediate_data(cleaned_df, self.cleaned_file)\n",
        "        else:\n",
        "            raise ValueError(\"No objects to concatenate\")\n",
        "\n",
        "        return cleaned_df\n",
        "\n",
        "    #Função para pré-processar os dados\n",
        "    def preprocess(self, remove_outliers=False):\n",
        "        #Carrega os dados\n",
        "        df = self.load_data()\n",
        "        #Converte os tipos de dados\n",
        "        df = self.convert_dtypes(df)\n",
        "        #Filtra linhas válidas\n",
        "        df = self.filter_valid_linhas(df)\n",
        "        #Remove outliers se solicitado\n",
        "        if remove_outliers and 'latitude' in df.columns and 'longitude' in df.columns:\n",
        "            df = self.remove_outliers(df)\n",
        "        #Cria diretório para salvar os dados processados\n",
        "        os.makedirs(os.path.dirname(self.prepared_file), exist_ok=True)\n",
        "        #Salva os dados processados em formato Parquet e CSV\n",
        "        df.to_parquet(self.prepared_file)\n",
        "        df.to_csv(self.prepared_file.replace('.parquet', '.csv'), index=False)\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    base_dir = os.getcwd()\n",
        "    base_final_dir = os.path.join(base_dir, 'dados_finais')\n",
        "    intermediarios_teste_dir = os.path.join(base_final_dir, 'intermediarios_teste')\n",
        "    processados_teste_dir = os.path.join(base_final_dir, 'processados_teste')\n",
        "\n",
        "    #Cria o diretório 'treated' dentro do diretório de dados processados de teste\n",
        "    os.makedirs(os.path.join(processados_teste_dir, 'treated'), exist_ok=True)\n",
        "\n",
        "    #Inicializa a classe DataPreprocessing para os dados de treino\n",
        "    treino_preprocessing = DataPreprocessing(\n",
        "        file_path=os.path.join(processados_teste_dir, 'treino_data.parquet'),\n",
        "        intermediarios_dir=intermediarios_teste_dir,\n",
        "        processados_dir=processados_teste_dir,\n",
        "        output_filename='treated_train_data.parquet'\n",
        "    )\n",
        "    #Preprocessa os dados de treino, removendo outliers\n",
        "    treino_preprocessing.preprocess(remove_outliers=True)\n",
        "\n",
        "    #Inicializa a classe DataPreprocessing para os dados de hora do teste\n",
        "    test_datahora_preprocessing = DataPreprocessing(\n",
        "        file_path=os.path.join(processados_teste_dir, 'test_datahora.parquet'),\n",
        "        intermediarios_dir=intermediarios_teste_dir,\n",
        "        processados_dir=processados_teste_dir,\n",
        "        output_filename='treated_test_datahora.parquet'\n",
        "    )\n",
        "    #Preprocessa os dados de hora do teste, sem remover outliers\n",
        "    test_datahora_preprocessing.preprocess(remove_outliers=False)\n",
        "\n",
        "    #Inicializa a classe DataPreprocessing para os dados de latitude e longitude do teste\n",
        "    test_lat_long_preprocessing = DataPreprocessing(\n",
        "        file_path=os.path.join(processados_teste_dir, 'test_lat_long.parquet'),\n",
        "        intermediarios_dir=intermediarios_teste_dir,\n",
        "        processados_dir=processados_teste_dir,\n",
        "        output_filename='treated_test_lat_long.parquet'\n",
        "    )\n",
        "    #Preprocessa os dados de latitude e longitude do teste, sem remover outliers\n",
        "    test_lat_long_preprocessing.preprocess(remove_outliers=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Processing the data and creating the features</h3>\n",
        "<p>A estratégia abaixo divide os arquivos de treino em df_datahora e df_lat_long. O objetivo é treinar arquivos separados para cada previsão que precisa ser feita. Além do mais, encoder específico para a coluna ordem foi criada, pois eu preciso transformar essa coluna em número mas sem alterar muito os seus valores. Também foram criadas novas features importantes, especialmente a velocidade média tanto para os datasets de treino quanto para os datasets de teste</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_and_split(df):\n",
        "    #Remove colunas específicas do DataFrame para criar dois datasets de treino distintos\n",
        "    df_datahora = df.drop(columns=['datahoraservidor', 'datahoraenvio', 'datahora_converted'])\n",
        "    df_lat_long = df.drop(columns=['datahoraservidor', 'datahoraenvio', 'datahora_converted'])\n",
        "    return df_datahora, df_lat_long\n",
        "\n",
        "'''\n",
        "Aqui estou criando uma função especificamente para transformar a coluna ordem em números.\n",
        "Isso é importante porque ao rodar alguns modelos de Machine Learning, precisamos passar \n",
        "somente valores numéricos. Como eu não queria alterar muito a estrutura dos valores da \n",
        "coluna ordem, eu criei uma estrutura própria para cuidar disso. Basicamente todos os \n",
        "valores da coluna ordem começa com uma letra seguida de números. Eu peguei essa letra\n",
        "do início de seus valores e transformei em número seguindo a ordem alfabética.\n",
        "Então o valor D123456, virou 4123456\n",
        "'''\n",
        "def custom_encode_ordem(df, column='ordem'):\n",
        "    def encode_value(val):\n",
        "        if isinstance(val, str) and val[0].isalpha():\n",
        "            letter = val[0].upper()\n",
        "            number = ord(letter) - ord('A') + 1\n",
        "            encoded_val = str(number) + val[1:]\n",
        "            return int(encoded_val)\n",
        "        elif val.isdigit():\n",
        "            return int(val)\n",
        "        return val\n",
        "\n",
        "    #Aplica a codificação personalizada à coluna 'ordem'\n",
        "    df[column] = df[column].apply(encode_value).astype('int32')\n",
        "    return df\n",
        "\n",
        "#Aqui está a função de feature engineering criando novas colunas\n",
        "def apply_feature_engineering(df, target):\n",
        "    try:\n",
        "        print(f\"Converting data types for {target}...\")\n",
        "        conv = DataPreprocessing()\n",
        "        df = conv.convert_dtypes(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in convert_dtypes: {e}\")\n",
        "        raise\n",
        "\n",
        "    try:\n",
        "        print(\"Criando novas features...\")\n",
        "        if 'datahora' in df.columns:\n",
        "            #Converte a coluna 'datahora' para datetime\n",
        "            df['datahora_converted'] = pd.to_datetime(df['datahora'], unit='ms')\n",
        "        \n",
        "        if 'datahora_converted' in df.columns and 'linha' in df.columns and 'ordem' in df.columns:\n",
        "            df = df.sort_values(by=['linha', 'ordem', 'datahora_converted'])\n",
        "            #Cria novas colunas baseadas em 'datahora_converted'\n",
        "            df['dia_da_semana'] = df['datahora_converted'].dt.weekday\n",
        "            df['hora'] = df['datahora_converted'].dt.hour\n",
        "            df['diff_timestamp'] = df.groupby(['linha', 'ordem'])['datahora_converted'].diff().astype('int64') / 10**6\n",
        "\n",
        "        if 'latitude' in df.columns and 'longitude' in df.columns:\n",
        "            df['latitude_diff'] = df.groupby(['linha', 'ordem'])['latitude'].diff().fillna(method='ffill')\n",
        "            df['longitude_diff'] = df.groupby(['linha', 'ordem'])['longitude'].diff().fillna(method='ffill')\n",
        "\n",
        "            #Calcula a distância entre pontos geográficos\n",
        "            lat1 = np.radians(df['latitude'])\n",
        "            lon1 = np.radians(df['longitude'])\n",
        "            lat2 = np.radians(df['latitude'] + df['latitude_diff'])\n",
        "            lon2 = np.radians(df['longitude'] + df['longitude_diff'])\n",
        "\n",
        "            dlat = lat2 - lat1\n",
        "            dlon = lon2 - lon1\n",
        "\n",
        "            a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
        "            c = 2 * np.arcsin(np.sqrt(a))\n",
        "            r = 6371\n",
        "            df['distancia'] = c * r * 1000\n",
        "\n",
        "        print(\"Novas features criadas: 'dia_da_semana', 'hora', 'diff_timestamp', 'latitude_diff', 'longitude_diff', 'distancia'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in creating new features: {e}\")\n",
        "        raise\n",
        "\n",
        "    try:\n",
        "        print(f\"Encoding categorical features for {target}...\")\n",
        "        df = custom_encode_ordem(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in encoding categorical features: {e}\")\n",
        "        raise\n",
        "\n",
        "    return df\n",
        "\n",
        "'''\n",
        "Um aspecto muito importante é a velocidade média. Nós temos a coluna velocidade\n",
        "nos datasets de treino, mas não a velocidade média. Eu me certifiquei de criar essa \n",
        "coluna nova. Eu agrupei os dataframes pelas colunas linha e ordem e algumas features\n",
        "temporais caso a variável alvo fosse datahora e features geoespaciais se as variáveis\n",
        "alvos fossem latitude e longitude, e a partir disso, calculei a velocidade média.\n",
        "'''\n",
        "def add_avg_velocity(train_df, test_df, target):\n",
        "    conv = DataPreprocessing()\n",
        "    train_df = conv.convert_dtypes(train_df.copy())\n",
        "    test_df = conv.convert_dtypes(test_df.copy())\n",
        "\n",
        "    if target == 'datahora':\n",
        "        train_df['datahora_converted'] = pd.to_datetime(train_df['datahora'], unit='ms')\n",
        "        test_df['datahora_converted'] = pd.to_datetime(test_df['datahora'], unit='ms')\n",
        "        if 'datahora_converted' in train_df.columns:\n",
        "            train_df['day_of_week'] = train_df['datahora_converted'].dt.weekday\n",
        "            train_df['hour'] = train_df['datahora_converted'].dt.hour\n",
        "            train_df['minute'] = train_df['datahora_converted'].dt.minute\n",
        "            avg_speed = train_df.groupby(['linha', 'ordem', 'day_of_week', 'hour', 'minute']).agg({'velocidade': 'mean'}).reset_index()\n",
        "            avg_speed.columns = ['linha', 'ordem', 'day_of_week', 'hour', 'minute', 'velocidade_for_datahora']\n",
        "\n",
        "            train_df = train_df.merge(avg_speed, on=['linha', 'ordem', 'day_of_week', 'hour', 'minute'], how='left')\n",
        "            train_df['velocidade_for_datahora'] = train_df['velocidade_for_datahora'].ffill().bfill()\n",
        "\n",
        "            if 'datahora_converted' in test_df.columns:\n",
        "                test_df['day_of_week'] = test_df['datahora_converted'].dt.weekday\n",
        "                test_df['hour'] = test_df['datahora_converted'].dt.hour\n",
        "                test_df['minute'] = test_df['datahora_converted'].dt.minute\n",
        "                test_df = test_df.merge(avg_speed, on=['linha', 'ordem', 'day_of_week', 'hour', 'minute'], how='left')\n",
        "                test_df['velocidade_for_datahora'] = test_df['velocidade_for_datahora'].ffill().bfill()\n",
        "                \n",
        "                test_df = test_df.rename(columns={'velocidade_for_datahora': 'avg_speed'})\n",
        "                train_df = train_df.rename(columns={'velocidade_for_datahora': 'avg_speed'})\n",
        "\n",
        "    elif target == 'lat_long':\n",
        "        if 'latitude' in train_df.columns and 'longitude' in train_df.columns:\n",
        "            train_df['lat_temp'] = (train_df['latitude'] * 100).astype(int)\n",
        "            train_df['lon_temp'] = (train_df['longitude'] * 100).astype(int)\n",
        "\n",
        "            avg_speed = train_df.groupby(['linha', 'ordem', 'hour_from_file', 'lat_temp', 'lon_temp']).agg({'velocidade': 'mean'}).reset_index()\n",
        "            avg_speed.columns = ['linha', 'ordem', 'hour_from_file', 'lat_temp', 'lon_temp', 'velocidade_for_lat_long']\n",
        "\n",
        "            train_df = train_df.merge(avg_speed, on=['linha', 'ordem', 'hour_from_file', 'lat_temp', 'lon_temp'], how='left')\n",
        "            train_df['velocidade_for_lat_long'] = train_df['velocidade_for_lat_long'].ffill().bfill()\n",
        "\n",
        "            if 'latitude' in test_df.columns and 'longitude' in test_df.columns:\n",
        "                test_df['lat_temp'] = (test_df['latitude'] * 100).astype(int)\n",
        "                test_df['lon_temp'] = (test_df['longitude'] * 100).astype(int)\n",
        "\n",
        "                test_df = test_df.merge(avg_speed, on=['linha', 'ordem', 'hour_from_file', 'lat_temp', 'lon_temp'], how='left')\n",
        "                test_df['velocidade_for_lat_long'] = test_df['velocidade_for_lat_long'].ffill().bfill()\n",
        "                \n",
        "                test_df = test_df.drop(columns=['lat_temp', 'lon_temp'])\n",
        "                train_df = train_df.drop(columns=['lat_temp', 'lon_temp'])\n",
        "\n",
        "                test_df = test_df.rename(columns={'velocidade_for_lat_long': 'avg_speed'})\n",
        "                train_df = train_df.rename(columns={'velocidade_for_lat_long': 'avg_speed'})\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def sort_columns(df, initial_columns=None, final_columns=None):\n",
        "    '''\n",
        "    Ordena as colunas de um DataFrame em ordem alfabética, \n",
        "    colocando colunas específicas no início e no final.\n",
        "    '''\n",
        "    if initial_columns is None:\n",
        "        initial_columns = []\n",
        "    if final_columns is None:\n",
        "        final_columns = []\n",
        "\n",
        "    #Obtenha a lista de todas as colunas\n",
        "    all_columns = list(df.columns)\n",
        "\n",
        "    #Remova as colunas específicas da lista geral de colunas\n",
        "    columns_to_sort = [col for col in all_columns if col not in initial_columns + final_columns]\n",
        "\n",
        "    #Ordene as colunas restantes em ordem alfabética\n",
        "    sorted_columns = sorted(columns_to_sort)\n",
        "\n",
        "    #Combine as colunas na ordem desejada\n",
        "    new_order = initial_columns + sorted_columns + final_columns\n",
        "\n",
        "    #Reorganiza o DataFrame com as colunas na nova ordem\n",
        "    df = df[new_order]\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_dir = os.getcwd()\n",
        "    base_final_dir = os.path.join(base_dir, 'dados_finais')\n",
        "    processados_teste_dir = os.path.join(base_final_dir, 'processados_teste', 'treated')\n",
        "    feature_modeling_dir = os.path.join(base_final_dir, 'feature_modeling')\n",
        "\n",
        "    os.makedirs(feature_modeling_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        print(\"Loading datasets...\")\n",
        "        train_df = pd.read_csv(os.path.join(processados_teste_dir, 'treated_train_data.csv'))\n",
        "        test_datahora_df = pd.read_csv(os.path.join(processados_teste_dir, 'treated_test_datahora.csv'))\n",
        "        test_lat_long_df = pd.read_csv(os.path.join(processados_teste_dir, 'treated_test_lat_long.csv'))\n",
        "\n",
        "        #Divide o conjunto de dados de treino em datahora e lat_long\n",
        "        print(\"Splitting training dataset into datahora and lat_long...\")\n",
        "        train_datahora_df, train_lat_long_df = preprocess_and_split(train_df)\n",
        "\n",
        "        #Adiciona velocidade média aos DataFrames de teste\n",
        "        print(\"Adding average velocity to test_datahora dataframe...\")\n",
        "        train_datahora_df, test_datahora_df = add_avg_velocity(train_datahora_df, test_datahora_df, 'datahora')\n",
        "        \n",
        "        print(\"Adding average velocity to test_lat_long dataframe...\")\n",
        "        train_lat_long_df, test_lat_long_df = add_avg_velocity(train_lat_long_df, test_lat_long_df, 'lat_long')\n",
        "\n",
        "        #Remove a coluna 'velocidade' onde existir. Só queremos a velocidade média\n",
        "        print(\"Remove 'velocidade' column where it exists...\")\n",
        "        if 'velocidade' in train_datahora_df.columns:\n",
        "            train_datahora_df = train_datahora_df.drop(columns=['velocidade'])\n",
        "        if 'velocidade' in test_datahora_df.columns:\n",
        "            test_datahora_df = test_datahora_df.drop(columns=['velocidade'])\n",
        "        if 'velocidade' in train_lat_long_df.columns:\n",
        "            train_lat_long_df = train_lat_long_df.drop(columns=['velocidade'])\n",
        "        if 'velocidade' in test_lat_long_df.columns:\n",
        "            test_lat_long_df = test_lat_long_df.drop(columns=['velocidade'])\n",
        "\n",
        "        #Aplica feature engineering\n",
        "        print(\"Apply feature engineering...\")\n",
        "        train_datahora_df = apply_feature_engineering(train_datahora_df, 'datahora')\n",
        "        train_lat_long_df = apply_feature_engineering(train_lat_long_df, 'lat_long')\n",
        "        test_datahora_df = apply_feature_engineering(test_datahora_df, 'datahora')\n",
        "        test_lat_long_df = apply_feature_engineering(test_lat_long_df, 'lat_long')\n",
        "\n",
        "        #Remove colunas desnecessárias\n",
        "        print(\"Dropping unnecessary columns...\")\n",
        "        train_datahora_df.drop(['latitude_diff', 'longitude_diff', \"distancia\", \"labels\", \"datahora_converted\"], axis=1, inplace=True)\n",
        "        test_datahora_df.drop([\"datahora_converted\"], axis=1, inplace=True)\n",
        "        train_lat_long_df.drop([\"labels\", \"datahora_converted\", \"dia_da_semana\", \"hora\", \"diff_timestamp\"], axis=1, inplace=True)\n",
        "\n",
        "        #Aplica a ordenação das colunas\n",
        "        print(\"Applying sorting to the dataframes...\")\n",
        "        train_datahora_df = sort_columns(train_datahora_df, None, final_columns=[\"latitude\", \"longitude\"])\n",
        "        test_datahora_df = sort_columns(test_datahora_df, initial_columns = [\"id\"], final_columns=None)\n",
        "        train_lat_long_df = sort_columns(train_lat_long_df, None, final_columns=[\"datahora\"])\n",
        "        test_lat_long_df = sort_columns(test_lat_long_df, initial_columns = [\"id\"], final_columns=None)\n",
        "\n",
        "        #Remove linhas com valores nulos\n",
        "        print(\"Dropping rows with null values...\")\n",
        "        train_datahora_df = train_datahora_df.dropna()\n",
        "        train_lat_long_df = train_lat_long_df.dropna()\n",
        "        test_datahora_df = test_datahora_df.dropna()\n",
        "        test_lat_long_df = test_lat_long_df.dropna()\n",
        "\n",
        "        #Salva os conjuntos de dados pré-processados para o treinamento do modelo\n",
        "        print(\"Saving preprocessed datasets for later model training...\")\n",
        "        train_datahora_df.to_csv(os.path.join(feature_modeling_dir, 'train_datahora.csv'), index=False)\n",
        "        train_lat_long_df.to_csv(os.path.join(feature_modeling_dir, 'train_lat_long.csv'), index=False)\n",
        "        test_datahora_df.to_csv(os.path.join(feature_modeling_dir, 'test_datahora.csv'), index=False)\n",
        "        test_lat_long_df.to_csv(os.path.join(feature_modeling_dir, 'test_lat_long.csv'), index=False)\n",
        "\n",
        "        print(\"Feature engineering and preprocessing complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in the main processing block: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Fazendo Cross Validation</h3>\n",
        "<p>Aqui irei fazer a validação cruzada. Basicamente, teremos duas previsões a serem feitas de maneira separada. Para isso, irei fazer duas validações cruzadas, testando Random Forest,XGBoost e CatBoost com vários parâmetros. Para cada validação cruzada, o objetivo é encontrar o melhor modelo, com os melhores parâmetros. </p>\n",
        "<p>No caso da previsão de datahora, a métrica utilizada para a avaliação do melhor modelo é o RMSE. No caso da previsão de latitude, a métrica utilizada para a avaliação do melhor modelo é a média da distância Harversine</p>\n",
        "<p>Estou usando o Optuna para a validação cruzada e estou usando a aceleração por CUDA. </p>\n",
        "<p>Após isso, o melhor modelo final é escolhido para cada um das previsões feitas</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Função auxiliar para cálculo da distância Haversine média, já que não encontrei uma biblioteca que faça esse cálculo diretamente\n",
        "def mean_haversine_distance(y_true, y_pred):\n",
        "    #Raio da Terra em quilômetros\n",
        "    R = 6371.0  \n",
        "    lat_true, lon_true = cp.radians(y_true[:, 0]), cp.radians(y_true[:, 1])\n",
        "    lat_pred, lon_pred = cp.radians(y_pred[:, 0]), cp.radians(y_pred[:, 1])\n",
        "    dlat = lat_pred - lat_true\n",
        "    dlon = lat_pred - lon_true\n",
        "    a = cp.sin(dlat / 2) ** 2 + cp.cos(lat_true) * cp.cos(lat_pred) * cp.sin(dlon / 2) ** 2\n",
        "    c = 2 * cp.arcsin(cp.sqrt(a))\n",
        "    return cp.mean(R * c)\n",
        "\n",
        "#Função para executar validação cruzada\n",
        "def run_cross_validation(df, target_cols, model_types, scoring, objective_func, save_dir, cv_name, is_location=False):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    best_params = {}\n",
        "\n",
        "    #Itera sobre cada tipo de modelo especificado\n",
        "    for model_type in model_types:\n",
        "        print(f\"Running Optuna for {model_type} on {cv_name} prediction...\")\n",
        "        study_path = os.path.join(save_dir, f\"{cv_name}_{model_type}_study.pkl\")\n",
        "\n",
        "        #Carrega estudo existente ou cria um novo. Isso é pra evitar refazer o que foi feito se der algum erro\n",
        "        if os.path.exists(study_path):\n",
        "            print(f\"Loading existing study for {model_type} on {cv_name} prediction...\")\n",
        "            study = joblib.load(study_path)\n",
        "        else:\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "            study.optimize(lambda trial: objective_func(trial, df, target_cols, is_location, model_type, scoring, cv_name), n_trials=10)\n",
        "            joblib.dump(study, study_path)\n",
        "\n",
        "        #Armazena os melhores parâmetros encontrados\n",
        "        if study.best_trial:\n",
        "            best_params[model_type] = study.best_trial.params\n",
        "        else:\n",
        "            print(f\"No completed trials for {model_type} on {cv_name} prediction. Skipping...\")\n",
        "            best_params[model_type] = None\n",
        "\n",
        "    return best_params\n",
        "\n",
        "'''\n",
        "Função objetivo para otimização. Essa função define os modelos e parâmetros que serão\n",
        "testados usando Optuna\n",
        "'''\n",
        "def objective(trial, df, target_cols, is_location, model_type, scoring, cv_name):\n",
        "    try:\n",
        "        #Define os parâmetros para cada tipo de modelo\n",
        "        if model_type == 'xgboost':\n",
        "            params = {\n",
        "                'verbosity': 0,\n",
        "                'objective': 'reg:squarederror',\n",
        "                'booster': 'gbtree',\n",
        "                'tree_method': 'gpu_hist',\n",
        "                'gpu_id': 0,\n",
        "                'eta': trial.suggest_float('eta', 1e-4, 1e-1, log=True),\n",
        "                'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
        "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "                'enable_categorical': True\n",
        "            }\n",
        "            model = xgb.XGBRegressor(**params)\n",
        "        elif model_type == 'cuml_rf':\n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
        "                'max_features': trial.suggest_float('max_features', 0.5, 1.0)\n",
        "            }\n",
        "            if is_location:\n",
        "                model_lat = cuRF(**params)\n",
        "                model_lon = cuRF(**params)\n",
        "            else:\n",
        "                model = cuRF(**params)\n",
        "        elif model_type == 'catboost':\n",
        "            params = {\n",
        "                'iterations': trial.suggest_int('iterations', 100, 1000),\n",
        "                'depth': trial.suggest_int('depth', 4, 10),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
        "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 1e-1, log=True),\n",
        "                'task_type': 'GPU',\n",
        "                'devices': '0'\n",
        "            }\n",
        "            model = CatBoostRegressor(**params)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "        '''\n",
        "        Define a validação cruzada. Se for para previsão de Latitude e Longitude\n",
        "        precisamos utilizar essa estratégia de MultiOutputRegressor, já que se trata\n",
        "        da previsão de duas colunas\n",
        "        '''\n",
        "        if is_location and model_type not in ['cuml_rf']:\n",
        "            model = MultiOutputRegressor(model)\n",
        "\n",
        "        scores = []\n",
        "        tss = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "        X = df.drop(columns=target_cols)\n",
        "        y = df[target_cols]\n",
        "\n",
        "        prediction_dir = os.path.join(\"dados_finais\", \"cross_validation\", \"intermediate_predictions\", model_type)\n",
        "        os.makedirs(prediction_dir, exist_ok=True)\n",
        "\n",
        "        #Itera sobre cada divisão da validação cruzada\n",
        "        for train_index, test_index in tqdm(tss.split(X), desc=\"Cross-validation\", leave=False):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "            #Verifica e trata valores NaN\n",
        "            if X_train.isna().sum().sum() > 0 or y_train.isna().sum().sum() > 0 or X_test.isna().sum().sum() > 0 or y_test.isna().sum().sum() > 0:\n",
        "                raise ValueError(f\"NaN values found in the data at fold {train_index}-{test_index}\")\n",
        "\n",
        "            '''\n",
        "            Essa parte abaixo é a estratégia que consiste em treinar as duas horas \n",
        "            consecutivas anteriores, para prever a hora seguinte. Essa estratégia\n",
        "            foi sugerida pelo professor Zimbrão. A coluna utilizada como referência\n",
        "            para a hora foi a hour_from_file. Essa coluna foi criada logo no começo\n",
        "            do processamento dos dados e os seus valores foram retirados dos próprios \n",
        "            nomes dos arquivos json, que continham a hora nos 2 últimos caracteres.\n",
        "            '''\n",
        "            unique_hours = sorted(X_test['hour_from_file'].to_pandas().unique())\n",
        "\n",
        "            '''\n",
        "            Itera sobre cada hora única. Como o kernel estava quebrando toda vez\n",
        "            que eu precisei adotar essa estratégia de treinar as duas horas anteriores\n",
        "            para prever as horas seguintes, eu resolvi fazer uma previsão de 1 hora\n",
        "            por vez, salvando essas previsões em arquivos intermediários. No final,\n",
        "            iremos usar todas essas previsões para criar um modelo preditivo só para \n",
        "            cada previsão que precisa ser feita: datahora ou latitude e longitude.\n",
        "            '''\n",
        "            for i in range(2, len(unique_hours)):\n",
        "                train_hours = unique_hours[i-2:i]\n",
        "                test_hour = unique_hours[i]\n",
        "\n",
        "                preds_path = os.path.join(prediction_dir, f\"{cv_name}_{model_type}_preds_{test_hour}.npy\")\n",
        "                actuals_path = os.path.join(prediction_dir, f\"{cv_name}_{model_type}_actuals_{test_hour}.npy\")\n",
        "\n",
        "                #Verifica se os arquivos intermediários já existem\n",
        "                if os.path.exists(preds_path) and os.path.exists(actuals_path):\n",
        "                    print(f\"Intermediate files for hour {test_hour} already exist. Skipping...\")\n",
        "                    continue\n",
        "\n",
        "                X_train_hour = X_train[X_train['hour_from_file'].isin(train_hours)]\n",
        "                y_train_hour = y_train.loc[X_train_hour.index]\n",
        "\n",
        "                X_test_hour = X_test[X_test['hour_from_file'] == test_hour]\n",
        "                y_test_hour = y_test.loc[X_test_hour.index]\n",
        "\n",
        "                #Verifica se o conjunto de treino ou teste está vazio\n",
        "                if X_train_hour.empty or X_test_hour.empty:\n",
        "                    print(f\"Skipping empty train or test set for hour {test_hour}\")\n",
        "                    continue\n",
        "\n",
        "                '''\n",
        "                Treina e faz as previsões hora a hora. No final, apenas concateno\n",
        "                todas essas previsões. Essa estratégia de fazer as previsões de \n",
        "                forma iterativa foi necessário apenas para alguns modelos.\n",
        "                '''\n",
        "                if is_location and model_type == 'cuml_rf':\n",
        "                    model_lat.fit(X_train_hour, y_train_hour['latitude'])\n",
        "                    model_lon.fit(X_train_hour, y_train_hour['longitude'])\n",
        "                    preds_lat = model_lat.predict(X_test_hour)\n",
        "                    preds_lon = model_lon.predict(X_test_hour)\n",
        "                    preds = cp.vstack((preds_lat, preds_lon)).T\n",
        "                else:\n",
        "                    model.fit(X_train_hour.to_pandas(), y_train_hour.to_pandas())\n",
        "                    preds = model.predict(X_test_hour.to_pandas())\n",
        "\n",
        "                '''\n",
        "                Como eu precisei utilizar recentemente o cudf para a aceleração por\n",
        "                GPU e Cuda, em alguns pontos do código, eu precisei retornar os \n",
        "                objetos para Pandas e/ou numpy.\n",
        "                '''\n",
        "                #Converte previsões para numpy array se necessário\n",
        "                if isinstance(preds, (pd.Series, pd.DataFrame)):\n",
        "                    preds = preds.to_numpy()\n",
        "                elif isinstance(preds, cp.ndarray):\n",
        "                    preds = preds.get()\n",
        "\n",
        "                # Garante que as previsões tenham o mesmo formato que os valores reais\n",
        "                if len(target_cols) == 1:\n",
        "                    preds = preds.reshape(-1, 1)\n",
        "                else:\n",
        "                    preds = preds.reshape(-1, len(target_cols))\n",
        "\n",
        "                cp.save(preds_path, preds)\n",
        "                cp.save(actuals_path, y_test_hour.to_pandas().values)\n",
        "\n",
        "                if preds.shape != y_test_hour.shape:\n",
        "                    print(f\"Inconsistent shape detected in predictions for hour {test_hour}. Skipping...\")\n",
        "                    continue\n",
        "\n",
        "        #Agrega todas as previsões e valores reais\n",
        "        predictions = []\n",
        "        actuals = []\n",
        "\n",
        "        #Itera sobre cada hora única novamente para agregar previsões e valores reais\n",
        "        for i in range(2, len(unique_hours)):\n",
        "            test_hour = unique_hours[i]\n",
        "\n",
        "            preds_path = os.path.join(prediction_dir, f\"{cv_name}_{model_type}_preds_{test_hour}.npy\")\n",
        "            actuals_path = os.path.join(prediction_dir, f\"{cv_name}_{model_type}_actuals_{test_hour}.npy\")\n",
        "\n",
        "            if os.path.exists(preds_path) and os.path.exists(actuals_path):\n",
        "                preds = cp.load(preds_path)\n",
        "                actuals_data = cp.load(actuals_path)\n",
        "\n",
        "                #Garante que previsões e valores reais tenham o mesmo formato\n",
        "                #É uma estratégia para que possamos concatenar as previsões sem conflitos.\n",
        "                preds = preds.reshape(-1, actuals_data.shape[1])\n",
        "\n",
        "                if preds.shape[0] != actuals_data.shape[0]:\n",
        "                    print(f\"Inconsistent sample size between actuals ({actuals_data.shape[0]}) and predictions ({preds.shape[0]}). Skipping hour {test_hour}...\")\n",
        "                    continue\n",
        "\n",
        "                predictions.append(preds)\n",
        "                actuals.append(actuals_data)\n",
        "\n",
        "        #Verifica se há previsões e valores reais válidos\n",
        "        if predictions and actuals:\n",
        "            predictions = cp.vstack(predictions)\n",
        "            actuals = cp.vstack(actuals)\n",
        "        else:\n",
        "            raise ValueError(\"No valid predictions were aggregated.\")\n",
        "\n",
        "        #Calcula a métrica de avaliação: ou RMSE para prever datahora ou \n",
        "        #Mean Haversine Distance para latitude e longitude.\n",
        "        if scoring == 'rmse':\n",
        "            score = mean_squared_error(actuals.get(), predictions.get(), squared=False)\n",
        "        elif scoring == 'mean_haversine_distance':\n",
        "            if is_location:\n",
        "                if predictions.shape[1] == 2 and actuals.shape[1] == 2:\n",
        "                    score = mean_haversine_distance(actuals, predictions)\n",
        "                else:\n",
        "                    raise ValueError(\"Prediction and actual shapes do not match the expected latitude/longitude format\")\n",
        "            else:\n",
        "                raise ValueError(\"mean_haversine_distance is not applicable for datahora prediction\")\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "        if len(scores) == 0:\n",
        "            raise ValueError(\"No scores were computed. Check data and model configurations.\")\n",
        "        return cp.mean(cp.array(scores))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in trial: {e}\")\n",
        "        raise ValueError(\"No scores were computed. Check data and model configurations.\")\n",
        "\n",
        "#Função para criar o modelo final a partir dos melhores parâmetros e melhores modelos \n",
        "#avaliados pela validação cruzada\n",
        "def create_final_model(X, y, best_model_name, best_params, save_dir, model_type):\n",
        "    final_model_path = os.path.join(save_dir, f'final_model_{model_type}.pkl')\n",
        "\n",
        "    if os.path.exists(final_model_path):\n",
        "        print(f\"Model file {final_model_path} already exists. Skipping model creation.\")\n",
        "        return joblib.load(final_model_path)\n",
        "\n",
        "    #Define o modelo com os melhores parâmetros\n",
        "    if best_model_name == 'xgboost':\n",
        "        model = xgb.XGBRegressor(**best_params)\n",
        "    elif best_model_name == 'cuml_rf':\n",
        "        model_lat = cuRF(**best_params)\n",
        "        model_lon = cuRF(**best_params)\n",
        "    elif best_model_name == 'catboost':\n",
        "        model = CatBoostRegressor(**best_params)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model type: {best_model_name}\")\n",
        "\n",
        "    #Treina e salva o modelo\n",
        "    if model_type == 'lat_long' and best_model_name == 'cuml_rf':\n",
        "        model_lat.fit(X.to_pandas(), y['latitude'].to_pandas())\n",
        "        model_lon.fit(X.to_pandas(), y['longitude'].to_pandas())\n",
        "        joblib.dump((model_lat, model_lon), final_model_path)\n",
        "    else:\n",
        "        model.fit(X.to_pandas(), y.to_pandas())\n",
        "        joblib.dump(model, final_model_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    base_dir = os.getcwd()\n",
        "    feature_modeling_dir = os.path.join(base_dir, 'dados_finais', 'feature_modeling')\n",
        "    cross_validation_dir = os.path.join(base_dir, 'dados_finais', 'cross_validation')\n",
        "    final_model_dir = os.path.join(cross_validation_dir, 'final_models')\n",
        "    os.makedirs(final_model_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        #Carrega os conjuntos de dados\n",
        "        train_lat_long = cudf.read_csv(os.path.join(feature_modeling_dir, 'train_lat_long.csv'))\n",
        "        train_datahora = cudf.read_csv(os.path.join(feature_modeling_dir, 'train_datahora.csv'))\n",
        "\n",
        "        model_types = ['xgboost', 'cuml_rf', 'catboost']\n",
        "\n",
        "        #Executa a validação cruzada para a previsão de datahora\n",
        "        print(\"Running cross-validation for datahora prediction...\")\n",
        "        best_params_datahora = run_cross_validation(train_lat_long, ['datahora'], model_types, 'rmse', objective, cross_validation_dir, 'datahora')\n",
        "\n",
        "        #Executa a validação cruzada para a previsão de latitude e longitude\n",
        "        print(\"Running cross-validation for lat_long prediction...\")\n",
        "        best_params_lat_long = run_cross_validation(train_datahora, ['latitude', 'longitude'], model_types, 'mean_haversine_distance', objective, cross_validation_dir, 'lat_long', is_location=True)\n",
        "\n",
        "        #Cria os modelos finais para a previsão de datahora\n",
        "        print(\"Creating final models for datahora prediction...\")\n",
        "        for model_type in model_types:\n",
        "            if best_params_datahora[model_type] is not None:\n",
        "                X_datahora = train_lat_long.drop(columns=['datahora'])\n",
        "                y_datahora = train_lat_long['datahora']\n",
        "                create_final_model(X_datahora, y_datahora, model_type, best_params_datahora[model_type], final_model_dir, 'datahora')\n",
        "\n",
        "        #Cria os modelos finais para a previsão de latitude e longitude\n",
        "        print(\"Creating final models for lat_long prediction...\")\n",
        "        for model_type in model_types:\n",
        "            if best_params_lat_long[model_type] is not None:\n",
        "                X_lat_long = train_datahora.drop(columns=['latitude', 'longitude'])\n",
        "                y_lat_long = train_datahora[['latitude', 'longitude']]\n",
        "                create_final_model(X_lat_long, y_lat_long, model_type, best_params_lat_long[model_type], final_model_dir, 'lat_long')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Realizando as previsões</h1>\n",
        "<p>Como não ficou claro como as previsões deveriam ser feitas eu tive que assumir algumas coisas:\n",
        "<li>Todos os dados usados foram coletados exclusivamente do último post em relação à tarefa 3 no Moodle</li>\n",
        "<li>Os dados foram treinados exclusivamente usando os dados criados a partir dos arquivos .JSON cujo nome começa com 2024</li>\n",
        "<li>Para fazer a avaliação do modelo durante a validação cruzada, os datasets de treino foram divididos entre treino e test, como normalmente é feito</li>\n",
        "<li>Os modelos criados durante a fase de treinamento foram utilizados nos datasets de teste disponibilizados no último link.</li>\n",
        "<li>Como previamente os datasets de treino haviam sido divididos para aplicar os modelos de previsão de datahora e para aplicar os modelos de previsão de latitude e longitude, colunas específicas foram adicionadas ou retiradas aos dois datasets de teste</li>\n",
        "<li>Para o dataset de teste onde o modelo de prever a datahora foi aplicado, as colunas temporais foram retiradas para evitar o data leakage</li>\n",
        "<li>Para o dataset de teste onde o modelo de prever latitude e longitude foi aplicado, as colunas com informações geoespaciais foram retiradas para evitar o data leakage.</li>\n",
        "<li>Após a previsão foram criados dois arquivos: um contendo todas as colunas anteriores, mais as previsões de datahora chamado predicted_datahora e outro contendo todas as colunas anteriores mais as previsões de latitude e longitude chamado predicted_lat_long</li>\n",
        " </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_predictions(model_path, test_data_path, target_cols, prediction_file_name, required_features, output_dir):\n",
        "    #Carrega o modelo\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "    #Carrega os dados de teste\n",
        "    test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "    #Salva a coluna 'id' e a remove dos dados de teste\n",
        "    id_column = test_data['id']\n",
        "    test_data = test_data.drop(columns=['id'], errors='ignore')\n",
        "\n",
        "    #Verifica se todas as features necessárias estão presentes\n",
        "    missing_features = set(required_features) - set(test_data.columns)\n",
        "    if missing_features:\n",
        "        raise ValueError(f\"Missing required features in test data: {missing_features}\")\n",
        "\n",
        "    #Garante que as colunas estejam na ordem correta\n",
        "    test_data = test_data[required_features]\n",
        "\n",
        "    #Faz as previsões\n",
        "    predictions = model.predict(test_data)\n",
        "\n",
        "    #Garante que as previsões tenham o formato correto para modelos MultiOutput\n",
        "    if isinstance(predictions, (pd.Series, pd.DataFrame)):\n",
        "        predictions = predictions.to_numpy()\n",
        "    elif isinstance(predictions, cp.ndarray):\n",
        "        predictions = predictions.get()\n",
        "\n",
        "    if predictions.ndim == 1:\n",
        "        predictions = predictions.reshape(-1, 1)\n",
        "\n",
        "    #Adiciona as previsões aos dados de teste\n",
        "    predictions_df = pd.DataFrame(predictions, columns=target_cols)\n",
        "    test_data_with_predictions = pd.concat([test_data, predictions_df], axis=1)\n",
        "\n",
        "    #Adiciona a coluna 'id' de volta aos dados de teste\n",
        "    test_data_with_predictions['id'] = id_column\n",
        "\n",
        "    #Salva os dados de teste modificados com as previsões em um arquivo CSV\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = os.path.join(output_dir, f\"{prediction_file_name}.csv\")\n",
        "    test_data_with_predictions.to_csv(output_path, index=False)\n",
        "    print(f\"Predictions added and saved to {output_path}\")\n",
        "\n",
        "def execute_predictions():\n",
        "    base_dir = os.getcwd()\n",
        "    cross_validation_dir = os.path.join(base_dir, 'dados_finais', 'cross_validation', 'final_models')\n",
        "    feature_modeling_dir = os.path.join(base_dir, 'dados_finais', 'feature_modeling')\n",
        "    output_dir = os.path.join(base_dir, 'dados_finais', 'respostas')\n",
        "\n",
        "    #Caminhos para os modelos finais\n",
        "    datahora_model_path = os.path.join(cross_validation_dir, 'final_model_datahora.pkl')\n",
        "    lat_long_model_path = os.path.join(cross_validation_dir, 'final_model_lat_long.pkl')\n",
        "\n",
        "    #Caminhos para os dados de teste\n",
        "    test_lat_long_path = os.path.join(feature_modeling_dir, 'test_lat_long.csv')\n",
        "    test_datahora_path = os.path.join(feature_modeling_dir, 'test_datahora.csv')\n",
        "\n",
        "    #Features necessárias\n",
        "    datahora_required_features = ['avg_speed', 'distancia', 'hour_from_file', 'latitude', 'latitude_diff', 'linha', 'longitude', 'longitude_diff', 'ordem']\n",
        "    lat_long_required_features = ['avg_speed', 'datahora', 'day_of_week', 'dia_da_semana', 'diff_timestamp', 'hora', 'hour', 'hour_from_file', 'linha', 'minute', 'ordem']\n",
        "\n",
        "    #Faz previsões para latitude e longitude usando final_model_lat_long.pkl com test_datahora.csv\n",
        "    print(\"Predicting latitude and longitude...\")\n",
        "    make_predictions(lat_long_model_path, test_datahora_path, ['latitude', 'longitude'], 'predicted_lat_long', lat_long_required_features, output_dir)\n",
        "\n",
        "    #Faz previsões para datahora usando final_model_datahora.pkl com test_lat_long.csv\n",
        "    print(\"Predicting datahora...\")\n",
        "    make_predictions(datahora_model_path, test_lat_long_path, ['datahora'], 'predicted_datahora', datahora_required_features, output_dir)\n",
        "\n",
        "#Executa as previsões\n",
        "execute_predictions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Criando os arquivos de submissão</h1>\n",
        "<p>Como especificado no enunciado, o formato dos arquivos de submissão deveriam ser em .json. Então eu fiz um arquivo .json para cada previsão feita. <b>O grande problema com esse approach é que não será possível submeter essas respostas, haja vista que isso gerou arquivos .json muito grande. </b> </p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Para a previsão de datahora\n",
        "#O arquivo .json foi criado no seguinte formato:\n",
        "\n",
        "'''\n",
        "{ \n",
        "   \"aluno\": \"Your Name\",\n",
        "   \"datahora\": \"2024-05-20 13:00:00\" #Valor aleatório que não foi explicado no enunciado,\n",
        "   \"previsoes\": [ \n",
        "                  [400172783234, 1716205511000] #Id e a previsão de datahora previstos, \n",
        "                  [282474448123, 1716204264000]\n",
        "                ], \n",
        "   \"senha\": \"your_password\"\n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Para a previsão de latitude e longitude\n",
        "#O arquivo .json foi criado no seguinte formato:\n",
        "'''\n",
        "{ \n",
        "   \"aluno\": \"Your Name\",\n",
        "   \"datahora\": \"2024-05-20 13:00:00\" #Valor aleatório que não foi explicado no enunciado,\n",
        "   \"previsoes\": [ \n",
        "                  [362511850614, -22.82553, -43.16925] #Id e valores de latitude e longitude previstos, \n",
        "                  [288961216441, -23.0202, -43.46159]\n",
        "                ], \n",
        "   \"senha\": \"your_password\"\n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "def create_submission_json(predictions_file, aluno, senha, prediction_type, output_dir):\n",
        "    #Carrega o arquivo de previsões\n",
        "    predictions = pd.read_csv(predictions_file)\n",
        "    \n",
        "    #Garante que a coluna 'datahora' esteja corretamente formatada\n",
        "    if 'datahora' in predictions.columns:\n",
        "        predictions['datahora'] = pd.to_datetime(predictions['datahora'], unit='ms')\n",
        "        \n",
        "    #Cria a coluna 'submission_datahora' combinando data e hora, com minutos e segundos\n",
        "    if 'datahora' in predictions.columns and 'hour_from_file' in predictions.columns:\n",
        "        predictions['submission_datahora'] = predictions['datahora'].dt.strftime('%Y-%m-%d') + ' ' + predictions['hour_from_file'].astype(str).str.zfill(2) + ':00:00'\n",
        "    \n",
        "    #Garante que as colunas 'latitude' e 'longitude' estejam corretamente formatadas\n",
        "    if 'latitude' in predictions.columns and 'longitude' in predictions.columns:\n",
        "        predictions['latitude'] = predictions['latitude'].astype(float)\n",
        "        predictions['longitude'] = predictions['longitude'].astype(float)\n",
        "    \n",
        "    #Cria diretório de saída, se não existir\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    #Processa cada grupo de previsões por 'submission_datahora'\n",
        "    for submission_datahora, group in predictions.groupby('submission_datahora'):\n",
        "        initial_datahora = submission_datahora\n",
        "        if prediction_type == 'datahora':\n",
        "            #Cria a lista de previsões para 'datahora'\n",
        "            previsoes = [[int(row['id']), row['datahora'].timestamp() * 1000] for _, row in group.iterrows()]\n",
        "        elif prediction_type == 'lat_long':\n",
        "            #Cria a lista de previsões para 'lat_long'\n",
        "            previsoes = [[int(row['id']), row['latitude'], row['longitude']] for _, row in group.iterrows()]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid prediction type. Must be 'datahora' or 'lat_long'.\")\n",
        "        \n",
        "        #Constrói o objeto JSON\n",
        "        json_object = {\n",
        "            \"aluno\": aluno,\n",
        "            \"datahora\": initial_datahora,\n",
        "            \"previsoes\": previsoes,\n",
        "            \"senha\": senha\n",
        "        }\n",
        "        \n",
        "        #Salva o objeto JSON em um arquivo\n",
        "        formatted_date = initial_datahora.replace(' ', '_').replace(':', '_')\n",
        "        output_file = os.path.join(output_dir, f\"resposta_{prediction_type}_{formatted_date}.json\")\n",
        "        with open(output_file, \"w\") as json_file:\n",
        "            json.dump(json_object, json_file, indent=4)\n",
        "        print(f\"Submission file saved to {output_file}\")\n",
        "\n",
        "#Define as credenciais do aluno\n",
        "aluno = \"Wagner Luiz Lobo Ferreira\"\n",
        "senha = \"muramassa_coi\"\n",
        "base_dir = os.getcwd()\n",
        "resposta_dir = os.path.join(base_dir, 'dados_finais', 'respostas')\n",
        "\n",
        "#Cria o arquivo JSON de submissão para previsões de 'datahora'\n",
        "create_submission_json(\n",
        "    predictions_file=os.path.join(resposta_dir, 'predicted_datahora.csv'),\n",
        "    aluno=aluno,\n",
        "    senha=senha,\n",
        "    prediction_type='datahora',\n",
        "    output_dir=os.path.join(resposta_dir, 'submission')\n",
        ")\n",
        "\n",
        "#Cria o arquivo JSON de submissão para previsões de 'latitude' e 'longitude'\n",
        "create_submission_json(\n",
        "    predictions_file=os.path.join(resposta_dir, 'predicted_lat_long.csv'),\n",
        "    aluno=aluno,\n",
        "    senha=senha,\n",
        "    prediction_type='lat_long',\n",
        "    output_dir=os.path.join(resposta_dir, 'submission')\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_submission_json(predictions_file, prediction_type, output_dir):\n",
        "    #Carrega o arquivo de previsões\n",
        "    predictions = pd.read_csv(predictions_file)\n",
        "    \n",
        "    #Garante que a coluna 'datahora' esteja corretamente formatada\n",
        "    if 'datahora' in predictions.columns:\n",
        "        predictions['datahora'] = pd.to_datetime(predictions['datahora'], unit='ms')\n",
        "        \n",
        "    #Cria a coluna 'submission_datahora' combinando data e hora, com minutos e segundos\n",
        "    if 'datahora' in predictions.columns and 'hour_from_file' in predictions.columns:\n",
        "        predictions['submission_datahora'] = predictions['datahora'].dt.strftime('%Y-%m-%d') + ' ' + predictions['hour_from_file'].astype(str).str.zfill(2) + ':00:00'\n",
        "    \n",
        "    #Garante que as colunas 'latitude' e 'longitude' estejam corretamente formatadas\n",
        "    if 'latitude' in predictions.columns and 'longitude' in predictions.columns:\n",
        "        predictions['latitude'] = predictions['latitude'].astype(float)\n",
        "        predictions['longitude'] = predictions['longitude'].astype(float)\n",
        "    \n",
        "    #Cria diretório de saída, se não existir\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    #Processa cada grupo de previsões por 'submission_datahora'\n",
        "    for submission_datahora, group in predictions.groupby('submission_datahora'):\n",
        "        initial_datahora = submission_datahora\n",
        "        if prediction_type == 'datahora':\n",
        "            #Cria a lista de previsões para 'datahora'\n",
        "            previsoes = [[int(row['id']), row['datahora'].timestamp() * 1000] for _, row in group.iterrows()]\n",
        "        elif prediction_type == 'lat_long':\n",
        "            #Cria a lista de previsões para 'lat_long'\n",
        "            previsoes = [[int(row['id']), row['latitude'], row['longitude']] for _, row in group.iterrows()]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid prediction type. Must be 'datahora' or 'lat_long'.\")\n",
        "        \n",
        "        #Constrói o objeto JSON\n",
        "        json_object = {\n",
        "            #\"aluno\": aluno,\n",
        "            \"datahora\": initial_datahora,\n",
        "            \"previsoes\": previsoes\n",
        "            #\"senha\": senha\n",
        "        }\n",
        "        \n",
        "        #Salva o objeto JSON em um arquivo\n",
        "        formatted_date = initial_datahora.replace(' ', '_').replace(':', '_')\n",
        "        output_file = os.path.join(output_dir, f\"resposta_{prediction_type}_{formatted_date}.json\")\n",
        "        with open(output_file, \"w\") as json_file:\n",
        "            json.dump(json_object, json_file, indent=4)\n",
        "        print(f\"Submission file saved to {output_file}\")\n",
        "\n",
        "#Define as credenciais do aluno\n",
        "#aluno = \"Wagner Luiz Lobo Ferreira\"\n",
        "#senha = \"muramassa_coi\"\n",
        "base_dir = os.getcwd()\n",
        "resposta_dir = os.path.join(base_dir, 'dados_finais', 'respostas')\n",
        "\n",
        "#Cria o arquivo JSON de submissão para previsões de 'datahora'\n",
        "create_submission_json(\n",
        "    predictions_file=os.path.join(resposta_dir, 'predicted_datahora.csv'),\n",
        "    #aluno=aluno,\n",
        "    #senha=senha,\n",
        "    prediction_type='datahora',\n",
        "    output_dir=os.path.join(resposta_dir, 'submission')\n",
        ")\n",
        "\n",
        "#Cria o arquivo JSON de submissão para previsões de 'latitude' e 'longitude'\n",
        "create_submission_json(\n",
        "    predictions_file=os.path.join(resposta_dir, 'predicted_lat_long.csv'),\n",
        "    #aluno=aluno,\n",
        "    #senha=senha,\n",
        "    prediction_type='lat_long',\n",
        "    output_dir=os.path.join(resposta_dir, 'submission')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File resposta_datahora_2024-05-17_07_00_00.json sent successfully.\n",
            "Failed to send file resposta_datahora_2024-05-17_11_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Encontrado: [[number, number]]\",\"hint\":\"Esperado: [number,number,number] (ex: [ [362511850614, -22.82553, -43.16925 ], [288961216441, -23.0202, -43.46159 ],  ... ] )\",\"message\":\"O parâmetro 'previsoes' deve ser um array contendo arrays de previsões para latitude,longitude\"}\n",
            "Failed to send file resposta_datahora_2024-05-17_13_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-17 13:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-17_19_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Encontrado: [[number, number]]\",\"hint\":\"Esperado: [number,number,number] (ex: [ [362511850614, -22.82553, -43.16925 ], [288961216441, -23.0202, -43.46159 ],  ... ] )\",\"message\":\"O parâmetro 'previsoes' deve ser um array contendo arrays de previsões para latitude,longitude\"}\n",
            "Failed to send file resposta_datahora_2024-05-18_07_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-18 07:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-18_11_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-18 11:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-18_13_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-18 13:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-18_15_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-18 15:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-18_19_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-18 19:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-19_07_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Encontrado: [[number, number]]\",\"hint\":\"Esperado: [number,number,number] (ex: [ [362511850614, -22.82553, -43.16925 ], [288961216441, -23.0202, -43.46159 ],  ... ] )\",\"message\":\"O parâmetro 'previsoes' deve ser um array contendo arrays de previsões para latitude,longitude\"}\n",
            "Failed to send file resposta_datahora_2024-05-19_08_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-19 08:00:00\"}\n",
            "File resposta_datahora_2024-05-19_11_00_00.json sent successfully.\n",
            "Failed to send file resposta_datahora_2024-05-19_12_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-19 12:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-19_13_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-19 13:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-19_15_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Encontrado: [[number, number]]\",\"hint\":\"Esperado: [number,number,number] (ex: [ [362511850614, -22.82553, -43.16925 ], [288961216441, -23.0202, -43.46159 ],  ... ] )\",\"message\":\"O parâmetro 'previsoes' deve ser um array contendo arrays de previsões para latitude,longitude\"}\n",
            "Failed to send file resposta_datahora_2024-05-19_16_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-19 16:00:00\"}\n",
            "File resposta_datahora_2024-05-19_19_00_00.json sent successfully.\n",
            "Failed to send file resposta_datahora_2024-05-19_20_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-19 20:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-20_07_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-20 07:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-20_08_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-20 08:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-20_12_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-20 12:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-20_15_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-20 15:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-20_16_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-20 16:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-20_19_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-20 19:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-20_20_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-20 20:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-20_21_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Encontrado: [[number, number]]\",\"hint\":\"Esperado: [number,number,number] (ex: [ [362511850614, -22.82553, -43.16925 ], [288961216441, -23.0202, -43.46159 ],  ... ] )\",\"message\":\"O parâmetro 'previsoes' deve ser um array contendo arrays de previsões para latitude,longitude\"}\n",
            "Failed to send file resposta_datahora_2024-05-21_12_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-21 12:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-21_16_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-21 16:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-21_20_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-21 20:00:00\"}\n",
            "Failed to send file resposta_datahora_2024-05-21_21_00_00.json. Status code: 400, Response: {\"code\":\"P0001\",\"details\":\"Exemplo: teste-2024-05-20_13.json ==> 2024-05-20 13:00:00\",\"hint\":\"O testes válidos estão nos arquivos teste-data_hora.json\",\"message\":\"Teste não encontrado: 2024-05-21 21:00:00\"}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Define the directory containing the JSON files\n",
        "base_dir = os.getcwd()\n",
        "resposta_dir = os.path.join(base_dir, 'dados_finais', 'respostas', 'submission')\n",
        "\n",
        "# Define the API endpoint\n",
        "api_url = 'https://barra.cos.ufrj.br:443/rest/rpc/avalia'\n",
        "\n",
        "# Define the student credentials\n",
        "aluno = \"Wagner Luiz Lobo Ferreira\"\n",
        "senha = \"muramassa_coi\"\n",
        "\n",
        "def send_json_file(file_path):\n",
        "    with open(file_path, 'r') as json_file:\n",
        "        json_data = json.load(json_file)\n",
        "    \n",
        "    # Add the student credentials to the JSON data\n",
        "    json_data['aluno'] = aluno\n",
        "    json_data['senha'] = senha\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                api_url,\n",
        "                headers={\n",
        "                    'accept': 'application/json',\n",
        "                    'Content-Type': 'application/json'\n",
        "                },\n",
        "                data=json.dumps(json_data),\n",
        "                timeout=10  # Set a timeout for the request\n",
        "            )\n",
        "            if response.status_code == 200:\n",
        "                print(f\"File {os.path.basename(file_path)} sent successfully.\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Failed to send file {os.path.basename(file_path)}. Status code: {response.status_code}, Response: {response.text}\")\n",
        "                break\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error sending file {os.path.basename(file_path)}: {e}. Retrying in 5 seconds...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "\n",
        "# Iterate over all JSON files in the directory and send each one\n",
        "for file_name in os.listdir(resposta_dir):\n",
        "    if file_name.endswith('.json'):\n",
        "        file_path = os.path.join(resposta_dir, file_name)\n",
        "        send_json_file(file_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
